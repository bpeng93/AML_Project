{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GCN(data_dir, ratio):\n",
    "    train_data = sio.loadmat(data_dir)\n",
    "    x_train = train_data['X']\n",
    "    y_train = train_data['y']\n",
    "    y_train[y_train == 10] = 0\n",
    "    x_train = x_train.transpose((3,0,1,2))\n",
    "    x_train.astype(float)\n",
    "    x_gray = np.dot(x_train, [[0.2989],[0.5870],[0.1140]])\n",
    "\n",
    "    imsize = x_gray.shape[0]\n",
    "    mean = np.mean(x_gray, axis=(1,2), dtype=float)\n",
    "    std = np.std(x_gray, axis=(1,2), dtype=float, ddof=1)\n",
    "    std[std < 1e-4] = 1\n",
    "    x_GCN = np.zeros(x_gray.shape, dtype=float)\n",
    "    for i in np.arange(imsize):\n",
    "        x_GCN[i,:,:] = (x_gray[i,:,:] - mean[i]) / std[i]\n",
    "    nums = x_GCN.shape[0]\n",
    "    x_GCN = x_GCN.reshape((nums,-1))\n",
    "    data = np.hstack((y_train,x_GCN))\n",
    "    np.random.shuffle(data)\n",
    "    cut=math.floor(nums*ratio)\n",
    "    train,val = data[:cut,:], data[cut:,:]\n",
    "\n",
    "    print(\"\\n------- GCN done -------\")\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_SVHN(data_dir, ratio, batch_size):\n",
    "\n",
    "    train,val = GCN(data_dir, ratio)\n",
    "    img_width = 32\n",
    "    img_height = 32\n",
    "    img_depth = 1\n",
    "    label_bytes = 1\n",
    "    image_bytes = 1024\n",
    "    record_bytes = 1025\n",
    "\n",
    "    with tf.name_scope('input'):\n",
    "        images_list = []\n",
    "        label_batch_list = []\n",
    "        for train_val in [train, val]:\n",
    "            q = tf.train.input_producer(train_val)\n",
    "            input_data = q.dequeue()\n",
    "\n",
    "            label = tf.slice(input_data , [0], [1])\n",
    "            label = tf.cast(label, tf.int32)\n",
    "\n",
    "            image_raw = tf.slice(input_data , [1], [1024])\n",
    "            image_raw = tf.reshape(image_raw, [1, 32, 32])\n",
    "            image = tf.transpose(image_raw, (1,2,0))\n",
    "            image = tf.cast(image, tf.float32)\n",
    "            images, label_batch = tf.train.batch([image, label],\n",
    "                                                batch_size = batch_size,\n",
    "                                                num_threads = 16,\n",
    "                                                capacity= 2000)\n",
    "\n",
    "            n_classes = 10\n",
    "            label_batch = tf.one_hot(label_batch, depth = n_classes)\n",
    "\n",
    "            label_batch_list.append(tf.reshape(label_batch, [batch_size, n_classes]))\n",
    "            images_list.append(images)\n",
    "\n",
    "\n",
    "        return images_list, label_batch_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------- GCN done -------\n",
      "train has the shape: (7325, 1025) \n",
      "val has the shape: (65932, 1025) \n",
      "Total has 73257 records\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data_dir = './data/SVHN/train_32x32.mat'\n",
    "    ratio = 0.1\n",
    "    train, val = GCN(data_dir, ratio)\n",
    "    print('train has the shape: {0} '.format(train.shape))\n",
    "    print('val has the shape: {0} '.format(val.shape))\n",
    "    print('Total has {0} records'.format(train.shape[0]+val.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(layer_name, x, out_channels, kernel_size=[5,5], stride=[1,1,1,1]):\n",
    "    '''Convolution op wrapper, use RELU activation after convolution\n",
    "    Args:\n",
    "        layer_name: e.g. conv1, pool1...\n",
    "        x: input tensor, [batch_size, height, width, channels]\n",
    "        out_channels: number of output channels (or comvolutional kernels)\n",
    "        kernel_size: the size of convolutional kernel, VGG paper used: [3,3]\n",
    "        stride: A list of ints. 1-D of length 4. VGG paper used: [1, 1, 1, 1]\n",
    "        is_pretrain: if load pretrained parameters, freeze all conv layers.\n",
    "        Depending on different situations, you can just set part of conv layers to be freezed.\n",
    "        the parameters of freezed layers will not change when training.\n",
    "    Returns:\n",
    "        4D tensor\n",
    "    '''\n",
    "\n",
    "    in_channels = x.get_shape()[-1]\n",
    "    with tf.variable_scope(layer_name):\n",
    "        w = tf.get_variable(name='weights',\n",
    "                            shape=[kernel_size[0], kernel_size[1], in_channels, out_channels],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer()) # default is uniform distribution initialization\n",
    "        b = tf.get_variable(name='biases',\n",
    "                            shape=[out_channels],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "        x = tf.nn.conv2d(x, w, stride, padding='SAME', name='conv')\n",
    "        x = tf.nn.bias_add(x, b, name='bias_add')\n",
    "        x = tf.nn.relu(x, name='relu')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool(layer_name, x, kernel=[1,2,2,1], stride=[1,2,2,1]):\n",
    "    '''Pooling op\n",
    "    Args:\n",
    "        x: input tensor\n",
    "        kernel: pooling kernel, VGG paper used [1,2,2,1], the size of kernel is 2X2\n",
    "        stride: stride size, VGG paper used [1,2,2,1]\n",
    "        padding:'SAME'\n",
    "    '''\n",
    "    x = tf.nn.max_pool(x, kernel, strides=stride, padding='SAME', name=layer_name)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FC_layer(layer_name, x, out_nodes):\n",
    "    '''Wrapper for fully connected layers with RELU activation as default\n",
    "    Args:\n",
    "        layer_name: e.g. 'FC1', 'FC2'\n",
    "        x: input feature map\n",
    "        out_nodes: number of neurons for current FC layer\n",
    "    '''\n",
    "    shape = x.get_shape()\n",
    "    size = shape[1].value * shape[2].value * shape[3].value\n",
    "\n",
    "    with tf.variable_scope(layer_name):\n",
    "        w = tf.get_variable('weights',\n",
    "                            shape=[size, out_nodes],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable('biases',\n",
    "                            shape=[out_nodes],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "        flat_x = tf.reshape(x, [-1, size]) # flatten into 1D\n",
    "\n",
    "        x = tf.nn.bias_add(tf.matmul(flat_x, w), b)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def final_layer(layer_name, x, out_nodes):\n",
    "    shape = x.get_shape()\n",
    "    size = shape[-1].value\n",
    "    with tf.variable_scope(layer_name):\n",
    "        w = tf.get_variable('weights',\n",
    "                            shape=[size, out_nodes],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable('biases',\n",
    "                            shape=[out_nodes],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "        flat_x = tf.reshape(x, [-1, size]) # flatten into 1D\n",
    "        x = tf.nn.bias_add(tf.matmul(flat_x, w), b)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_out(layer_name, x, keep_prob = 0.5):\n",
    "    with tf.variable_scope(layer_name):\n",
    "        x = tf.nn.dropout(x, keep_prob)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "    with tf.variable_scope('loss') as scope:\n",
    "        labels = tf.cast(labels, tf.int64)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits\\\n",
    "                        (logits=logits, labels=labels, name='loss')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "        tf.summary.scalar(scope.name+'/loss', loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy(logits, labels):\n",
    "  \"\"\"Evaluate the quality of the logits at predicting the label.\n",
    "  Args:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor,\n",
    "  \"\"\"\n",
    "  with tf.name_scope('accuracy') as scope:\n",
    "      correct = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "      correct = tf.cast(correct, tf.float32)\n",
    "      accuracy = tf.reduce_mean(correct)*100.0\n",
    "      tf.summary.scalar(scope+'/accuracy', accuracy)\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(loss, learning_rate, global_step):\n",
    "    '''optimization, use Gradient Descent as default\n",
    "    '''\n",
    "    with tf.name_scope('optimizer'):\n",
    "        # optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
