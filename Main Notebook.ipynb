{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import gfile\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import scipy.misc\n",
    "import math\n",
    "import pickle\n",
    "from random import randint\n",
    "import os\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GCN(data_dir, ratio):\n",
    "    ''' Perform image transformation Global Contraction Nornimlization \n",
    "    Args: \n",
    "        data_dir: the path to the SVHN, the data is in .mat format\n",
    "        ratio: test and train ratio\n",
    "    Return: \n",
    "        images after GCN transformation\n",
    "    '''\n",
    "    \n",
    "    train_data = sio.loadmat(data_dir)\n",
    "    x_train = train_data['X']\n",
    "    y_train = train_data['y']\n",
    "    y_train[y_train == 10] = 0\n",
    "    x_train = x_train.transpose((3,0,1,2))\n",
    "    x_train.astype(float)\n",
    "    x_gray = np.dot(x_train, [[0.2989],[0.5870],[0.1140]])\n",
    "\n",
    "    imsize = x_gray.shape[0]\n",
    "    mean = np.mean(x_gray, axis=(1,2), dtype=float)\n",
    "    std = np.std(x_gray, axis=(1,2), dtype=float, ddof=1)\n",
    "    std[std < 1e-4] = 1\n",
    "    x_GCN = np.zeros(x_gray.shape, dtype=float)\n",
    "    for i in np.arange(imsize):\n",
    "        x_GCN[i,:,:] = (x_gray[i,:,:] - mean[i]) / std[i]\n",
    "    nums = x_GCN.shape[0]\n",
    "    x_GCN = x_GCN.reshape((nums,-1))\n",
    "    data = np.hstack((y_train,x_GCN))\n",
    "    np.random.shuffle(data)\n",
    "    cut=math.floor(nums*ratio)\n",
    "    train,val = data[:cut,:], data[cut:,:]\n",
    "\n",
    "    print(\"\\n------- GCN done -------\")\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_SVHN(data_dir, ratio, batch_size):\n",
    "    ''' read SCVN function images are 32 * 32 and in gray scale \n",
    "    Args: \n",
    "        data_dir: the path to the SVHN, the data is in .mat format\n",
    "        ratio: test and train ratio\n",
    "        batch_size: number of train images trained per time\n",
    "    Return: \n",
    "        image_list: the first element of image_list is the train batch the second is the validation batch\n",
    "        label_list: the first element of label_list is the train batch the second is the validation batch\n",
    "    '''\n",
    "    train,val = GCN(data_dir, ratio)\n",
    "    img_width = 32\n",
    "    img_height = 32\n",
    "    img_depth = 1\n",
    "    label_bytes = 1\n",
    "    image_bytes = 1024\n",
    "    record_bytes = 1025\n",
    "\n",
    "    with tf.name_scope('input'):\n",
    "        images_list = []\n",
    "        label_batch_list = []\n",
    "        for train_val in [train, val]:\n",
    "            q = tf.train.input_producer(train_val)\n",
    "            input_data = q.dequeue()\n",
    "\n",
    "            label = tf.slice(input_data , [0], [1])\n",
    "            label = tf.cast(label, tf.int32)\n",
    "\n",
    "            image_raw = tf.slice(input_data , [1], [1024])\n",
    "            image_raw = tf.reshape(image_raw, [1, 32, 32])\n",
    "            image = tf.transpose(image_raw, (1,2,0))\n",
    "            image = tf.cast(image, tf.float32)\n",
    "            images, label_batch = tf.train.batch([image, label],\n",
    "                                                batch_size = batch_size,\n",
    "                                                num_threads = 16,\n",
    "                                                capacity= 2000)\n",
    "\n",
    "            n_classes = 10\n",
    "            label_batch = tf.one_hot(label_batch, depth = n_classes)\n",
    "\n",
    "            label_batch_list.append(tf.reshape(label_batch, [batch_size, n_classes]))\n",
    "            images_list.append(images)\n",
    "\n",
    "\n",
    "        return images_list, label_batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv(layer_name, x, out_channels, kernel_size=[5,5], stride=[1,1,1,1]):\n",
    "    '''Convolution op wrapper, use RELU activation after convolution\n",
    "    Args:\n",
    "        layer_name: e.g. conv1, pool1...\n",
    "        x: input tensor, [batch_size, height, width, channels]\n",
    "        out_channels: number of output channels (or comvolutional kernels)\n",
    "        kernel_size: the size of convolutional kernel, VGG paper used: [3,3]\n",
    "        stride: A list of ints. 1-D of length 4. VGG paper used: [1, 1, 1, 1]\n",
    "        is_pretrain: if load pretrained parameters, freeze all conv layers.\n",
    "        Depending on different situations, you can just set part of conv layers to be freezed.\n",
    "        the parameters of freezed layers will not change when training.\n",
    "    Returns:\n",
    "        4D tensor\n",
    "    '''\n",
    "\n",
    "    in_channels = x.get_shape()[-1]\n",
    "    with tf.variable_scope(layer_name):\n",
    "        w = tf.get_variable(name='weights',\n",
    "                            shape=[kernel_size[0], kernel_size[1], in_channels, out_channels],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer()) # default is uniform distribution initialization\n",
    "        b = tf.get_variable(name='biases',\n",
    "                            shape=[out_channels],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "        x = tf.nn.conv2d(x, w, stride, padding='SAME', name='conv')\n",
    "        x = tf.nn.bias_add(x, b, name='bias_add')\n",
    "        x = tf.nn.relu(x, name='relu')\n",
    "        return x\n",
    "    \n",
    "def pool(layer_name, x, kernel=[1,2,2,1], stride=[1,2,2,1]):\n",
    "    '''Pooling op\n",
    "    Args:\n",
    "        x: input tensor\n",
    "        kernel: pooling kernel, VGG paper used [1,2,2,1], the size of kernel is 2X2\n",
    "        stride: stride size, VGG paper used [1,2,2,1]\n",
    "        padding:'SAME'\n",
    "    '''\n",
    "    with tf.variable_scope(layer_name):\n",
    "        x = tf.nn.max_pool(x, kernel, strides=stride, padding='SAME', name=layer_name)\n",
    "        return x\n",
    "\n",
    "def FC_layer(layer_name, x, out_nodes):\n",
    "    '''Wrapper for fully connected layers with RELU activation as default\n",
    "    Args:\n",
    "        layer_name: e.g. 'FC1', 'FC2'\n",
    "        x: input feature map\n",
    "        out_nodes: number of neurons for current FC layer\n",
    "    '''\n",
    "    shape = x.get_shape()\n",
    "    size = shape[1].value * shape[2].value * shape[3].value\n",
    "\n",
    "    with tf.variable_scope(layer_name):\n",
    "        w = tf.get_variable('weights',\n",
    "                            shape=[size, out_nodes],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable('biases',\n",
    "                            shape=[out_nodes],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "        flat_x = tf.reshape(x, [-1, size]) # flatten into 1D\n",
    "\n",
    "        x = tf.nn.bias_add(tf.matmul(flat_x, w), b)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x\n",
    "    \n",
    "def final_layer(layer_name, x, out_nodes):\n",
    "    '''Wrapper for fully connected layers without activation function\n",
    "    Args:\n",
    "        layer_name: e.g. 'final_layer'\n",
    "        x: input feature map\n",
    "        out_nodes: number of neurons for current FC layer\n",
    "    '''\n",
    "    shape = x.get_shape()\n",
    "    size = shape[-1].value\n",
    "    with tf.variable_scope(layer_name):\n",
    "        w = tf.get_variable('weights',\n",
    "                            shape=[size, out_nodes],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable('biases',\n",
    "                            shape=[out_nodes],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "        flat_x = tf.reshape(x, [-1, size]) # flatten into 1D\n",
    "        x = tf.nn.bias_add(tf.matmul(flat_x, w), b)\n",
    "        return x\n",
    "\n",
    "def drop_out(layer_name, x, keep_prob = 0.5):\n",
    "    '''Wrapper for drop_out layer\n",
    "    Args:\n",
    "        layer_name: e.g. 'drop_out'\n",
    "        x: input feature map\n",
    "        kep_prob: the prob of a specific neuron is inactive for the next layer\n",
    "    '''    \n",
    "    with tf.variable_scope(layer_name):\n",
    "        x = tf.nn.dropout(x, keep_prob)\n",
    "        return x\n",
    "\n",
    "def lossFn(logits, labels):\n",
    "    '''Lost function\n",
    "    Args: \n",
    "        logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "        labels: Labels tensor\n",
    "    Returns: \n",
    "        loss tensor\n",
    "    '''\n",
    "    \n",
    "    with tf.variable_scope('loss') as scope:\n",
    "        labels = tf.cast(labels, tf.int64)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits\\\n",
    "                        (logits=logits, labels=labels, name='loss')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "        tf.summary.scalar(scope.name+'/loss', loss)\n",
    "    return loss\n",
    "\n",
    "def accuracyFn(logits, labels):\n",
    "    \"\"\"Evaluate the quality of the logits at predicting the label.\n",
    "    Args:\n",
    "        logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "        labels: Labels tensor\n",
    "    Returns: \n",
    "        accuracy tensor\n",
    "    \"\"\"\n",
    "    with tf.name_scope('accuracy') as scope:\n",
    "      correct = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "      correct = tf.cast(correct, tf.float32)\n",
    "      accuracy = tf.reduce_mean(correct)*100.0\n",
    "      tf.summary.scalar(scope+'/accuracy', accuracy)\n",
    "        \n",
    "    return accuracy\n",
    "\n",
    "def optimize(loss, learning_rate, global_step):\n",
    "    '''optimization use AdamOptimizer\n",
    "    Args: \n",
    "        loss: Loss tensor\n",
    "        learning_rate: e10-4 is desired \n",
    "        global_step: non trainable varibal\n",
    "    Returns: \n",
    "        train_op: train operation \n",
    "    '''\n",
    "    with tf.name_scope('optimizer'):\n",
    "        # optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameter Section "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_CLASSES = 10\n",
    "IMG_W =  32 # resize the image, if the input image is too large, training will be very slow.\n",
    "IMG_H = 32\n",
    "RATIO = 0.1 # take 20% of dataset as validation data\n",
    "BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 20\n",
    "CAPACITY = 20\n",
    "MAX_STEP = 10000 # with current parameters, it is suggested to use MAX_STEP>10k\n",
    "learning_rate = 0.0001 # with current parameters, it is suggested to use learning rate<0.0001\n",
    "\n",
    "data_dir = './data/SVHN/train_32x32.mat'\n",
    "bin_dir = './data/SVHN/'\n",
    "train_log_dir = './logs/train/'\n",
    "val_log_dir = './logs/val/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------- GCN done -------\n",
      "Step: 0, loss: 2.3326, accuracy: 9.3750%\n",
      "**  Step 0, val loss = 2.31, val accuracy = 14.06%  **\n",
      "Step: 5, loss: 2.3141, accuracy: 12.5000%\n",
      "**  Step 5, val loss = 2.30, val accuracy = 21.88%  **\n",
      "Step: 10, loss: 2.2362, accuracy: 12.5000%\n",
      "**  Step 10, val loss = 2.26, val accuracy = 12.50%  **\n",
      "Step: 15, loss: 2.2694, accuracy: 17.1875%\n",
      "**  Step 15, val loss = 2.26, val accuracy = 14.06%  **\n",
      "Step: 20, loss: 2.2241, accuracy: 25.0000%\n",
      "**  Step 20, val loss = 2.23, val accuracy = 23.44%  **\n",
      "Step: 25, loss: 2.2572, accuracy: 21.8750%\n",
      "**  Step 25, val loss = 2.25, val accuracy = 23.44%  **\n",
      "Step: 30, loss: 2.2848, accuracy: 17.1875%\n",
      "**  Step 30, val loss = 2.19, val accuracy = 17.19%  **\n",
      "Step: 35, loss: 2.3277, accuracy: 10.9375%\n",
      "**  Step 35, val loss = 2.22, val accuracy = 17.19%  **\n",
      "Step: 40, loss: 2.2416, accuracy: 17.1875%\n",
      "**  Step 40, val loss = 2.20, val accuracy = 17.19%  **\n",
      "Step: 45, loss: 2.2144, accuracy: 20.3125%\n",
      "**  Step 45, val loss = 2.16, val accuracy = 28.12%  **\n",
      "Step: 50, loss: 2.1947, accuracy: 20.3125%\n",
      "**  Step 50, val loss = 2.18, val accuracy = 29.69%  **\n",
      "Step: 55, loss: 2.2020, accuracy: 20.3125%\n",
      "**  Step 55, val loss = 2.13, val accuracy = 28.12%  **\n",
      "Step: 60, loss: 2.1855, accuracy: 21.8750%\n",
      "**  Step 60, val loss = 2.13, val accuracy = 32.81%  **\n",
      "Step: 65, loss: 2.0860, accuracy: 31.2500%\n",
      "**  Step 65, val loss = 2.13, val accuracy = 26.56%  **\n",
      "Step: 70, loss: 1.9838, accuracy: 37.5000%\n",
      "**  Step 70, val loss = 2.12, val accuracy = 29.69%  **\n",
      "Step: 75, loss: 2.1431, accuracy: 21.8750%\n",
      "**  Step 75, val loss = 2.11, val accuracy = 26.56%  **\n",
      "Step: 80, loss: 2.0546, accuracy: 31.2500%\n",
      "**  Step 80, val loss = 1.92, val accuracy = 42.19%  **\n",
      "Step: 85, loss: 2.0712, accuracy: 26.5625%\n",
      "**  Step 85, val loss = 2.13, val accuracy = 25.00%  **\n",
      "Step: 90, loss: 2.0760, accuracy: 29.6875%\n",
      "**  Step 90, val loss = 1.83, val accuracy = 37.50%  **\n",
      "Step: 95, loss: 2.0079, accuracy: 35.9375%\n",
      "**  Step 95, val loss = 2.01, val accuracy = 31.25%  **\n",
      "Step: 100, loss: 2.0484, accuracy: 32.8125%\n",
      "**  Step 100, val loss = 2.03, val accuracy = 28.12%  **\n",
      "Step: 105, loss: 2.0092, accuracy: 35.9375%\n",
      "**  Step 105, val loss = 2.10, val accuracy = 25.00%  **\n",
      "Step: 110, loss: 1.9013, accuracy: 39.0625%\n",
      "**  Step 110, val loss = 1.86, val accuracy = 39.06%  **\n",
      "Step: 115, loss: 2.0502, accuracy: 21.8750%\n",
      "**  Step 115, val loss = 1.96, val accuracy = 31.25%  **\n",
      "Step: 120, loss: 1.8638, accuracy: 37.5000%\n",
      "**  Step 120, val loss = 2.11, val accuracy = 26.56%  **\n",
      "Step: 125, loss: 2.0184, accuracy: 31.2500%\n",
      "**  Step 125, val loss = 1.91, val accuracy = 32.81%  **\n",
      "Step: 130, loss: 1.7657, accuracy: 45.3125%\n",
      "**  Step 130, val loss = 1.91, val accuracy = 34.38%  **\n",
      "Step: 135, loss: 1.8032, accuracy: 40.6250%\n",
      "**  Step 135, val loss = 1.83, val accuracy = 45.31%  **\n",
      "Step: 140, loss: 1.6427, accuracy: 42.1875%\n",
      "**  Step 140, val loss = 1.88, val accuracy = 34.38%  **\n",
      "Step: 145, loss: 1.8483, accuracy: 32.8125%\n",
      "**  Step 145, val loss = 1.63, val accuracy = 45.31%  **\n",
      "Step: 150, loss: 1.5698, accuracy: 48.4375%\n",
      "**  Step 150, val loss = 1.74, val accuracy = 39.06%  **\n",
      "Step: 155, loss: 1.7270, accuracy: 45.3125%\n",
      "**  Step 155, val loss = 1.74, val accuracy = 40.62%  **\n",
      "Step: 160, loss: 1.7042, accuracy: 32.8125%\n",
      "**  Step 160, val loss = 1.74, val accuracy = 43.75%  **\n",
      "Step: 165, loss: 1.5824, accuracy: 51.5625%\n",
      "**  Step 165, val loss = 1.82, val accuracy = 37.50%  **\n",
      "Step: 170, loss: 1.7768, accuracy: 32.8125%\n",
      "**  Step 170, val loss = 1.74, val accuracy = 43.75%  **\n",
      "Step: 175, loss: 1.7567, accuracy: 37.5000%\n",
      "**  Step 175, val loss = 1.67, val accuracy = 42.19%  **\n",
      "Step: 180, loss: 1.5798, accuracy: 53.1250%\n",
      "**  Step 180, val loss = 1.58, val accuracy = 48.44%  **\n",
      "Step: 185, loss: 1.5258, accuracy: 54.6875%\n",
      "**  Step 185, val loss = 1.78, val accuracy = 35.94%  **\n",
      "Step: 190, loss: 1.7422, accuracy: 43.7500%\n",
      "**  Step 190, val loss = 1.42, val accuracy = 51.56%  **\n",
      "Step: 195, loss: 1.7270, accuracy: 43.7500%\n",
      "**  Step 195, val loss = 1.73, val accuracy = 40.62%  **\n",
      "Step: 200, loss: 1.5945, accuracy: 46.8750%\n",
      "**  Step 200, val loss = 1.49, val accuracy = 53.12%  **\n",
      "Step: 205, loss: 1.6872, accuracy: 42.1875%\n",
      "**  Step 205, val loss = 1.56, val accuracy = 46.88%  **\n",
      "Step: 210, loss: 1.6599, accuracy: 42.1875%\n",
      "**  Step 210, val loss = 1.49, val accuracy = 57.81%  **\n",
      "Step: 215, loss: 1.4888, accuracy: 54.6875%\n",
      "**  Step 215, val loss = 1.62, val accuracy = 48.44%  **\n",
      "Step: 220, loss: 1.5618, accuracy: 48.4375%\n",
      "**  Step 220, val loss = 1.71, val accuracy = 40.62%  **\n",
      "Step: 225, loss: 1.5585, accuracy: 50.0000%\n",
      "**  Step 225, val loss = 1.64, val accuracy = 46.88%  **\n",
      "Step: 230, loss: 1.3532, accuracy: 56.2500%\n",
      "**  Step 230, val loss = 1.38, val accuracy = 53.12%  **\n",
      "Step: 235, loss: 1.4604, accuracy: 53.1250%\n",
      "**  Step 235, val loss = 1.63, val accuracy = 40.62%  **\n",
      "Step: 240, loss: 1.4551, accuracy: 51.5625%\n",
      "**  Step 240, val loss = 1.55, val accuracy = 51.56%  **\n",
      "Step: 245, loss: 1.4126, accuracy: 42.1875%\n",
      "**  Step 245, val loss = 1.41, val accuracy = 50.00%  **\n",
      "Step: 250, loss: 1.2825, accuracy: 59.3750%\n",
      "**  Step 250, val loss = 1.43, val accuracy = 51.56%  **\n",
      "Step: 255, loss: 1.3580, accuracy: 53.1250%\n",
      "**  Step 255, val loss = 1.39, val accuracy = 46.88%  **\n",
      "Step: 260, loss: 1.4982, accuracy: 46.8750%\n",
      "**  Step 260, val loss = 1.35, val accuracy = 56.25%  **\n",
      "Step: 265, loss: 1.3879, accuracy: 59.3750%\n",
      "**  Step 265, val loss = 1.19, val accuracy = 62.50%  **\n",
      "Step: 270, loss: 1.3291, accuracy: 62.5000%\n",
      "**  Step 270, val loss = 1.40, val accuracy = 45.31%  **\n",
      "Step: 275, loss: 1.4819, accuracy: 53.1250%\n",
      "**  Step 275, val loss = 1.18, val accuracy = 59.38%  **\n",
      "Step: 280, loss: 1.1800, accuracy: 57.8125%\n",
      "**  Step 280, val loss = 1.42, val accuracy = 54.69%  **\n",
      "Step: 285, loss: 1.3788, accuracy: 53.1250%\n",
      "**  Step 285, val loss = 1.52, val accuracy = 53.12%  **\n",
      "Step: 290, loss: 1.2053, accuracy: 54.6875%\n",
      "**  Step 290, val loss = 1.22, val accuracy = 60.94%  **\n",
      "Step: 295, loss: 1.4653, accuracy: 56.2500%\n",
      "**  Step 295, val loss = 1.18, val accuracy = 64.06%  **\n",
      "Step: 300, loss: 1.3391, accuracy: 50.0000%\n",
      "**  Step 300, val loss = 1.10, val accuracy = 64.06%  **\n",
      "Step: 305, loss: 1.3660, accuracy: 53.1250%\n",
      "**  Step 305, val loss = 1.35, val accuracy = 53.12%  **\n",
      "Step: 310, loss: 1.1467, accuracy: 65.6250%\n",
      "**  Step 310, val loss = 1.20, val accuracy = 64.06%  **\n",
      "Step: 315, loss: 1.1830, accuracy: 68.7500%\n",
      "**  Step 315, val loss = 1.28, val accuracy = 57.81%  **\n",
      "Step: 320, loss: 1.1639, accuracy: 64.0625%\n",
      "**  Step 320, val loss = 1.19, val accuracy = 67.19%  **\n",
      "Step: 325, loss: 1.3248, accuracy: 50.0000%\n",
      "**  Step 325, val loss = 1.28, val accuracy = 51.56%  **\n",
      "Step: 330, loss: 1.2517, accuracy: 62.5000%\n",
      "**  Step 330, val loss = 1.13, val accuracy = 57.81%  **\n",
      "Step: 335, loss: 1.4153, accuracy: 51.5625%\n",
      "**  Step 335, val loss = 1.09, val accuracy = 65.62%  **\n",
      "Step: 340, loss: 1.1480, accuracy: 65.6250%\n",
      "**  Step 340, val loss = 1.21, val accuracy = 62.50%  **\n",
      "Step: 345, loss: 1.3410, accuracy: 50.0000%\n",
      "**  Step 345, val loss = 1.29, val accuracy = 56.25%  **\n",
      "Step: 350, loss: 1.1530, accuracy: 57.8125%\n",
      "**  Step 350, val loss = 1.11, val accuracy = 64.06%  **\n",
      "Step: 355, loss: 1.1527, accuracy: 57.8125%\n",
      "**  Step 355, val loss = 1.30, val accuracy = 60.94%  **\n",
      "Step: 360, loss: 1.1959, accuracy: 59.3750%\n",
      "**  Step 360, val loss = 1.30, val accuracy = 50.00%  **\n",
      "Step: 365, loss: 1.2099, accuracy: 59.3750%\n",
      "**  Step 365, val loss = 1.31, val accuracy = 51.56%  **\n",
      "Step: 370, loss: 1.1535, accuracy: 60.9375%\n",
      "**  Step 370, val loss = 1.04, val accuracy = 64.06%  **\n",
      "Step: 375, loss: 0.9719, accuracy: 70.3125%\n",
      "**  Step 375, val loss = 1.33, val accuracy = 57.81%  **\n",
      "Step: 380, loss: 1.0790, accuracy: 68.7500%\n",
      "**  Step 380, val loss = 0.98, val accuracy = 67.19%  **\n",
      "Step: 385, loss: 1.2925, accuracy: 57.8125%\n",
      "**  Step 385, val loss = 1.44, val accuracy = 46.88%  **\n",
      "Step: 390, loss: 1.2295, accuracy: 60.9375%\n",
      "**  Step 390, val loss = 1.16, val accuracy = 64.06%  **\n",
      "Step: 395, loss: 1.3934, accuracy: 59.3750%\n",
      "**  Step 395, val loss = 1.24, val accuracy = 53.12%  **\n",
      "Step: 400, loss: 1.1016, accuracy: 70.3125%\n",
      "**  Step 400, val loss = 1.29, val accuracy = 57.81%  **\n",
      "Step: 405, loss: 1.3420, accuracy: 56.2500%\n",
      "**  Step 405, val loss = 1.04, val accuracy = 65.62%  **\n",
      "Step: 410, loss: 1.2784, accuracy: 62.5000%\n",
      "**  Step 410, val loss = 1.06, val accuracy = 68.75%  **\n",
      "Step: 415, loss: 1.1626, accuracy: 60.9375%\n",
      "**  Step 415, val loss = 1.18, val accuracy = 53.12%  **\n",
      "Step: 420, loss: 1.0942, accuracy: 65.6250%\n",
      "**  Step 420, val loss = 1.12, val accuracy = 59.38%  **\n",
      "Step: 425, loss: 1.0480, accuracy: 64.0625%\n",
      "**  Step 425, val loss = 1.11, val accuracy = 65.62%  **\n",
      "Step: 430, loss: 1.1509, accuracy: 56.2500%\n",
      "**  Step 430, val loss = 1.21, val accuracy = 59.38%  **\n",
      "Step: 435, loss: 0.8743, accuracy: 75.0000%\n",
      "**  Step 435, val loss = 1.14, val accuracy = 64.06%  **\n",
      "Step: 440, loss: 1.4744, accuracy: 48.4375%\n",
      "**  Step 440, val loss = 1.09, val accuracy = 64.06%  **\n",
      "Step: 445, loss: 0.9035, accuracy: 71.8750%\n",
      "**  Step 445, val loss = 1.09, val accuracy = 62.50%  **\n",
      "Step: 450, loss: 1.2030, accuracy: 60.9375%\n",
      "**  Step 450, val loss = 0.87, val accuracy = 76.56%  **\n",
      "Step: 455, loss: 0.9778, accuracy: 70.3125%\n",
      "**  Step 455, val loss = 1.20, val accuracy = 56.25%  **\n",
      "Step: 460, loss: 1.1969, accuracy: 53.1250%\n",
      "**  Step 460, val loss = 1.28, val accuracy = 51.56%  **\n",
      "Step: 465, loss: 0.9368, accuracy: 73.4375%\n",
      "**  Step 465, val loss = 1.07, val accuracy = 71.88%  **\n",
      "Step: 470, loss: 0.8392, accuracy: 68.7500%\n",
      "**  Step 470, val loss = 1.00, val accuracy = 67.19%  **\n",
      "Step: 475, loss: 1.0157, accuracy: 60.9375%\n",
      "**  Step 475, val loss = 1.04, val accuracy = 67.19%  **\n",
      "Step: 480, loss: 0.8791, accuracy: 71.8750%\n",
      "**  Step 480, val loss = 1.09, val accuracy = 60.94%  **\n",
      "Step: 485, loss: 1.0553, accuracy: 67.1875%\n",
      "**  Step 485, val loss = 0.91, val accuracy = 70.31%  **\n",
      "Step: 490, loss: 1.1393, accuracy: 62.5000%\n",
      "**  Step 490, val loss = 0.97, val accuracy = 71.88%  **\n",
      "Step: 495, loss: 0.7440, accuracy: 82.8125%\n",
      "**  Step 495, val loss = 1.25, val accuracy = 60.94%  **\n",
      "Step: 500, loss: 1.0272, accuracy: 67.1875%\n",
      "**  Step 500, val loss = 0.80, val accuracy = 75.00%  **\n",
      "Step: 505, loss: 0.8385, accuracy: 73.4375%\n",
      "**  Step 505, val loss = 0.95, val accuracy = 67.19%  **\n",
      "Step: 510, loss: 0.8697, accuracy: 76.5625%\n",
      "**  Step 510, val loss = 1.04, val accuracy = 67.19%  **\n",
      "Step: 515, loss: 0.9634, accuracy: 71.8750%\n",
      "**  Step 515, val loss = 1.09, val accuracy = 64.06%  **\n",
      "Step: 520, loss: 1.2483, accuracy: 59.3750%\n",
      "**  Step 520, val loss = 0.98, val accuracy = 78.12%  **\n",
      "Step: 525, loss: 1.0137, accuracy: 62.5000%\n",
      "**  Step 525, val loss = 0.85, val accuracy = 70.31%  **\n",
      "Step: 530, loss: 0.9371, accuracy: 70.3125%\n",
      "**  Step 530, val loss = 0.92, val accuracy = 65.62%  **\n",
      "Step: 535, loss: 1.1600, accuracy: 56.2500%\n",
      "**  Step 535, val loss = 0.95, val accuracy = 67.19%  **\n",
      "Step: 540, loss: 0.8256, accuracy: 70.3125%\n",
      "**  Step 540, val loss = 0.87, val accuracy = 70.31%  **\n",
      "Step: 545, loss: 0.9143, accuracy: 68.7500%\n",
      "**  Step 545, val loss = 0.95, val accuracy = 71.88%  **\n",
      "Step: 550, loss: 0.8110, accuracy: 75.0000%\n",
      "**  Step 550, val loss = 1.11, val accuracy = 65.62%  **\n",
      "Step: 555, loss: 0.7539, accuracy: 75.0000%\n",
      "**  Step 555, val loss = 1.17, val accuracy = 57.81%  **\n",
      "Step: 560, loss: 0.8749, accuracy: 79.6875%\n",
      "**  Step 560, val loss = 0.98, val accuracy = 68.75%  **\n",
      "Step: 565, loss: 0.8437, accuracy: 70.3125%\n",
      "**  Step 565, val loss = 1.03, val accuracy = 68.75%  **\n",
      "Step: 570, loss: 0.8834, accuracy: 71.8750%\n",
      "**  Step 570, val loss = 0.74, val accuracy = 79.69%  **\n",
      "Step: 575, loss: 0.7872, accuracy: 76.5625%\n",
      "**  Step 575, val loss = 1.02, val accuracy = 71.88%  **\n",
      "Step: 580, loss: 1.1239, accuracy: 62.5000%\n",
      "**  Step 580, val loss = 0.95, val accuracy = 76.56%  **\n",
      "Step: 585, loss: 0.9399, accuracy: 68.7500%\n",
      "**  Step 585, val loss = 0.91, val accuracy = 60.94%  **\n",
      "Step: 590, loss: 0.7487, accuracy: 71.8750%\n",
      "**  Step 590, val loss = 1.09, val accuracy = 70.31%  **\n",
      "Step: 595, loss: 0.9797, accuracy: 68.7500%\n",
      "**  Step 595, val loss = 1.12, val accuracy = 65.62%  **\n",
      "Step: 600, loss: 0.9154, accuracy: 65.6250%\n",
      "**  Step 600, val loss = 1.04, val accuracy = 65.62%  **\n",
      "Step: 605, loss: 0.5793, accuracy: 87.5000%\n",
      "**  Step 605, val loss = 0.74, val accuracy = 76.56%  **\n",
      "Step: 610, loss: 0.8985, accuracy: 70.3125%\n",
      "**  Step 610, val loss = 0.80, val accuracy = 71.88%  **\n",
      "Step: 615, loss: 0.8558, accuracy: 75.0000%\n",
      "**  Step 615, val loss = 1.15, val accuracy = 65.62%  **\n",
      "Step: 620, loss: 1.0133, accuracy: 68.7500%\n",
      "**  Step 620, val loss = 1.07, val accuracy = 64.06%  **\n",
      "Step: 625, loss: 0.7283, accuracy: 78.1250%\n",
      "**  Step 625, val loss = 0.85, val accuracy = 79.69%  **\n",
      "Step: 630, loss: 1.1717, accuracy: 60.9375%\n",
      "**  Step 630, val loss = 0.96, val accuracy = 73.44%  **\n",
      "Step: 635, loss: 1.1446, accuracy: 68.7500%\n",
      "**  Step 635, val loss = 0.88, val accuracy = 75.00%  **\n",
      "Step: 640, loss: 0.9651, accuracy: 67.1875%\n",
      "**  Step 640, val loss = 0.91, val accuracy = 70.31%  **\n",
      "Step: 645, loss: 0.8614, accuracy: 70.3125%\n",
      "**  Step 645, val loss = 0.87, val accuracy = 65.62%  **\n",
      "Step: 650, loss: 0.7036, accuracy: 81.2500%\n",
      "**  Step 650, val loss = 0.85, val accuracy = 75.00%  **\n",
      "Step: 655, loss: 0.9298, accuracy: 68.7500%\n",
      "**  Step 655, val loss = 0.86, val accuracy = 78.12%  **\n",
      "Step: 660, loss: 0.9118, accuracy: 68.7500%\n",
      "**  Step 660, val loss = 1.03, val accuracy = 68.75%  **\n",
      "Step: 665, loss: 0.9151, accuracy: 68.7500%\n",
      "**  Step 665, val loss = 0.72, val accuracy = 76.56%  **\n",
      "Step: 670, loss: 0.7126, accuracy: 75.0000%\n",
      "**  Step 670, val loss = 0.80, val accuracy = 76.56%  **\n",
      "Step: 675, loss: 0.7954, accuracy: 78.1250%\n",
      "**  Step 675, val loss = 0.94, val accuracy = 70.31%  **\n",
      "Step: 680, loss: 0.7391, accuracy: 76.5625%\n",
      "**  Step 680, val loss = 0.71, val accuracy = 73.44%  **\n",
      "Step: 685, loss: 1.1870, accuracy: 65.6250%\n",
      "**  Step 685, val loss = 0.94, val accuracy = 75.00%  **\n",
      "Step: 690, loss: 0.5215, accuracy: 92.1875%\n",
      "**  Step 690, val loss = 0.95, val accuracy = 67.19%  **\n",
      "Step: 695, loss: 0.9519, accuracy: 68.7500%\n",
      "**  Step 695, val loss = 1.03, val accuracy = 62.50%  **\n",
      "Step: 700, loss: 0.7922, accuracy: 76.5625%\n",
      "**  Step 700, val loss = 0.99, val accuracy = 70.31%  **\n",
      "Step: 705, loss: 0.7225, accuracy: 71.8750%\n",
      "**  Step 705, val loss = 0.94, val accuracy = 70.31%  **\n",
      "Step: 710, loss: 0.6643, accuracy: 82.8125%\n",
      "**  Step 710, val loss = 0.70, val accuracy = 81.25%  **\n",
      "Step: 715, loss: 0.7302, accuracy: 79.6875%\n",
      "**  Step 715, val loss = 0.75, val accuracy = 79.69%  **\n",
      "Step: 720, loss: 1.1401, accuracy: 62.5000%\n",
      "**  Step 720, val loss = 0.65, val accuracy = 81.25%  **\n",
      "Step: 725, loss: 0.8108, accuracy: 78.1250%\n",
      "**  Step 725, val loss = 0.89, val accuracy = 65.62%  **\n",
      "Step: 730, loss: 0.8014, accuracy: 78.1250%\n",
      "**  Step 730, val loss = 0.81, val accuracy = 76.56%  **\n",
      "Step: 735, loss: 0.6252, accuracy: 84.3750%\n",
      "**  Step 735, val loss = 0.88, val accuracy = 68.75%  **\n",
      "Step: 740, loss: 0.8886, accuracy: 65.6250%\n",
      "**  Step 740, val loss = 0.59, val accuracy = 81.25%  **\n",
      "Step: 745, loss: 0.8118, accuracy: 78.1250%\n",
      "**  Step 745, val loss = 0.75, val accuracy = 75.00%  **\n",
      "Step: 750, loss: 0.9255, accuracy: 78.1250%\n",
      "**  Step 750, val loss = 1.01, val accuracy = 71.88%  **\n",
      "Step: 755, loss: 0.7141, accuracy: 81.2500%\n",
      "**  Step 755, val loss = 0.93, val accuracy = 70.31%  **\n",
      "Step: 760, loss: 0.7253, accuracy: 76.5625%\n",
      "**  Step 760, val loss = 0.85, val accuracy = 73.44%  **\n",
      "Step: 765, loss: 0.6954, accuracy: 81.2500%\n",
      "**  Step 765, val loss = 0.91, val accuracy = 71.88%  **\n",
      "Step: 770, loss: 1.0092, accuracy: 68.7500%\n",
      "**  Step 770, val loss = 0.53, val accuracy = 89.06%  **\n",
      "Step: 775, loss: 1.0448, accuracy: 68.7500%\n",
      "**  Step 775, val loss = 0.71, val accuracy = 79.69%  **\n",
      "Step: 780, loss: 1.0523, accuracy: 71.8750%\n",
      "**  Step 780, val loss = 0.86, val accuracy = 67.19%  **\n",
      "Step: 785, loss: 0.6250, accuracy: 78.1250%\n",
      "**  Step 785, val loss = 0.85, val accuracy = 75.00%  **\n",
      "Step: 790, loss: 0.8138, accuracy: 67.1875%\n",
      "**  Step 790, val loss = 0.97, val accuracy = 67.19%  **\n",
      "Step: 795, loss: 0.5899, accuracy: 81.2500%\n",
      "**  Step 795, val loss = 1.35, val accuracy = 62.50%  **\n",
      "Step: 800, loss: 0.7235, accuracy: 76.5625%\n",
      "**  Step 800, val loss = 0.97, val accuracy = 68.75%  **\n",
      "Step: 805, loss: 0.7201, accuracy: 76.5625%\n",
      "**  Step 805, val loss = 0.69, val accuracy = 76.56%  **\n",
      "Step: 810, loss: 0.9122, accuracy: 73.4375%\n",
      "**  Step 810, val loss = 1.08, val accuracy = 62.50%  **\n",
      "Step: 815, loss: 0.8159, accuracy: 75.0000%\n",
      "**  Step 815, val loss = 0.91, val accuracy = 68.75%  **\n",
      "Step: 820, loss: 0.8500, accuracy: 73.4375%\n",
      "**  Step 820, val loss = 0.88, val accuracy = 67.19%  **\n",
      "Step: 825, loss: 0.7946, accuracy: 70.3125%\n",
      "**  Step 825, val loss = 1.07, val accuracy = 75.00%  **\n",
      "Step: 830, loss: 0.7358, accuracy: 76.5625%\n",
      "**  Step 830, val loss = 0.79, val accuracy = 71.88%  **\n",
      "Step: 835, loss: 0.8038, accuracy: 73.4375%\n",
      "**  Step 835, val loss = 0.81, val accuracy = 82.81%  **\n",
      "Step: 840, loss: 0.6511, accuracy: 81.2500%\n",
      "**  Step 840, val loss = 0.98, val accuracy = 67.19%  **\n",
      "Step: 845, loss: 0.7553, accuracy: 71.8750%\n",
      "**  Step 845, val loss = 0.94, val accuracy = 75.00%  **\n",
      "Step: 850, loss: 0.6377, accuracy: 85.9375%\n",
      "**  Step 850, val loss = 0.68, val accuracy = 73.44%  **\n",
      "Step: 855, loss: 0.5819, accuracy: 79.6875%\n",
      "**  Step 855, val loss = 0.63, val accuracy = 81.25%  **\n",
      "Step: 860, loss: 0.6398, accuracy: 81.2500%\n",
      "**  Step 860, val loss = 0.76, val accuracy = 73.44%  **\n",
      "Step: 865, loss: 0.8731, accuracy: 71.8750%\n",
      "**  Step 865, val loss = 0.73, val accuracy = 79.69%  **\n",
      "Step: 870, loss: 0.7753, accuracy: 75.0000%\n",
      "**  Step 870, val loss = 1.09, val accuracy = 68.75%  **\n",
      "Step: 875, loss: 0.8436, accuracy: 68.7500%\n",
      "**  Step 875, val loss = 0.89, val accuracy = 70.31%  **\n",
      "Step: 880, loss: 0.5338, accuracy: 79.6875%\n",
      "**  Step 880, val loss = 0.67, val accuracy = 75.00%  **\n",
      "Step: 885, loss: 0.8212, accuracy: 73.4375%\n",
      "**  Step 885, val loss = 0.94, val accuracy = 70.31%  **\n",
      "Step: 890, loss: 0.5982, accuracy: 81.2500%\n",
      "**  Step 890, val loss = 0.79, val accuracy = 76.56%  **\n",
      "Step: 895, loss: 0.5853, accuracy: 79.6875%\n",
      "**  Step 895, val loss = 0.88, val accuracy = 70.31%  **\n",
      "Step: 900, loss: 0.7174, accuracy: 76.5625%\n",
      "**  Step 900, val loss = 0.93, val accuracy = 70.31%  **\n",
      "Step: 905, loss: 0.7021, accuracy: 76.5625%\n",
      "**  Step 905, val loss = 0.85, val accuracy = 79.69%  **\n",
      "Step: 910, loss: 0.9259, accuracy: 71.8750%\n",
      "**  Step 910, val loss = 0.96, val accuracy = 65.62%  **\n",
      "Step: 915, loss: 0.8855, accuracy: 71.8750%\n",
      "**  Step 915, val loss = 0.72, val accuracy = 75.00%  **\n",
      "Step: 920, loss: 0.6373, accuracy: 71.8750%\n",
      "**  Step 920, val loss = 0.82, val accuracy = 70.31%  **\n",
      "Step: 925, loss: 0.7241, accuracy: 79.6875%\n",
      "**  Step 925, val loss = 0.95, val accuracy = 73.44%  **\n",
      "Step: 930, loss: 0.5653, accuracy: 85.9375%\n",
      "**  Step 930, val loss = 0.85, val accuracy = 71.88%  **\n",
      "Step: 935, loss: 0.5819, accuracy: 81.2500%\n",
      "**  Step 935, val loss = 0.71, val accuracy = 76.56%  **\n",
      "Step: 940, loss: 0.6287, accuracy: 81.2500%\n",
      "**  Step 940, val loss = 0.99, val accuracy = 68.75%  **\n",
      "Step: 945, loss: 0.8214, accuracy: 75.0000%\n",
      "**  Step 945, val loss = 0.87, val accuracy = 73.44%  **\n",
      "Step: 950, loss: 0.6898, accuracy: 78.1250%\n",
      "**  Step 950, val loss = 0.90, val accuracy = 71.88%  **\n",
      "Step: 955, loss: 0.6187, accuracy: 84.3750%\n",
      "**  Step 955, val loss = 0.82, val accuracy = 68.75%  **\n",
      "Step: 960, loss: 0.6051, accuracy: 76.5625%\n",
      "**  Step 960, val loss = 1.08, val accuracy = 71.88%  **\n",
      "Step: 965, loss: 0.5014, accuracy: 89.0625%\n",
      "**  Step 965, val loss = 0.61, val accuracy = 85.94%  **\n",
      "Step: 970, loss: 0.5084, accuracy: 84.3750%\n",
      "**  Step 970, val loss = 0.78, val accuracy = 73.44%  **\n",
      "Step: 975, loss: 0.8506, accuracy: 76.5625%\n",
      "**  Step 975, val loss = 0.76, val accuracy = 75.00%  **\n",
      "Step: 980, loss: 0.8030, accuracy: 70.3125%\n",
      "**  Step 980, val loss = 0.89, val accuracy = 70.31%  **\n",
      "Step: 985, loss: 0.8845, accuracy: 76.5625%\n",
      "**  Step 985, val loss = 0.87, val accuracy = 78.12%  **\n",
      "Step: 990, loss: 0.7444, accuracy: 78.1250%\n",
      "**  Step 990, val loss = 0.76, val accuracy = 76.56%  **\n",
      "Step: 995, loss: 0.7350, accuracy: 78.1250%\n",
      "**  Step 995, val loss = 0.86, val accuracy = 78.12%  **\n",
      "Step: 1000, loss: 0.5390, accuracy: 84.3750%\n",
      "**  Step 1000, val loss = 0.55, val accuracy = 76.56%  **\n",
      "Step: 1005, loss: 0.5576, accuracy: 84.3750%\n",
      "**  Step 1005, val loss = 0.94, val accuracy = 70.31%  **\n",
      "Step: 1010, loss: 0.7397, accuracy: 75.0000%\n",
      "**  Step 1010, val loss = 0.77, val accuracy = 79.69%  **\n",
      "Step: 1015, loss: 0.8007, accuracy: 70.3125%\n",
      "**  Step 1015, val loss = 0.80, val accuracy = 65.62%  **\n",
      "Step: 1020, loss: 0.6460, accuracy: 78.1250%\n",
      "**  Step 1020, val loss = 0.79, val accuracy = 73.44%  **\n",
      "Step: 1025, loss: 0.7682, accuracy: 75.0000%\n",
      "**  Step 1025, val loss = 0.64, val accuracy = 76.56%  **\n",
      "Step: 1030, loss: 0.6705, accuracy: 75.0000%\n",
      "**  Step 1030, val loss = 0.81, val accuracy = 75.00%  **\n",
      "Step: 1035, loss: 0.5478, accuracy: 81.2500%\n",
      "**  Step 1035, val loss = 0.67, val accuracy = 81.25%  **\n",
      "Step: 1040, loss: 0.6283, accuracy: 76.5625%\n",
      "**  Step 1040, val loss = 0.82, val accuracy = 70.31%  **\n",
      "Step: 1045, loss: 0.6793, accuracy: 79.6875%\n",
      "**  Step 1045, val loss = 0.70, val accuracy = 76.56%  **\n",
      "Step: 1050, loss: 0.7547, accuracy: 76.5625%\n",
      "**  Step 1050, val loss = 0.74, val accuracy = 79.69%  **\n",
      "Step: 1055, loss: 0.7134, accuracy: 76.5625%\n",
      "**  Step 1055, val loss = 0.74, val accuracy = 73.44%  **\n",
      "Step: 1060, loss: 0.5725, accuracy: 82.8125%\n",
      "**  Step 1060, val loss = 0.85, val accuracy = 73.44%  **\n",
      "Step: 1065, loss: 0.4877, accuracy: 87.5000%\n",
      "**  Step 1065, val loss = 1.03, val accuracy = 75.00%  **\n",
      "Step: 1070, loss: 0.8444, accuracy: 75.0000%\n",
      "**  Step 1070, val loss = 0.94, val accuracy = 71.88%  **\n",
      "Step: 1075, loss: 0.7085, accuracy: 73.4375%\n",
      "**  Step 1075, val loss = 0.81, val accuracy = 71.88%  **\n",
      "Step: 1080, loss: 0.7918, accuracy: 76.5625%\n",
      "**  Step 1080, val loss = 0.68, val accuracy = 79.69%  **\n",
      "Step: 1085, loss: 0.6535, accuracy: 76.5625%\n",
      "**  Step 1085, val loss = 0.85, val accuracy = 78.12%  **\n",
      "Step: 1090, loss: 0.5973, accuracy: 84.3750%\n",
      "**  Step 1090, val loss = 0.67, val accuracy = 81.25%  **\n",
      "Step: 1095, loss: 1.0440, accuracy: 67.1875%\n",
      "**  Step 1095, val loss = 0.88, val accuracy = 71.88%  **\n",
      "Step: 1100, loss: 0.7021, accuracy: 79.6875%\n",
      "**  Step 1100, val loss = 1.20, val accuracy = 62.50%  **\n",
      "Step: 1105, loss: 0.6321, accuracy: 79.6875%\n",
      "**  Step 1105, val loss = 0.92, val accuracy = 75.00%  **\n",
      "Step: 1110, loss: 0.6634, accuracy: 73.4375%\n",
      "**  Step 1110, val loss = 0.79, val accuracy = 78.12%  **\n",
      "Step: 1115, loss: 0.5173, accuracy: 82.8125%\n",
      "**  Step 1115, val loss = 0.83, val accuracy = 73.44%  **\n",
      "Step: 1120, loss: 0.6472, accuracy: 75.0000%\n",
      "**  Step 1120, val loss = 0.86, val accuracy = 73.44%  **\n",
      "Step: 1125, loss: 0.5938, accuracy: 81.2500%\n",
      "**  Step 1125, val loss = 0.74, val accuracy = 71.88%  **\n",
      "Step: 1130, loss: 0.5680, accuracy: 82.8125%\n",
      "**  Step 1130, val loss = 0.65, val accuracy = 79.69%  **\n",
      "Step: 1135, loss: 0.7142, accuracy: 73.4375%\n",
      "**  Step 1135, val loss = 1.18, val accuracy = 75.00%  **\n",
      "Step: 1140, loss: 0.6256, accuracy: 76.5625%\n",
      "**  Step 1140, val loss = 0.92, val accuracy = 67.19%  **\n",
      "Step: 1145, loss: 0.6023, accuracy: 84.3750%\n",
      "**  Step 1145, val loss = 0.57, val accuracy = 84.38%  **\n",
      "Step: 1150, loss: 0.6980, accuracy: 75.0000%\n",
      "**  Step 1150, val loss = 0.88, val accuracy = 75.00%  **\n",
      "Step: 1155, loss: 0.6706, accuracy: 79.6875%\n",
      "**  Step 1155, val loss = 0.60, val accuracy = 81.25%  **\n",
      "Step: 1160, loss: 0.5501, accuracy: 81.2500%\n",
      "**  Step 1160, val loss = 0.87, val accuracy = 79.69%  **\n",
      "Step: 1165, loss: 0.4827, accuracy: 85.9375%\n",
      "**  Step 1165, val loss = 0.82, val accuracy = 68.75%  **\n",
      "Step: 1170, loss: 0.6081, accuracy: 81.2500%\n",
      "**  Step 1170, val loss = 0.46, val accuracy = 82.81%  **\n",
      "Step: 1175, loss: 0.7464, accuracy: 73.4375%\n",
      "**  Step 1175, val loss = 0.70, val accuracy = 79.69%  **\n",
      "Step: 1180, loss: 0.6572, accuracy: 79.6875%\n",
      "**  Step 1180, val loss = 0.98, val accuracy = 68.75%  **\n",
      "Step: 1185, loss: 0.3768, accuracy: 90.6250%\n",
      "**  Step 1185, val loss = 0.73, val accuracy = 78.12%  **\n",
      "Step: 1190, loss: 0.6709, accuracy: 76.5625%\n",
      "**  Step 1190, val loss = 0.73, val accuracy = 75.00%  **\n",
      "Step: 1195, loss: 0.6851, accuracy: 75.0000%\n",
      "**  Step 1195, val loss = 0.51, val accuracy = 85.94%  **\n",
      "Step: 1200, loss: 0.4691, accuracy: 85.9375%\n",
      "**  Step 1200, val loss = 0.83, val accuracy = 71.88%  **\n",
      "Step: 1205, loss: 0.6592, accuracy: 76.5625%\n",
      "**  Step 1205, val loss = 0.73, val accuracy = 75.00%  **\n",
      "Step: 1210, loss: 0.4777, accuracy: 87.5000%\n",
      "**  Step 1210, val loss = 0.67, val accuracy = 81.25%  **\n",
      "Step: 1215, loss: 0.6185, accuracy: 79.6875%\n",
      "**  Step 1215, val loss = 0.94, val accuracy = 67.19%  **\n",
      "Step: 1220, loss: 0.5759, accuracy: 78.1250%\n",
      "**  Step 1220, val loss = 0.71, val accuracy = 75.00%  **\n",
      "Step: 1225, loss: 0.4240, accuracy: 89.0625%\n",
      "**  Step 1225, val loss = 0.78, val accuracy = 75.00%  **\n",
      "Step: 1230, loss: 0.6174, accuracy: 79.6875%\n",
      "**  Step 1230, val loss = 0.65, val accuracy = 85.94%  **\n",
      "Step: 1235, loss: 0.5497, accuracy: 79.6875%\n",
      "**  Step 1235, val loss = 0.70, val accuracy = 79.69%  **\n",
      "Step: 1240, loss: 0.6917, accuracy: 81.2500%\n",
      "**  Step 1240, val loss = 0.71, val accuracy = 78.12%  **\n",
      "Step: 1245, loss: 0.6390, accuracy: 78.1250%\n",
      "**  Step 1245, val loss = 0.86, val accuracy = 73.44%  **\n",
      "Step: 1250, loss: 0.6196, accuracy: 76.5625%\n",
      "**  Step 1250, val loss = 0.66, val accuracy = 84.38%  **\n",
      "Step: 1255, loss: 0.6444, accuracy: 79.6875%\n",
      "**  Step 1255, val loss = 0.90, val accuracy = 79.69%  **\n",
      "Step: 1260, loss: 0.4585, accuracy: 87.5000%\n",
      "**  Step 1260, val loss = 0.77, val accuracy = 67.19%  **\n",
      "Step: 1265, loss: 0.5026, accuracy: 87.5000%\n",
      "**  Step 1265, val loss = 0.94, val accuracy = 68.75%  **\n",
      "Step: 1270, loss: 0.6835, accuracy: 76.5625%\n",
      "**  Step 1270, val loss = 0.61, val accuracy = 81.25%  **\n",
      "Step: 1275, loss: 0.3657, accuracy: 87.5000%\n",
      "**  Step 1275, val loss = 0.82, val accuracy = 75.00%  **\n",
      "Step: 1280, loss: 0.6399, accuracy: 79.6875%\n",
      "**  Step 1280, val loss = 0.84, val accuracy = 73.44%  **\n",
      "Step: 1285, loss: 0.5882, accuracy: 81.2500%\n",
      "**  Step 1285, val loss = 1.09, val accuracy = 73.44%  **\n",
      "Step: 1290, loss: 0.4562, accuracy: 85.9375%\n",
      "**  Step 1290, val loss = 0.63, val accuracy = 79.69%  **\n",
      "Step: 1295, loss: 0.5761, accuracy: 81.2500%\n",
      "**  Step 1295, val loss = 0.61, val accuracy = 82.81%  **\n",
      "Step: 1300, loss: 0.4987, accuracy: 79.6875%\n",
      "**  Step 1300, val loss = 0.73, val accuracy = 79.69%  **\n",
      "Step: 1305, loss: 0.6822, accuracy: 78.1250%\n",
      "**  Step 1305, val loss = 0.65, val accuracy = 75.00%  **\n",
      "Step: 1310, loss: 0.4432, accuracy: 85.9375%\n",
      "**  Step 1310, val loss = 0.54, val accuracy = 78.12%  **\n",
      "Step: 1315, loss: 0.5873, accuracy: 79.6875%\n",
      "**  Step 1315, val loss = 0.72, val accuracy = 76.56%  **\n",
      "Step: 1320, loss: 0.5534, accuracy: 82.8125%\n",
      "**  Step 1320, val loss = 0.69, val accuracy = 78.12%  **\n",
      "Step: 1325, loss: 0.7418, accuracy: 76.5625%\n",
      "**  Step 1325, val loss = 0.69, val accuracy = 75.00%  **\n",
      "Step: 1330, loss: 0.5382, accuracy: 81.2500%\n",
      "**  Step 1330, val loss = 0.59, val accuracy = 79.69%  **\n",
      "Step: 1335, loss: 0.5790, accuracy: 82.8125%\n",
      "**  Step 1335, val loss = 0.46, val accuracy = 81.25%  **\n",
      "Step: 1340, loss: 0.4066, accuracy: 87.5000%\n",
      "**  Step 1340, val loss = 0.61, val accuracy = 75.00%  **\n",
      "Step: 1345, loss: 0.4520, accuracy: 82.8125%\n",
      "**  Step 1345, val loss = 0.79, val accuracy = 75.00%  **\n",
      "Step: 1350, loss: 0.5686, accuracy: 76.5625%\n",
      "**  Step 1350, val loss = 0.68, val accuracy = 82.81%  **\n",
      "Step: 1355, loss: 0.6443, accuracy: 76.5625%\n",
      "**  Step 1355, val loss = 0.42, val accuracy = 89.06%  **\n",
      "Step: 1360, loss: 0.6002, accuracy: 84.3750%\n",
      "**  Step 1360, val loss = 0.88, val accuracy = 73.44%  **\n",
      "Step: 1365, loss: 0.5962, accuracy: 78.1250%\n",
      "**  Step 1365, val loss = 0.88, val accuracy = 81.25%  **\n",
      "Step: 1370, loss: 0.6196, accuracy: 84.3750%\n",
      "**  Step 1370, val loss = 0.82, val accuracy = 78.12%  **\n",
      "Step: 1375, loss: 0.6500, accuracy: 78.1250%\n",
      "**  Step 1375, val loss = 0.74, val accuracy = 76.56%  **\n",
      "Step: 1380, loss: 0.6239, accuracy: 84.3750%\n",
      "**  Step 1380, val loss = 0.65, val accuracy = 81.25%  **\n",
      "Step: 1385, loss: 0.6712, accuracy: 78.1250%\n",
      "**  Step 1385, val loss = 0.74, val accuracy = 76.56%  **\n",
      "Step: 1390, loss: 0.5939, accuracy: 81.2500%\n",
      "**  Step 1390, val loss = 0.71, val accuracy = 81.25%  **\n",
      "Step: 1395, loss: 0.4731, accuracy: 84.3750%\n",
      "**  Step 1395, val loss = 0.78, val accuracy = 73.44%  **\n",
      "Step: 1400, loss: 0.5877, accuracy: 81.2500%\n",
      "**  Step 1400, val loss = 0.67, val accuracy = 78.12%  **\n",
      "Step: 1405, loss: 0.5434, accuracy: 84.3750%\n",
      "**  Step 1405, val loss = 0.74, val accuracy = 82.81%  **\n",
      "Step: 1410, loss: 0.4530, accuracy: 87.5000%\n",
      "**  Step 1410, val loss = 0.84, val accuracy = 68.75%  **\n",
      "Step: 1415, loss: 0.4778, accuracy: 82.8125%\n",
      "**  Step 1415, val loss = 0.71, val accuracy = 78.12%  **\n",
      "Step: 1420, loss: 0.6728, accuracy: 78.1250%\n",
      "**  Step 1420, val loss = 0.81, val accuracy = 78.12%  **\n",
      "Step: 1425, loss: 0.4418, accuracy: 82.8125%\n",
      "**  Step 1425, val loss = 0.89, val accuracy = 71.88%  **\n",
      "Step: 1430, loss: 0.3840, accuracy: 84.3750%\n",
      "**  Step 1430, val loss = 0.55, val accuracy = 84.38%  **\n",
      "Step: 1435, loss: 0.2871, accuracy: 92.1875%\n",
      "**  Step 1435, val loss = 0.74, val accuracy = 78.12%  **\n",
      "Step: 1440, loss: 0.6077, accuracy: 79.6875%\n",
      "**  Step 1440, val loss = 0.67, val accuracy = 79.69%  **\n",
      "Step: 1445, loss: 0.5876, accuracy: 79.6875%\n",
      "**  Step 1445, val loss = 0.93, val accuracy = 64.06%  **\n",
      "Step: 1450, loss: 0.3797, accuracy: 89.0625%\n",
      "**  Step 1450, val loss = 0.77, val accuracy = 79.69%  **\n",
      "Step: 1455, loss: 0.4400, accuracy: 89.0625%\n",
      "**  Step 1455, val loss = 0.66, val accuracy = 81.25%  **\n",
      "Step: 1460, loss: 0.3928, accuracy: 90.6250%\n",
      "**  Step 1460, val loss = 0.59, val accuracy = 78.12%  **\n",
      "Step: 1465, loss: 0.6406, accuracy: 84.3750%\n",
      "**  Step 1465, val loss = 0.54, val accuracy = 82.81%  **\n",
      "Step: 1470, loss: 0.4576, accuracy: 84.3750%\n",
      "**  Step 1470, val loss = 0.43, val accuracy = 84.38%  **\n",
      "Step: 1475, loss: 0.5774, accuracy: 78.1250%\n",
      "**  Step 1475, val loss = 0.98, val accuracy = 71.88%  **\n",
      "Step: 1480, loss: 0.5917, accuracy: 79.6875%\n",
      "**  Step 1480, val loss = 0.66, val accuracy = 81.25%  **\n",
      "Step: 1485, loss: 0.7420, accuracy: 81.2500%\n",
      "**  Step 1485, val loss = 0.71, val accuracy = 73.44%  **\n",
      "Step: 1490, loss: 0.4706, accuracy: 85.9375%\n",
      "**  Step 1490, val loss = 0.97, val accuracy = 68.75%  **\n",
      "Step: 1495, loss: 0.4119, accuracy: 85.9375%\n",
      "**  Step 1495, val loss = 0.61, val accuracy = 79.69%  **\n",
      "Step: 1500, loss: 0.5736, accuracy: 84.3750%\n",
      "**  Step 1500, val loss = 0.56, val accuracy = 82.81%  **\n",
      "Step: 1505, loss: 0.5495, accuracy: 78.1250%\n",
      "**  Step 1505, val loss = 0.38, val accuracy = 90.62%  **\n",
      "Step: 1510, loss: 0.5466, accuracy: 79.6875%\n",
      "**  Step 1510, val loss = 1.21, val accuracy = 70.31%  **\n",
      "Step: 1515, loss: 0.4687, accuracy: 87.5000%\n",
      "**  Step 1515, val loss = 0.63, val accuracy = 78.12%  **\n",
      "Step: 1520, loss: 0.4289, accuracy: 87.5000%\n",
      "**  Step 1520, val loss = 0.91, val accuracy = 73.44%  **\n",
      "Step: 1525, loss: 0.7078, accuracy: 71.8750%\n",
      "**  Step 1525, val loss = 0.65, val accuracy = 79.69%  **\n",
      "Step: 1530, loss: 0.5166, accuracy: 84.3750%\n",
      "**  Step 1530, val loss = 0.87, val accuracy = 73.44%  **\n",
      "Step: 1535, loss: 0.5785, accuracy: 89.0625%\n",
      "**  Step 1535, val loss = 0.71, val accuracy = 73.44%  **\n",
      "Step: 1540, loss: 0.5751, accuracy: 79.6875%\n",
      "**  Step 1540, val loss = 0.55, val accuracy = 85.94%  **\n",
      "Step: 1545, loss: 0.6373, accuracy: 76.5625%\n",
      "**  Step 1545, val loss = 0.80, val accuracy = 82.81%  **\n",
      "Step: 1550, loss: 0.4214, accuracy: 82.8125%\n",
      "**  Step 1550, val loss = 0.61, val accuracy = 76.56%  **\n",
      "Step: 1555, loss: 0.5670, accuracy: 78.1250%\n",
      "**  Step 1555, val loss = 0.71, val accuracy = 76.56%  **\n",
      "Step: 1560, loss: 0.7648, accuracy: 73.4375%\n",
      "**  Step 1560, val loss = 0.83, val accuracy = 79.69%  **\n",
      "Step: 1565, loss: 0.6057, accuracy: 78.1250%\n",
      "**  Step 1565, val loss = 0.72, val accuracy = 71.88%  **\n",
      "Step: 1570, loss: 0.5860, accuracy: 81.2500%\n",
      "**  Step 1570, val loss = 0.65, val accuracy = 78.12%  **\n",
      "Step: 1575, loss: 0.5800, accuracy: 84.3750%\n",
      "**  Step 1575, val loss = 0.62, val accuracy = 79.69%  **\n",
      "Step: 1580, loss: 0.4096, accuracy: 85.9375%\n",
      "**  Step 1580, val loss = 0.68, val accuracy = 76.56%  **\n",
      "Step: 1585, loss: 0.5217, accuracy: 82.8125%\n",
      "**  Step 1585, val loss = 0.74, val accuracy = 71.88%  **\n",
      "Step: 1590, loss: 0.5922, accuracy: 81.2500%\n",
      "**  Step 1590, val loss = 0.55, val accuracy = 81.25%  **\n",
      "Step: 1595, loss: 0.6289, accuracy: 82.8125%\n",
      "**  Step 1595, val loss = 0.64, val accuracy = 76.56%  **\n",
      "Step: 1600, loss: 0.3941, accuracy: 89.0625%\n",
      "**  Step 1600, val loss = 1.09, val accuracy = 70.31%  **\n",
      "Step: 1605, loss: 0.4333, accuracy: 87.5000%\n",
      "**  Step 1605, val loss = 0.68, val accuracy = 75.00%  **\n",
      "Step: 1610, loss: 0.4958, accuracy: 79.6875%\n",
      "**  Step 1610, val loss = 0.90, val accuracy = 73.44%  **\n",
      "Step: 1615, loss: 0.5492, accuracy: 81.2500%\n",
      "**  Step 1615, val loss = 0.34, val accuracy = 92.19%  **\n",
      "Step: 1620, loss: 0.4069, accuracy: 81.2500%\n",
      "**  Step 1620, val loss = 0.87, val accuracy = 75.00%  **\n",
      "Step: 1625, loss: 0.4370, accuracy: 85.9375%\n",
      "**  Step 1625, val loss = 0.41, val accuracy = 84.38%  **\n",
      "Step: 1630, loss: 0.5400, accuracy: 84.3750%\n",
      "**  Step 1630, val loss = 0.35, val accuracy = 85.94%  **\n",
      "Step: 1635, loss: 0.3979, accuracy: 87.5000%\n",
      "**  Step 1635, val loss = 0.75, val accuracy = 79.69%  **\n",
      "Step: 1640, loss: 0.5144, accuracy: 87.5000%\n",
      "**  Step 1640, val loss = 0.38, val accuracy = 87.50%  **\n",
      "Step: 1645, loss: 0.5433, accuracy: 82.8125%\n",
      "**  Step 1645, val loss = 0.66, val accuracy = 82.81%  **\n",
      "Step: 1650, loss: 0.4088, accuracy: 87.5000%\n",
      "**  Step 1650, val loss = 0.72, val accuracy = 82.81%  **\n",
      "Step: 1655, loss: 0.2553, accuracy: 93.7500%\n",
      "**  Step 1655, val loss = 0.63, val accuracy = 82.81%  **\n",
      "Step: 1660, loss: 0.5854, accuracy: 78.1250%\n",
      "**  Step 1660, val loss = 0.48, val accuracy = 81.25%  **\n",
      "Step: 1665, loss: 0.4962, accuracy: 79.6875%\n",
      "**  Step 1665, val loss = 0.49, val accuracy = 84.38%  **\n",
      "Step: 1670, loss: 0.4187, accuracy: 92.1875%\n",
      "**  Step 1670, val loss = 0.61, val accuracy = 79.69%  **\n",
      "Step: 1675, loss: 0.5555, accuracy: 84.3750%\n",
      "**  Step 1675, val loss = 0.56, val accuracy = 82.81%  **\n",
      "Step: 1680, loss: 0.6513, accuracy: 81.2500%\n",
      "**  Step 1680, val loss = 1.01, val accuracy = 73.44%  **\n",
      "Step: 1685, loss: 0.5893, accuracy: 87.5000%\n",
      "**  Step 1685, val loss = 0.76, val accuracy = 79.69%  **\n",
      "Step: 1690, loss: 0.3663, accuracy: 92.1875%\n",
      "**  Step 1690, val loss = 0.65, val accuracy = 82.81%  **\n",
      "Step: 1695, loss: 0.6261, accuracy: 81.2500%\n",
      "**  Step 1695, val loss = 0.57, val accuracy = 78.12%  **\n",
      "Step: 1700, loss: 0.5346, accuracy: 79.6875%\n",
      "**  Step 1700, val loss = 0.84, val accuracy = 79.69%  **\n",
      "Step: 1705, loss: 0.5320, accuracy: 85.9375%\n",
      "**  Step 1705, val loss = 0.80, val accuracy = 81.25%  **\n",
      "Step: 1710, loss: 0.4591, accuracy: 85.9375%\n",
      "**  Step 1710, val loss = 0.51, val accuracy = 87.50%  **\n",
      "Step: 1715, loss: 0.5293, accuracy: 84.3750%\n",
      "**  Step 1715, val loss = 0.94, val accuracy = 71.88%  **\n",
      "Step: 1720, loss: 0.4682, accuracy: 82.8125%\n",
      "**  Step 1720, val loss = 0.69, val accuracy = 76.56%  **\n",
      "Step: 1725, loss: 0.3966, accuracy: 85.9375%\n",
      "**  Step 1725, val loss = 0.51, val accuracy = 87.50%  **\n",
      "Step: 1730, loss: 0.6038, accuracy: 84.3750%\n",
      "**  Step 1730, val loss = 0.93, val accuracy = 75.00%  **\n",
      "Step: 1735, loss: 0.5694, accuracy: 78.1250%\n",
      "**  Step 1735, val loss = 0.47, val accuracy = 82.81%  **\n",
      "Step: 1740, loss: 0.4518, accuracy: 85.9375%\n",
      "**  Step 1740, val loss = 0.65, val accuracy = 75.00%  **\n",
      "Step: 1745, loss: 0.4605, accuracy: 81.2500%\n",
      "**  Step 1745, val loss = 0.38, val accuracy = 90.62%  **\n",
      "Step: 1750, loss: 0.5171, accuracy: 84.3750%\n",
      "**  Step 1750, val loss = 0.47, val accuracy = 87.50%  **\n",
      "Step: 1755, loss: 0.5193, accuracy: 79.6875%\n",
      "**  Step 1755, val loss = 0.54, val accuracy = 84.38%  **\n",
      "Step: 1760, loss: 0.4448, accuracy: 87.5000%\n",
      "**  Step 1760, val loss = 0.74, val accuracy = 75.00%  **\n",
      "Step: 1765, loss: 0.3541, accuracy: 90.6250%\n",
      "**  Step 1765, val loss = 0.60, val accuracy = 82.81%  **\n",
      "Step: 1770, loss: 0.5302, accuracy: 89.0625%\n",
      "**  Step 1770, val loss = 0.81, val accuracy = 71.88%  **\n",
      "Step: 1775, loss: 0.5088, accuracy: 81.2500%\n",
      "**  Step 1775, val loss = 0.58, val accuracy = 79.69%  **\n",
      "Step: 1780, loss: 0.2843, accuracy: 87.5000%\n",
      "**  Step 1780, val loss = 1.02, val accuracy = 65.62%  **\n",
      "Step: 1785, loss: 0.4533, accuracy: 87.5000%\n",
      "**  Step 1785, val loss = 0.77, val accuracy = 78.12%  **\n",
      "Step: 1790, loss: 0.8114, accuracy: 71.8750%\n",
      "**  Step 1790, val loss = 0.47, val accuracy = 87.50%  **\n",
      "Step: 1795, loss: 0.5095, accuracy: 85.9375%\n",
      "**  Step 1795, val loss = 0.52, val accuracy = 87.50%  **\n",
      "Step: 1800, loss: 0.4229, accuracy: 82.8125%\n",
      "**  Step 1800, val loss = 0.68, val accuracy = 79.69%  **\n",
      "Step: 1805, loss: 0.6192, accuracy: 79.6875%\n",
      "**  Step 1805, val loss = 0.73, val accuracy = 81.25%  **\n",
      "Step: 1810, loss: 0.4620, accuracy: 85.9375%\n",
      "**  Step 1810, val loss = 0.71, val accuracy = 76.56%  **\n",
      "Step: 1815, loss: 0.5199, accuracy: 82.8125%\n",
      "**  Step 1815, val loss = 0.58, val accuracy = 79.69%  **\n",
      "Step: 1820, loss: 0.5769, accuracy: 82.8125%\n",
      "**  Step 1820, val loss = 0.37, val accuracy = 87.50%  **\n",
      "Step: 1825, loss: 0.6035, accuracy: 82.8125%\n",
      "**  Step 1825, val loss = 0.65, val accuracy = 79.69%  **\n",
      "Step: 1830, loss: 0.3703, accuracy: 89.0625%\n",
      "**  Step 1830, val loss = 0.48, val accuracy = 82.81%  **\n",
      "Step: 1835, loss: 0.2106, accuracy: 93.7500%\n",
      "**  Step 1835, val loss = 0.87, val accuracy = 67.19%  **\n",
      "Step: 1840, loss: 0.4308, accuracy: 87.5000%\n",
      "**  Step 1840, val loss = 0.58, val accuracy = 85.94%  **\n",
      "Step: 1845, loss: 0.4190, accuracy: 79.6875%\n",
      "**  Step 1845, val loss = 1.21, val accuracy = 84.38%  **\n",
      "Step: 1850, loss: 0.4544, accuracy: 85.9375%\n",
      "**  Step 1850, val loss = 0.66, val accuracy = 85.94%  **\n",
      "Step: 1855, loss: 0.4567, accuracy: 82.8125%\n",
      "**  Step 1855, val loss = 0.56, val accuracy = 82.81%  **\n",
      "Step: 1860, loss: 0.5348, accuracy: 78.1250%\n",
      "**  Step 1860, val loss = 0.43, val accuracy = 92.19%  **\n",
      "Step: 1865, loss: 0.3758, accuracy: 92.1875%\n",
      "**  Step 1865, val loss = 1.04, val accuracy = 65.62%  **\n",
      "Step: 1870, loss: 0.3198, accuracy: 90.6250%\n",
      "**  Step 1870, val loss = 0.64, val accuracy = 76.56%  **\n",
      "Step: 1875, loss: 0.3620, accuracy: 93.7500%\n",
      "**  Step 1875, val loss = 0.50, val accuracy = 89.06%  **\n",
      "Step: 1880, loss: 0.6358, accuracy: 78.1250%\n",
      "**  Step 1880, val loss = 0.59, val accuracy = 79.69%  **\n",
      "Step: 1885, loss: 0.4200, accuracy: 84.3750%\n",
      "**  Step 1885, val loss = 0.73, val accuracy = 81.25%  **\n",
      "Step: 1890, loss: 0.5308, accuracy: 79.6875%\n",
      "**  Step 1890, val loss = 0.51, val accuracy = 87.50%  **\n",
      "Step: 1895, loss: 0.5075, accuracy: 90.6250%\n",
      "**  Step 1895, val loss = 0.61, val accuracy = 81.25%  **\n",
      "Step: 1900, loss: 0.4373, accuracy: 82.8125%\n",
      "**  Step 1900, val loss = 0.62, val accuracy = 75.00%  **\n",
      "Step: 1905, loss: 0.3701, accuracy: 85.9375%\n",
      "**  Step 1905, val loss = 0.43, val accuracy = 87.50%  **\n",
      "Step: 1910, loss: 0.5838, accuracy: 81.2500%\n",
      "**  Step 1910, val loss = 0.48, val accuracy = 82.81%  **\n",
      "Step: 1915, loss: 0.3945, accuracy: 90.6250%\n",
      "**  Step 1915, val loss = 0.59, val accuracy = 81.25%  **\n",
      "Step: 1920, loss: 0.2756, accuracy: 85.9375%\n",
      "**  Step 1920, val loss = 0.71, val accuracy = 78.12%  **\n",
      "Step: 1925, loss: 0.5614, accuracy: 87.5000%\n",
      "**  Step 1925, val loss = 0.61, val accuracy = 82.81%  **\n",
      "Step: 1930, loss: 0.5723, accuracy: 84.3750%\n",
      "**  Step 1930, val loss = 0.46, val accuracy = 84.38%  **\n",
      "Step: 1935, loss: 0.4763, accuracy: 85.9375%\n",
      "**  Step 1935, val loss = 0.40, val accuracy = 89.06%  **\n",
      "Step: 1940, loss: 0.2506, accuracy: 92.1875%\n",
      "**  Step 1940, val loss = 0.67, val accuracy = 78.12%  **\n",
      "Step: 1945, loss: 0.5044, accuracy: 84.3750%\n",
      "**  Step 1945, val loss = 0.60, val accuracy = 82.81%  **\n",
      "Step: 1950, loss: 0.4701, accuracy: 84.3750%\n",
      "**  Step 1950, val loss = 0.47, val accuracy = 84.38%  **\n",
      "Step: 1955, loss: 0.1600, accuracy: 98.4375%\n",
      "**  Step 1955, val loss = 0.31, val accuracy = 89.06%  **\n",
      "Step: 1960, loss: 0.3973, accuracy: 85.9375%\n",
      "**  Step 1960, val loss = 0.98, val accuracy = 76.56%  **\n",
      "Step: 1965, loss: 0.3313, accuracy: 87.5000%\n",
      "**  Step 1965, val loss = 0.94, val accuracy = 75.00%  **\n",
      "Step: 1970, loss: 0.6102, accuracy: 81.2500%\n",
      "**  Step 1970, val loss = 0.78, val accuracy = 76.56%  **\n",
      "Step: 1975, loss: 0.4817, accuracy: 87.5000%\n",
      "**  Step 1975, val loss = 0.44, val accuracy = 82.81%  **\n",
      "Step: 1980, loss: 0.3081, accuracy: 87.5000%\n",
      "**  Step 1980, val loss = 0.98, val accuracy = 73.44%  **\n",
      "Step: 1985, loss: 0.3724, accuracy: 85.9375%\n",
      "**  Step 1985, val loss = 0.53, val accuracy = 85.94%  **\n",
      "Step: 1990, loss: 0.5024, accuracy: 84.3750%\n",
      "**  Step 1990, val loss = 0.41, val accuracy = 82.81%  **\n",
      "Step: 1995, loss: 0.3303, accuracy: 90.6250%\n",
      "**  Step 1995, val loss = 0.78, val accuracy = 73.44%  **\n",
      "Step: 2000, loss: 0.3382, accuracy: 89.0625%\n",
      "**  Step 2000, val loss = 0.53, val accuracy = 75.00%  **\n",
      "Step: 2005, loss: 0.7894, accuracy: 85.9375%\n",
      "**  Step 2005, val loss = 0.65, val accuracy = 84.38%  **\n",
      "Step: 2010, loss: 0.5718, accuracy: 84.3750%\n",
      "**  Step 2010, val loss = 0.54, val accuracy = 87.50%  **\n",
      "Step: 2015, loss: 0.5038, accuracy: 84.3750%\n",
      "**  Step 2015, val loss = 0.84, val accuracy = 78.12%  **\n",
      "Step: 2020, loss: 0.4244, accuracy: 92.1875%\n",
      "**  Step 2020, val loss = 0.73, val accuracy = 79.69%  **\n",
      "Step: 2025, loss: 0.2320, accuracy: 95.3125%\n",
      "**  Step 2025, val loss = 0.32, val accuracy = 87.50%  **\n",
      "Step: 2030, loss: 0.5062, accuracy: 79.6875%\n",
      "**  Step 2030, val loss = 0.79, val accuracy = 75.00%  **\n",
      "Step: 2035, loss: 0.4031, accuracy: 82.8125%\n",
      "**  Step 2035, val loss = 0.38, val accuracy = 89.06%  **\n",
      "Step: 2040, loss: 0.6801, accuracy: 73.4375%\n",
      "**  Step 2040, val loss = 0.76, val accuracy = 78.12%  **\n",
      "Step: 2045, loss: 0.5057, accuracy: 78.1250%\n",
      "**  Step 2045, val loss = 0.80, val accuracy = 78.12%  **\n",
      "Step: 2050, loss: 0.5138, accuracy: 82.8125%\n",
      "**  Step 2050, val loss = 0.61, val accuracy = 87.50%  **\n",
      "Step: 2055, loss: 0.4550, accuracy: 89.0625%\n",
      "**  Step 2055, val loss = 0.78, val accuracy = 79.69%  **\n",
      "Step: 2060, loss: 0.3941, accuracy: 89.0625%\n",
      "**  Step 2060, val loss = 0.69, val accuracy = 84.38%  **\n",
      "Step: 2065, loss: 0.4471, accuracy: 85.9375%\n",
      "**  Step 2065, val loss = 0.89, val accuracy = 76.56%  **\n",
      "Step: 2070, loss: 0.4755, accuracy: 82.8125%\n",
      "**  Step 2070, val loss = 0.53, val accuracy = 82.81%  **\n",
      "Step: 2075, loss: 0.3381, accuracy: 90.6250%\n",
      "**  Step 2075, val loss = 0.64, val accuracy = 84.38%  **\n",
      "Step: 2080, loss: 0.4390, accuracy: 81.2500%\n",
      "**  Step 2080, val loss = 0.40, val accuracy = 89.06%  **\n",
      "Step: 2085, loss: 0.4315, accuracy: 87.5000%\n",
      "**  Step 2085, val loss = 0.52, val accuracy = 82.81%  **\n",
      "Step: 2090, loss: 0.4038, accuracy: 85.9375%\n",
      "**  Step 2090, val loss = 0.80, val accuracy = 70.31%  **\n",
      "Step: 2095, loss: 0.5201, accuracy: 84.3750%\n",
      "**  Step 2095, val loss = 0.58, val accuracy = 81.25%  **\n",
      "Step: 2100, loss: 0.2874, accuracy: 93.7500%\n",
      "**  Step 2100, val loss = 0.58, val accuracy = 79.69%  **\n",
      "Step: 2105, loss: 0.5642, accuracy: 78.1250%\n",
      "**  Step 2105, val loss = 0.52, val accuracy = 81.25%  **\n",
      "Step: 2110, loss: 0.6024, accuracy: 84.3750%\n",
      "**  Step 2110, val loss = 0.58, val accuracy = 82.81%  **\n",
      "Step: 2115, loss: 0.5984, accuracy: 76.5625%\n",
      "**  Step 2115, val loss = 0.68, val accuracy = 79.69%  **\n",
      "Step: 2120, loss: 0.2689, accuracy: 90.6250%\n",
      "**  Step 2120, val loss = 0.70, val accuracy = 78.12%  **\n",
      "Step: 2125, loss: 0.2676, accuracy: 90.6250%\n",
      "**  Step 2125, val loss = 0.55, val accuracy = 81.25%  **\n",
      "Step: 2130, loss: 0.5136, accuracy: 79.6875%\n",
      "**  Step 2130, val loss = 0.80, val accuracy = 75.00%  **\n",
      "Step: 2135, loss: 0.4306, accuracy: 87.5000%\n",
      "**  Step 2135, val loss = 0.60, val accuracy = 78.12%  **\n",
      "Step: 2140, loss: 0.4370, accuracy: 81.2500%\n",
      "**  Step 2140, val loss = 0.72, val accuracy = 67.19%  **\n",
      "Step: 2145, loss: 0.2509, accuracy: 93.7500%\n",
      "**  Step 2145, val loss = 0.62, val accuracy = 79.69%  **\n",
      "Step: 2150, loss: 0.4500, accuracy: 85.9375%\n",
      "**  Step 2150, val loss = 0.72, val accuracy = 75.00%  **\n",
      "Step: 2155, loss: 0.5796, accuracy: 79.6875%\n",
      "**  Step 2155, val loss = 0.73, val accuracy = 76.56%  **\n",
      "Step: 2160, loss: 0.4053, accuracy: 89.0625%\n",
      "**  Step 2160, val loss = 0.64, val accuracy = 75.00%  **\n",
      "Step: 2165, loss: 0.3437, accuracy: 89.0625%\n",
      "**  Step 2165, val loss = 0.63, val accuracy = 81.25%  **\n",
      "Step: 2170, loss: 0.3597, accuracy: 90.6250%\n",
      "**  Step 2170, val loss = 0.77, val accuracy = 73.44%  **\n",
      "Step: 2175, loss: 0.3582, accuracy: 89.0625%\n",
      "**  Step 2175, val loss = 0.59, val accuracy = 73.44%  **\n",
      "Step: 2180, loss: 0.2358, accuracy: 93.7500%\n",
      "**  Step 2180, val loss = 0.58, val accuracy = 79.69%  **\n",
      "Step: 2185, loss: 0.2604, accuracy: 90.6250%\n",
      "**  Step 2185, val loss = 0.54, val accuracy = 81.25%  **\n",
      "Step: 2190, loss: 0.3034, accuracy: 95.3125%\n",
      "**  Step 2190, val loss = 0.53, val accuracy = 82.81%  **\n",
      "Step: 2195, loss: 0.4105, accuracy: 84.3750%\n",
      "**  Step 2195, val loss = 0.61, val accuracy = 85.94%  **\n",
      "Step: 2200, loss: 0.2290, accuracy: 96.8750%\n",
      "**  Step 2200, val loss = 0.81, val accuracy = 81.25%  **\n",
      "Step: 2205, loss: 0.5709, accuracy: 82.8125%\n",
      "**  Step 2205, val loss = 0.46, val accuracy = 87.50%  **\n",
      "Step: 2210, loss: 0.3839, accuracy: 81.2500%\n",
      "**  Step 2210, val loss = 0.50, val accuracy = 85.94%  **\n",
      "Step: 2215, loss: 0.5611, accuracy: 81.2500%\n",
      "**  Step 2215, val loss = 0.57, val accuracy = 78.12%  **\n",
      "Step: 2220, loss: 0.3281, accuracy: 87.5000%\n",
      "**  Step 2220, val loss = 0.59, val accuracy = 78.12%  **\n",
      "Step: 2225, loss: 0.4885, accuracy: 82.8125%\n",
      "**  Step 2225, val loss = 0.65, val accuracy = 79.69%  **\n",
      "Step: 2230, loss: 0.4689, accuracy: 82.8125%\n",
      "**  Step 2230, val loss = 0.53, val accuracy = 82.81%  **\n",
      "Step: 2235, loss: 0.4384, accuracy: 87.5000%\n",
      "**  Step 2235, val loss = 0.64, val accuracy = 81.25%  **\n",
      "Step: 2240, loss: 0.3890, accuracy: 85.9375%\n",
      "**  Step 2240, val loss = 0.55, val accuracy = 85.94%  **\n",
      "Step: 2245, loss: 0.4842, accuracy: 87.5000%\n",
      "**  Step 2245, val loss = 0.53, val accuracy = 84.38%  **\n",
      "Step: 2250, loss: 0.4136, accuracy: 84.3750%\n",
      "**  Step 2250, val loss = 0.64, val accuracy = 84.38%  **\n",
      "Step: 2255, loss: 0.3078, accuracy: 89.0625%\n",
      "**  Step 2255, val loss = 0.84, val accuracy = 79.69%  **\n",
      "Step: 2260, loss: 0.7208, accuracy: 89.0625%\n",
      "**  Step 2260, val loss = 0.50, val accuracy = 89.06%  **\n",
      "Step: 2265, loss: 0.3399, accuracy: 92.1875%\n",
      "**  Step 2265, val loss = 0.61, val accuracy = 84.38%  **\n",
      "Step: 2270, loss: 0.5324, accuracy: 84.3750%\n",
      "**  Step 2270, val loss = 0.74, val accuracy = 79.69%  **\n",
      "Step: 2275, loss: 0.3480, accuracy: 89.0625%\n",
      "**  Step 2275, val loss = 0.51, val accuracy = 81.25%  **\n",
      "Step: 2280, loss: 0.4101, accuracy: 85.9375%\n",
      "**  Step 2280, val loss = 0.82, val accuracy = 68.75%  **\n",
      "Step: 2285, loss: 0.4110, accuracy: 87.5000%\n",
      "**  Step 2285, val loss = 0.39, val accuracy = 81.25%  **\n",
      "Step: 2290, loss: 0.4726, accuracy: 85.9375%\n",
      "**  Step 2290, val loss = 0.75, val accuracy = 78.12%  **\n",
      "Step: 2295, loss: 0.3892, accuracy: 84.3750%\n",
      "**  Step 2295, val loss = 0.48, val accuracy = 84.38%  **\n",
      "Step: 2300, loss: 0.3222, accuracy: 87.5000%\n",
      "**  Step 2300, val loss = 0.50, val accuracy = 82.81%  **\n",
      "Step: 2305, loss: 0.2991, accuracy: 90.6250%\n",
      "**  Step 2305, val loss = 0.57, val accuracy = 82.81%  **\n",
      "Step: 2310, loss: 0.3299, accuracy: 89.0625%\n",
      "**  Step 2310, val loss = 1.01, val accuracy = 79.69%  **\n",
      "Step: 2315, loss: 0.3202, accuracy: 92.1875%\n",
      "**  Step 2315, val loss = 0.57, val accuracy = 78.12%  **\n",
      "Step: 2320, loss: 0.3694, accuracy: 89.0625%\n",
      "**  Step 2320, val loss = 0.67, val accuracy = 79.69%  **\n",
      "Step: 2325, loss: 0.2844, accuracy: 95.3125%\n",
      "**  Step 2325, val loss = 0.62, val accuracy = 78.12%  **\n",
      "Step: 2330, loss: 0.4055, accuracy: 85.9375%\n",
      "**  Step 2330, val loss = 0.57, val accuracy = 81.25%  **\n",
      "Step: 2335, loss: 0.2574, accuracy: 87.5000%\n",
      "**  Step 2335, val loss = 0.91, val accuracy = 78.12%  **\n",
      "Step: 2340, loss: 0.2644, accuracy: 95.3125%\n",
      "**  Step 2340, val loss = 0.51, val accuracy = 84.38%  **\n",
      "Step: 2345, loss: 0.2786, accuracy: 92.1875%\n",
      "**  Step 2345, val loss = 0.64, val accuracy = 81.25%  **\n",
      "Step: 2350, loss: 0.4646, accuracy: 89.0625%\n",
      "**  Step 2350, val loss = 0.79, val accuracy = 81.25%  **\n",
      "Step: 2355, loss: 0.3445, accuracy: 85.9375%\n",
      "**  Step 2355, val loss = 0.50, val accuracy = 81.25%  **\n",
      "Step: 2360, loss: 0.6613, accuracy: 79.6875%\n",
      "**  Step 2360, val loss = 0.39, val accuracy = 85.94%  **\n",
      "Step: 2365, loss: 0.4714, accuracy: 89.0625%\n",
      "**  Step 2365, val loss = 0.52, val accuracy = 82.81%  **\n",
      "Step: 2370, loss: 0.3293, accuracy: 85.9375%\n",
      "**  Step 2370, val loss = 0.85, val accuracy = 76.56%  **\n",
      "Step: 2375, loss: 0.3809, accuracy: 84.3750%\n",
      "**  Step 2375, val loss = 0.83, val accuracy = 81.25%  **\n",
      "Step: 2380, loss: 0.2777, accuracy: 90.6250%\n",
      "**  Step 2380, val loss = 0.57, val accuracy = 75.00%  **\n",
      "Step: 2385, loss: 0.3049, accuracy: 93.7500%\n",
      "**  Step 2385, val loss = 0.79, val accuracy = 76.56%  **\n",
      "Step: 2390, loss: 0.4942, accuracy: 79.6875%\n",
      "**  Step 2390, val loss = 0.59, val accuracy = 84.38%  **\n",
      "Step: 2395, loss: 0.2244, accuracy: 92.1875%\n",
      "**  Step 2395, val loss = 0.62, val accuracy = 84.38%  **\n",
      "Step: 2400, loss: 0.3960, accuracy: 87.5000%\n",
      "**  Step 2400, val loss = 0.58, val accuracy = 81.25%  **\n",
      "Step: 2405, loss: 0.2941, accuracy: 84.3750%\n",
      "**  Step 2405, val loss = 0.75, val accuracy = 75.00%  **\n",
      "Step: 2410, loss: 0.2821, accuracy: 90.6250%\n",
      "**  Step 2410, val loss = 0.58, val accuracy = 84.38%  **\n",
      "Step: 2415, loss: 0.4038, accuracy: 90.6250%\n",
      "**  Step 2415, val loss = 0.83, val accuracy = 75.00%  **\n",
      "Step: 2420, loss: 0.3042, accuracy: 95.3125%\n",
      "**  Step 2420, val loss = 0.72, val accuracy = 76.56%  **\n",
      "Step: 2425, loss: 0.4407, accuracy: 87.5000%\n",
      "**  Step 2425, val loss = 0.72, val accuracy = 79.69%  **\n",
      "Step: 2430, loss: 0.3031, accuracy: 92.1875%\n",
      "**  Step 2430, val loss = 0.46, val accuracy = 82.81%  **\n",
      "Step: 2435, loss: 0.2774, accuracy: 90.6250%\n",
      "**  Step 2435, val loss = 0.53, val accuracy = 84.38%  **\n",
      "Step: 2440, loss: 0.4325, accuracy: 90.6250%\n",
      "**  Step 2440, val loss = 0.49, val accuracy = 84.38%  **\n",
      "Step: 2445, loss: 0.3219, accuracy: 87.5000%\n",
      "**  Step 2445, val loss = 0.67, val accuracy = 81.25%  **\n",
      "Step: 2450, loss: 0.2237, accuracy: 92.1875%\n",
      "**  Step 2450, val loss = 0.83, val accuracy = 75.00%  **\n",
      "Step: 2455, loss: 0.4075, accuracy: 87.5000%\n",
      "**  Step 2455, val loss = 0.38, val accuracy = 84.38%  **\n",
      "Step: 2460, loss: 0.3771, accuracy: 85.9375%\n",
      "**  Step 2460, val loss = 0.66, val accuracy = 79.69%  **\n",
      "Step: 2465, loss: 0.4053, accuracy: 89.0625%\n",
      "**  Step 2465, val loss = 0.69, val accuracy = 78.12%  **\n",
      "Step: 2470, loss: 0.3297, accuracy: 87.5000%\n",
      "**  Step 2470, val loss = 0.83, val accuracy = 82.81%  **\n",
      "Step: 2475, loss: 0.3545, accuracy: 92.1875%\n",
      "**  Step 2475, val loss = 0.60, val accuracy = 85.94%  **\n",
      "Step: 2480, loss: 0.3619, accuracy: 92.1875%\n",
      "**  Step 2480, val loss = 0.54, val accuracy = 87.50%  **\n",
      "Step: 2485, loss: 0.4407, accuracy: 85.9375%\n",
      "**  Step 2485, val loss = 0.40, val accuracy = 93.75%  **\n",
      "Step: 2490, loss: 0.3195, accuracy: 84.3750%\n",
      "**  Step 2490, val loss = 0.88, val accuracy = 76.56%  **\n",
      "Step: 2495, loss: 0.3709, accuracy: 85.9375%\n",
      "**  Step 2495, val loss = 0.33, val accuracy = 85.94%  **\n",
      "Step: 2500, loss: 0.3673, accuracy: 89.0625%\n",
      "**  Step 2500, val loss = 0.78, val accuracy = 82.81%  **\n",
      "Step: 2505, loss: 0.4079, accuracy: 89.0625%\n",
      "**  Step 2505, val loss = 0.56, val accuracy = 84.38%  **\n",
      "Step: 2510, loss: 0.2682, accuracy: 90.6250%\n",
      "**  Step 2510, val loss = 0.57, val accuracy = 81.25%  **\n",
      "Step: 2515, loss: 0.4194, accuracy: 82.8125%\n",
      "**  Step 2515, val loss = 0.42, val accuracy = 82.81%  **\n",
      "Step: 2520, loss: 0.4218, accuracy: 81.2500%\n",
      "**  Step 2520, val loss = 0.60, val accuracy = 84.38%  **\n",
      "Step: 2525, loss: 0.3238, accuracy: 92.1875%\n",
      "**  Step 2525, val loss = 0.54, val accuracy = 84.38%  **\n",
      "Step: 2530, loss: 0.2362, accuracy: 95.3125%\n",
      "**  Step 2530, val loss = 0.66, val accuracy = 76.56%  **\n",
      "Step: 2535, loss: 0.2190, accuracy: 93.7500%\n",
      "**  Step 2535, val loss = 0.70, val accuracy = 84.38%  **\n",
      "Step: 2540, loss: 0.5877, accuracy: 89.0625%\n",
      "**  Step 2540, val loss = 0.73, val accuracy = 76.56%  **\n",
      "Step: 2545, loss: 0.2527, accuracy: 93.7500%\n",
      "**  Step 2545, val loss = 0.86, val accuracy = 85.94%  **\n",
      "Step: 2550, loss: 0.4492, accuracy: 85.9375%\n",
      "**  Step 2550, val loss = 0.67, val accuracy = 71.88%  **\n",
      "Step: 2555, loss: 0.2966, accuracy: 89.0625%\n",
      "**  Step 2555, val loss = 0.48, val accuracy = 78.12%  **\n",
      "Step: 2560, loss: 0.5176, accuracy: 85.9375%\n",
      "**  Step 2560, val loss = 0.42, val accuracy = 92.19%  **\n",
      "Step: 2565, loss: 0.2406, accuracy: 93.7500%\n",
      "**  Step 2565, val loss = 0.42, val accuracy = 82.81%  **\n",
      "Step: 2570, loss: 0.4537, accuracy: 85.9375%\n",
      "**  Step 2570, val loss = 0.80, val accuracy = 78.12%  **\n",
      "Step: 2575, loss: 0.3261, accuracy: 87.5000%\n",
      "**  Step 2575, val loss = 0.48, val accuracy = 93.75%  **\n",
      "Step: 2580, loss: 0.3717, accuracy: 90.6250%\n",
      "**  Step 2580, val loss = 0.96, val accuracy = 73.44%  **\n",
      "Step: 2585, loss: 0.3607, accuracy: 90.6250%\n",
      "**  Step 2585, val loss = 0.79, val accuracy = 81.25%  **\n",
      "Step: 2590, loss: 0.2903, accuracy: 92.1875%\n",
      "**  Step 2590, val loss = 0.49, val accuracy = 82.81%  **\n",
      "Step: 2595, loss: 0.3299, accuracy: 87.5000%\n",
      "**  Step 2595, val loss = 0.48, val accuracy = 87.50%  **\n",
      "Step: 2600, loss: 0.1687, accuracy: 93.7500%\n",
      "**  Step 2600, val loss = 0.58, val accuracy = 81.25%  **\n",
      "Step: 2605, loss: 0.4175, accuracy: 87.5000%\n",
      "**  Step 2605, val loss = 0.43, val accuracy = 87.50%  **\n",
      "Step: 2610, loss: 0.2162, accuracy: 93.7500%\n",
      "**  Step 2610, val loss = 0.75, val accuracy = 79.69%  **\n",
      "Step: 2615, loss: 0.2824, accuracy: 89.0625%\n",
      "**  Step 2615, val loss = 0.84, val accuracy = 76.56%  **\n",
      "Step: 2620, loss: 0.5794, accuracy: 87.5000%\n",
      "**  Step 2620, val loss = 0.93, val accuracy = 76.56%  **\n",
      "Step: 2625, loss: 0.2834, accuracy: 95.3125%\n",
      "**  Step 2625, val loss = 0.44, val accuracy = 85.94%  **\n",
      "Step: 2630, loss: 0.3837, accuracy: 85.9375%\n",
      "**  Step 2630, val loss = 0.73, val accuracy = 81.25%  **\n",
      "Step: 2635, loss: 0.3213, accuracy: 85.9375%\n",
      "**  Step 2635, val loss = 0.59, val accuracy = 79.69%  **\n",
      "Step: 2640, loss: 0.3522, accuracy: 85.9375%\n",
      "**  Step 2640, val loss = 0.37, val accuracy = 85.94%  **\n",
      "Step: 2645, loss: 0.3074, accuracy: 89.0625%\n",
      "**  Step 2645, val loss = 0.73, val accuracy = 76.56%  **\n",
      "Step: 2650, loss: 0.3743, accuracy: 85.9375%\n",
      "**  Step 2650, val loss = 0.66, val accuracy = 82.81%  **\n",
      "Step: 2655, loss: 0.2883, accuracy: 93.7500%\n",
      "**  Step 2655, val loss = 0.43, val accuracy = 84.38%  **\n",
      "Step: 2660, loss: 0.3714, accuracy: 84.3750%\n",
      "**  Step 2660, val loss = 0.81, val accuracy = 81.25%  **\n",
      "Step: 2665, loss: 0.4532, accuracy: 85.9375%\n",
      "**  Step 2665, val loss = 0.56, val accuracy = 82.81%  **\n",
      "Step: 2670, loss: 0.2946, accuracy: 89.0625%\n",
      "**  Step 2670, val loss = 0.79, val accuracy = 78.12%  **\n",
      "Step: 2675, loss: 0.4794, accuracy: 89.0625%\n",
      "**  Step 2675, val loss = 0.65, val accuracy = 78.12%  **\n",
      "Step: 2680, loss: 0.4078, accuracy: 84.3750%\n",
      "**  Step 2680, val loss = 0.73, val accuracy = 82.81%  **\n",
      "Step: 2685, loss: 0.2667, accuracy: 89.0625%\n",
      "**  Step 2685, val loss = 0.57, val accuracy = 84.38%  **\n",
      "Step: 2690, loss: 0.5347, accuracy: 79.6875%\n",
      "**  Step 2690, val loss = 0.62, val accuracy = 85.94%  **\n",
      "Step: 2695, loss: 0.4302, accuracy: 89.0625%\n",
      "**  Step 2695, val loss = 0.93, val accuracy = 71.88%  **\n",
      "Step: 2700, loss: 0.1983, accuracy: 92.1875%\n",
      "**  Step 2700, val loss = 0.62, val accuracy = 85.94%  **\n",
      "Step: 2705, loss: 0.2981, accuracy: 87.5000%\n",
      "**  Step 2705, val loss = 0.76, val accuracy = 79.69%  **\n",
      "Step: 2710, loss: 0.2852, accuracy: 87.5000%\n",
      "**  Step 2710, val loss = 0.98, val accuracy = 78.12%  **\n",
      "Step: 2715, loss: 0.2943, accuracy: 90.6250%\n",
      "**  Step 2715, val loss = 0.58, val accuracy = 81.25%  **\n",
      "Step: 2720, loss: 0.5107, accuracy: 79.6875%\n",
      "**  Step 2720, val loss = 0.96, val accuracy = 81.25%  **\n",
      "Step: 2725, loss: 0.4305, accuracy: 85.9375%\n",
      "**  Step 2725, val loss = 0.41, val accuracy = 87.50%  **\n",
      "Step: 2730, loss: 0.2737, accuracy: 95.3125%\n",
      "**  Step 2730, val loss = 0.43, val accuracy = 85.94%  **\n",
      "Step: 2735, loss: 0.2384, accuracy: 90.6250%\n",
      "**  Step 2735, val loss = 0.70, val accuracy = 76.56%  **\n",
      "Step: 2740, loss: 0.2942, accuracy: 85.9375%\n",
      "**  Step 2740, val loss = 0.75, val accuracy = 76.56%  **\n",
      "Step: 2745, loss: 0.2806, accuracy: 92.1875%\n",
      "**  Step 2745, val loss = 0.56, val accuracy = 85.94%  **\n",
      "Step: 2750, loss: 0.4168, accuracy: 93.7500%\n",
      "**  Step 2750, val loss = 0.45, val accuracy = 84.38%  **\n",
      "Step: 2755, loss: 0.2583, accuracy: 93.7500%\n",
      "**  Step 2755, val loss = 0.72, val accuracy = 79.69%  **\n",
      "Step: 2760, loss: 0.3371, accuracy: 87.5000%\n",
      "**  Step 2760, val loss = 0.58, val accuracy = 85.94%  **\n",
      "Step: 2765, loss: 0.4986, accuracy: 84.3750%\n",
      "**  Step 2765, val loss = 0.56, val accuracy = 87.50%  **\n",
      "Step: 2770, loss: 0.3249, accuracy: 85.9375%\n",
      "**  Step 2770, val loss = 0.61, val accuracy = 82.81%  **\n",
      "Step: 2775, loss: 0.3104, accuracy: 90.6250%\n",
      "**  Step 2775, val loss = 0.74, val accuracy = 79.69%  **\n",
      "Step: 2780, loss: 0.2810, accuracy: 92.1875%\n",
      "**  Step 2780, val loss = 0.37, val accuracy = 84.38%  **\n",
      "Step: 2785, loss: 0.3060, accuracy: 92.1875%\n",
      "**  Step 2785, val loss = 0.55, val accuracy = 79.69%  **\n",
      "Step: 2790, loss: 0.3187, accuracy: 90.6250%\n",
      "**  Step 2790, val loss = 0.57, val accuracy = 82.81%  **\n",
      "Step: 2795, loss: 0.2861, accuracy: 90.6250%\n",
      "**  Step 2795, val loss = 0.69, val accuracy = 78.12%  **\n",
      "Step: 2800, loss: 0.5517, accuracy: 87.5000%\n",
      "**  Step 2800, val loss = 0.77, val accuracy = 79.69%  **\n",
      "Step: 2805, loss: 0.3892, accuracy: 84.3750%\n",
      "**  Step 2805, val loss = 0.63, val accuracy = 78.12%  **\n",
      "Step: 2810, loss: 0.3347, accuracy: 92.1875%\n",
      "**  Step 2810, val loss = 0.60, val accuracy = 81.25%  **\n",
      "Step: 2815, loss: 0.3156, accuracy: 87.5000%\n",
      "**  Step 2815, val loss = 0.73, val accuracy = 76.56%  **\n",
      "Step: 2820, loss: 0.2264, accuracy: 93.7500%\n",
      "**  Step 2820, val loss = 0.56, val accuracy = 79.69%  **\n",
      "Step: 2825, loss: 0.2983, accuracy: 92.1875%\n",
      "**  Step 2825, val loss = 0.68, val accuracy = 81.25%  **\n",
      "Step: 2830, loss: 0.4933, accuracy: 81.2500%\n",
      "**  Step 2830, val loss = 0.47, val accuracy = 85.94%  **\n",
      "Step: 2835, loss: 0.4462, accuracy: 92.1875%\n",
      "**  Step 2835, val loss = 0.59, val accuracy = 85.94%  **\n",
      "Step: 2840, loss: 0.3043, accuracy: 90.6250%\n",
      "**  Step 2840, val loss = 0.35, val accuracy = 84.38%  **\n",
      "Step: 2845, loss: 0.2786, accuracy: 90.6250%\n",
      "**  Step 2845, val loss = 0.84, val accuracy = 78.12%  **\n",
      "Step: 2850, loss: 0.3223, accuracy: 90.6250%\n",
      "**  Step 2850, val loss = 0.41, val accuracy = 92.19%  **\n",
      "Step: 2855, loss: 0.3482, accuracy: 92.1875%\n",
      "**  Step 2855, val loss = 0.40, val accuracy = 84.38%  **\n",
      "Step: 2860, loss: 0.2341, accuracy: 90.6250%\n",
      "**  Step 2860, val loss = 0.60, val accuracy = 81.25%  **\n",
      "Step: 2865, loss: 0.2956, accuracy: 89.0625%\n",
      "**  Step 2865, val loss = 1.13, val accuracy = 73.44%  **\n",
      "Step: 2870, loss: 0.2861, accuracy: 90.6250%\n",
      "**  Step 2870, val loss = 0.42, val accuracy = 81.25%  **\n",
      "Step: 2875, loss: 0.3275, accuracy: 92.1875%\n",
      "**  Step 2875, val loss = 0.57, val accuracy = 79.69%  **\n",
      "Step: 2880, loss: 0.2682, accuracy: 92.1875%\n",
      "**  Step 2880, val loss = 0.88, val accuracy = 78.12%  **\n",
      "Step: 2885, loss: 0.2498, accuracy: 92.1875%\n",
      "**  Step 2885, val loss = 0.58, val accuracy = 81.25%  **\n",
      "Step: 2890, loss: 0.3328, accuracy: 90.6250%\n",
      "**  Step 2890, val loss = 0.49, val accuracy = 82.81%  **\n",
      "Step: 2895, loss: 0.2052, accuracy: 95.3125%\n",
      "**  Step 2895, val loss = 0.68, val accuracy = 84.38%  **\n",
      "Step: 2900, loss: 0.2136, accuracy: 96.8750%\n",
      "**  Step 2900, val loss = 0.41, val accuracy = 89.06%  **\n",
      "Step: 2905, loss: 0.2058, accuracy: 92.1875%\n",
      "**  Step 2905, val loss = 0.46, val accuracy = 87.50%  **\n",
      "Step: 2910, loss: 0.2298, accuracy: 95.3125%\n",
      "**  Step 2910, val loss = 0.56, val accuracy = 82.81%  **\n",
      "Step: 2915, loss: 0.3784, accuracy: 85.9375%\n",
      "**  Step 2915, val loss = 0.46, val accuracy = 87.50%  **\n",
      "Step: 2920, loss: 0.3301, accuracy: 90.6250%\n",
      "**  Step 2920, val loss = 0.58, val accuracy = 81.25%  **\n",
      "Step: 2925, loss: 0.3970, accuracy: 87.5000%\n",
      "**  Step 2925, val loss = 0.63, val accuracy = 82.81%  **\n",
      "Step: 2930, loss: 0.2467, accuracy: 90.6250%\n",
      "**  Step 2930, val loss = 0.64, val accuracy = 79.69%  **\n",
      "Step: 2935, loss: 0.2955, accuracy: 89.0625%\n",
      "**  Step 2935, val loss = 0.61, val accuracy = 81.25%  **\n",
      "Step: 2940, loss: 0.3605, accuracy: 89.0625%\n",
      "**  Step 2940, val loss = 0.67, val accuracy = 79.69%  **\n",
      "Step: 2945, loss: 0.5775, accuracy: 82.8125%\n",
      "**  Step 2945, val loss = 0.74, val accuracy = 73.44%  **\n",
      "Step: 2950, loss: 0.2480, accuracy: 90.6250%\n",
      "**  Step 2950, val loss = 1.07, val accuracy = 73.44%  **\n",
      "Step: 2955, loss: 0.3217, accuracy: 90.6250%\n",
      "**  Step 2955, val loss = 0.85, val accuracy = 87.50%  **\n",
      "Step: 2960, loss: 0.2737, accuracy: 87.5000%\n",
      "**  Step 2960, val loss = 0.57, val accuracy = 84.38%  **\n",
      "Step: 2965, loss: 0.3726, accuracy: 85.9375%\n",
      "**  Step 2965, val loss = 0.38, val accuracy = 89.06%  **\n",
      "Step: 2970, loss: 0.3042, accuracy: 89.0625%\n",
      "**  Step 2970, val loss = 0.74, val accuracy = 84.38%  **\n",
      "Step: 2975, loss: 0.4989, accuracy: 85.9375%\n",
      "**  Step 2975, val loss = 0.61, val accuracy = 79.69%  **\n",
      "Step: 2980, loss: 0.2734, accuracy: 90.6250%\n",
      "**  Step 2980, val loss = 0.86, val accuracy = 81.25%  **\n",
      "Step: 2985, loss: 0.2494, accuracy: 92.1875%\n",
      "**  Step 2985, val loss = 0.81, val accuracy = 82.81%  **\n",
      "Step: 2990, loss: 0.3414, accuracy: 85.9375%\n",
      "**  Step 2990, val loss = 0.80, val accuracy = 79.69%  **\n",
      "Step: 2995, loss: 0.2609, accuracy: 90.6250%\n",
      "**  Step 2995, val loss = 0.68, val accuracy = 81.25%  **\n",
      "Step: 3000, loss: 0.2376, accuracy: 89.0625%\n",
      "**  Step 3000, val loss = 0.71, val accuracy = 81.25%  **\n",
      "Step: 3005, loss: 0.4400, accuracy: 85.9375%\n",
      "**  Step 3005, val loss = 0.84, val accuracy = 76.56%  **\n",
      "Step: 3010, loss: 0.2896, accuracy: 90.6250%\n",
      "**  Step 3010, val loss = 1.15, val accuracy = 81.25%  **\n",
      "Step: 3015, loss: 0.3185, accuracy: 90.6250%\n",
      "**  Step 3015, val loss = 0.47, val accuracy = 87.50%  **\n",
      "Step: 3020, loss: 0.4693, accuracy: 84.3750%\n",
      "**  Step 3020, val loss = 0.46, val accuracy = 82.81%  **\n",
      "Step: 3025, loss: 0.2569, accuracy: 92.1875%\n",
      "**  Step 3025, val loss = 0.51, val accuracy = 84.38%  **\n",
      "Step: 3030, loss: 0.3544, accuracy: 87.5000%\n",
      "**  Step 3030, val loss = 0.55, val accuracy = 89.06%  **\n",
      "Step: 3035, loss: 0.2569, accuracy: 92.1875%\n",
      "**  Step 3035, val loss = 0.74, val accuracy = 84.38%  **\n",
      "Step: 3040, loss: 0.2291, accuracy: 92.1875%\n",
      "**  Step 3040, val loss = 0.64, val accuracy = 81.25%  **\n",
      "Step: 3045, loss: 0.1023, accuracy: 98.4375%\n",
      "**  Step 3045, val loss = 0.56, val accuracy = 79.69%  **\n",
      "Step: 3050, loss: 0.3023, accuracy: 89.0625%\n",
      "**  Step 3050, val loss = 1.00, val accuracy = 73.44%  **\n",
      "Step: 3055, loss: 0.2637, accuracy: 92.1875%\n",
      "**  Step 3055, val loss = 0.61, val accuracy = 85.94%  **\n",
      "Step: 3060, loss: 0.1885, accuracy: 95.3125%\n",
      "**  Step 3060, val loss = 0.66, val accuracy = 84.38%  **\n",
      "Step: 3065, loss: 0.4506, accuracy: 89.0625%\n",
      "**  Step 3065, val loss = 0.54, val accuracy = 84.38%  **\n",
      "Step: 3070, loss: 0.3650, accuracy: 92.1875%\n",
      "**  Step 3070, val loss = 0.63, val accuracy = 78.12%  **\n",
      "Step: 3075, loss: 0.4111, accuracy: 82.8125%\n",
      "**  Step 3075, val loss = 0.64, val accuracy = 75.00%  **\n",
      "Step: 3080, loss: 0.2283, accuracy: 90.6250%\n",
      "**  Step 3080, val loss = 0.61, val accuracy = 78.12%  **\n",
      "Step: 3085, loss: 0.3808, accuracy: 90.6250%\n",
      "**  Step 3085, val loss = 0.66, val accuracy = 81.25%  **\n",
      "Step: 3090, loss: 0.3119, accuracy: 87.5000%\n",
      "**  Step 3090, val loss = 0.40, val accuracy = 84.38%  **\n",
      "Step: 3095, loss: 0.2753, accuracy: 89.0625%\n",
      "**  Step 3095, val loss = 0.40, val accuracy = 87.50%  **\n",
      "Step: 3100, loss: 0.2058, accuracy: 92.1875%\n",
      "**  Step 3100, val loss = 0.70, val accuracy = 81.25%  **\n",
      "Step: 3105, loss: 0.3407, accuracy: 87.5000%\n",
      "**  Step 3105, val loss = 0.83, val accuracy = 76.56%  **\n",
      "Step: 3110, loss: 0.2559, accuracy: 93.7500%\n",
      "**  Step 3110, val loss = 1.05, val accuracy = 71.88%  **\n",
      "Step: 3115, loss: 0.2137, accuracy: 93.7500%\n",
      "**  Step 3115, val loss = 0.71, val accuracy = 82.81%  **\n",
      "Step: 3120, loss: 0.3176, accuracy: 92.1875%\n",
      "**  Step 3120, val loss = 0.85, val accuracy = 78.12%  **\n",
      "Step: 3125, loss: 0.2356, accuracy: 93.7500%\n",
      "**  Step 3125, val loss = 0.57, val accuracy = 81.25%  **\n",
      "Step: 3130, loss: 0.3183, accuracy: 87.5000%\n",
      "**  Step 3130, val loss = 0.61, val accuracy = 84.38%  **\n",
      "Step: 3135, loss: 0.2187, accuracy: 93.7500%\n",
      "**  Step 3135, val loss = 0.71, val accuracy = 79.69%  **\n",
      "Step: 3140, loss: 0.2056, accuracy: 90.6250%\n",
      "**  Step 3140, val loss = 0.42, val accuracy = 87.50%  **\n",
      "Step: 3145, loss: 0.2000, accuracy: 93.7500%\n",
      "**  Step 3145, val loss = 0.83, val accuracy = 81.25%  **\n",
      "Step: 3150, loss: 0.3578, accuracy: 85.9375%\n",
      "**  Step 3150, val loss = 0.70, val accuracy = 76.56%  **\n",
      "Step: 3155, loss: 0.2990, accuracy: 90.6250%\n",
      "**  Step 3155, val loss = 0.68, val accuracy = 78.12%  **\n",
      "Step: 3160, loss: 0.3069, accuracy: 85.9375%\n",
      "**  Step 3160, val loss = 0.65, val accuracy = 81.25%  **\n",
      "Step: 3165, loss: 0.3282, accuracy: 90.6250%\n",
      "**  Step 3165, val loss = 0.83, val accuracy = 76.56%  **\n",
      "Step: 3170, loss: 0.2698, accuracy: 92.1875%\n",
      "**  Step 3170, val loss = 0.64, val accuracy = 76.56%  **\n",
      "Step: 3175, loss: 0.2926, accuracy: 92.1875%\n",
      "**  Step 3175, val loss = 0.79, val accuracy = 78.12%  **\n",
      "Step: 3180, loss: 0.2365, accuracy: 93.7500%\n",
      "**  Step 3180, val loss = 0.70, val accuracy = 79.69%  **\n",
      "Step: 3185, loss: 0.1756, accuracy: 93.7500%\n",
      "**  Step 3185, val loss = 0.44, val accuracy = 87.50%  **\n",
      "Step: 3190, loss: 0.3237, accuracy: 87.5000%\n",
      "**  Step 3190, val loss = 0.57, val accuracy = 85.94%  **\n",
      "Step: 3195, loss: 0.3221, accuracy: 87.5000%\n",
      "**  Step 3195, val loss = 0.35, val accuracy = 87.50%  **\n",
      "Step: 3200, loss: 0.2416, accuracy: 89.0625%\n",
      "**  Step 3200, val loss = 0.27, val accuracy = 92.19%  **\n",
      "Step: 3205, loss: 0.4282, accuracy: 81.2500%\n",
      "**  Step 3205, val loss = 0.95, val accuracy = 75.00%  **\n",
      "Step: 3210, loss: 0.3570, accuracy: 87.5000%\n",
      "**  Step 3210, val loss = 0.49, val accuracy = 79.69%  **\n",
      "Step: 3215, loss: 0.3678, accuracy: 82.8125%\n",
      "**  Step 3215, val loss = 1.05, val accuracy = 73.44%  **\n",
      "Step: 3220, loss: 0.3345, accuracy: 90.6250%\n",
      "**  Step 3220, val loss = 0.44, val accuracy = 84.38%  **\n",
      "Step: 3225, loss: 0.2715, accuracy: 90.6250%\n",
      "**  Step 3225, val loss = 0.56, val accuracy = 78.12%  **\n",
      "Step: 3230, loss: 0.2256, accuracy: 93.7500%\n",
      "**  Step 3230, val loss = 0.72, val accuracy = 79.69%  **\n",
      "Step: 3235, loss: 0.1817, accuracy: 93.7500%\n",
      "**  Step 3235, val loss = 0.28, val accuracy = 90.62%  **\n",
      "Step: 3240, loss: 0.3616, accuracy: 87.5000%\n",
      "**  Step 3240, val loss = 0.35, val accuracy = 89.06%  **\n",
      "Step: 3245, loss: 0.2723, accuracy: 87.5000%\n",
      "**  Step 3245, val loss = 0.34, val accuracy = 85.94%  **\n",
      "Step: 3250, loss: 0.2158, accuracy: 93.7500%\n",
      "**  Step 3250, val loss = 0.59, val accuracy = 82.81%  **\n",
      "Step: 3255, loss: 0.3286, accuracy: 89.0625%\n",
      "**  Step 3255, val loss = 0.61, val accuracy = 76.56%  **\n",
      "Step: 3260, loss: 0.2021, accuracy: 96.8750%\n",
      "**  Step 3260, val loss = 0.66, val accuracy = 81.25%  **\n",
      "Step: 3265, loss: 0.2024, accuracy: 95.3125%\n",
      "**  Step 3265, val loss = 0.80, val accuracy = 78.12%  **\n",
      "Step: 3270, loss: 0.3733, accuracy: 82.8125%\n",
      "**  Step 3270, val loss = 0.56, val accuracy = 85.94%  **\n",
      "Step: 3275, loss: 0.2302, accuracy: 92.1875%\n",
      "**  Step 3275, val loss = 0.41, val accuracy = 89.06%  **\n",
      "Step: 3280, loss: 0.1405, accuracy: 96.8750%\n",
      "**  Step 3280, val loss = 0.59, val accuracy = 79.69%  **\n",
      "Step: 3285, loss: 0.1700, accuracy: 95.3125%\n",
      "**  Step 3285, val loss = 0.65, val accuracy = 79.69%  **\n",
      "Step: 3290, loss: 0.3188, accuracy: 89.0625%\n",
      "**  Step 3290, val loss = 0.43, val accuracy = 84.38%  **\n",
      "Step: 3295, loss: 0.2837, accuracy: 93.7500%\n",
      "**  Step 3295, val loss = 0.76, val accuracy = 78.12%  **\n",
      "Step: 3300, loss: 0.2773, accuracy: 89.0625%\n",
      "**  Step 3300, val loss = 0.70, val accuracy = 81.25%  **\n",
      "Step: 3305, loss: 0.2602, accuracy: 90.6250%\n",
      "**  Step 3305, val loss = 0.42, val accuracy = 89.06%  **\n",
      "Step: 3310, loss: 0.4043, accuracy: 87.5000%\n",
      "**  Step 3310, val loss = 0.58, val accuracy = 79.69%  **\n",
      "Step: 3315, loss: 0.1219, accuracy: 96.8750%\n",
      "**  Step 3315, val loss = 0.84, val accuracy = 78.12%  **\n",
      "Step: 3320, loss: 0.0862, accuracy: 98.4375%\n",
      "**  Step 3320, val loss = 0.66, val accuracy = 79.69%  **\n",
      "Step: 3325, loss: 0.1910, accuracy: 92.1875%\n",
      "**  Step 3325, val loss = 0.77, val accuracy = 82.81%  **\n",
      "Step: 3330, loss: 0.2442, accuracy: 90.6250%\n",
      "**  Step 3330, val loss = 0.73, val accuracy = 75.00%  **\n",
      "Step: 3335, loss: 0.1672, accuracy: 96.8750%\n",
      "**  Step 3335, val loss = 0.51, val accuracy = 85.94%  **\n",
      "Step: 3340, loss: 0.2119, accuracy: 93.7500%\n",
      "**  Step 3340, val loss = 0.53, val accuracy = 81.25%  **\n",
      "Step: 3345, loss: 0.2266, accuracy: 90.6250%\n",
      "**  Step 3345, val loss = 0.73, val accuracy = 85.94%  **\n",
      "Step: 3350, loss: 0.2723, accuracy: 90.6250%\n",
      "**  Step 3350, val loss = 0.72, val accuracy = 79.69%  **\n",
      "Step: 3355, loss: 0.2432, accuracy: 93.7500%\n",
      "**  Step 3355, val loss = 0.43, val accuracy = 87.50%  **\n",
      "Step: 3360, loss: 0.2591, accuracy: 93.7500%\n",
      "**  Step 3360, val loss = 0.40, val accuracy = 90.62%  **\n",
      "Step: 3365, loss: 0.2988, accuracy: 87.5000%\n",
      "**  Step 3365, val loss = 0.55, val accuracy = 84.38%  **\n",
      "Step: 3370, loss: 0.2345, accuracy: 92.1875%\n",
      "**  Step 3370, val loss = 0.55, val accuracy = 87.50%  **\n",
      "Step: 3375, loss: 0.2306, accuracy: 93.7500%\n",
      "**  Step 3375, val loss = 0.63, val accuracy = 82.81%  **\n",
      "Step: 3380, loss: 0.3168, accuracy: 90.6250%\n",
      "**  Step 3380, val loss = 0.66, val accuracy = 76.56%  **\n",
      "Step: 3385, loss: 0.2485, accuracy: 90.6250%\n",
      "**  Step 3385, val loss = 0.44, val accuracy = 81.25%  **\n",
      "Step: 3390, loss: 0.3986, accuracy: 84.3750%\n",
      "**  Step 3390, val loss = 0.34, val accuracy = 87.50%  **\n",
      "Step: 3395, loss: 0.2022, accuracy: 95.3125%\n",
      "**  Step 3395, val loss = 0.83, val accuracy = 79.69%  **\n",
      "Step: 3400, loss: 0.2703, accuracy: 90.6250%\n",
      "**  Step 3400, val loss = 0.63, val accuracy = 79.69%  **\n",
      "Step: 3405, loss: 0.3084, accuracy: 93.7500%\n",
      "**  Step 3405, val loss = 0.56, val accuracy = 82.81%  **\n",
      "Step: 3410, loss: 0.3012, accuracy: 89.0625%\n",
      "**  Step 3410, val loss = 0.97, val accuracy = 71.88%  **\n",
      "Step: 3415, loss: 0.3284, accuracy: 90.6250%\n",
      "**  Step 3415, val loss = 0.71, val accuracy = 81.25%  **\n",
      "Step: 3420, loss: 0.3179, accuracy: 87.5000%\n",
      "**  Step 3420, val loss = 0.71, val accuracy = 81.25%  **\n",
      "Step: 3425, loss: 0.4340, accuracy: 82.8125%\n",
      "**  Step 3425, val loss = 0.52, val accuracy = 82.81%  **\n",
      "Step: 3430, loss: 0.3333, accuracy: 85.9375%\n",
      "**  Step 3430, val loss = 0.66, val accuracy = 82.81%  **\n",
      "Step: 3435, loss: 0.2659, accuracy: 89.0625%\n",
      "**  Step 3435, val loss = 0.82, val accuracy = 76.56%  **\n",
      "Step: 3440, loss: 0.3132, accuracy: 89.0625%\n",
      "**  Step 3440, val loss = 0.60, val accuracy = 79.69%  **\n",
      "Step: 3445, loss: 0.2804, accuracy: 95.3125%\n",
      "**  Step 3445, val loss = 0.58, val accuracy = 84.38%  **\n",
      "Step: 3450, loss: 0.2738, accuracy: 89.0625%\n",
      "**  Step 3450, val loss = 0.68, val accuracy = 79.69%  **\n",
      "Step: 3455, loss: 0.2474, accuracy: 93.7500%\n",
      "**  Step 3455, val loss = 0.76, val accuracy = 79.69%  **\n",
      "Step: 3460, loss: 0.1959, accuracy: 95.3125%\n",
      "**  Step 3460, val loss = 0.77, val accuracy = 78.12%  **\n",
      "Step: 3465, loss: 0.2282, accuracy: 95.3125%\n",
      "**  Step 3465, val loss = 0.59, val accuracy = 79.69%  **\n",
      "Step: 3470, loss: 0.3254, accuracy: 92.1875%\n",
      "**  Step 3470, val loss = 0.43, val accuracy = 89.06%  **\n",
      "Step: 3475, loss: 0.1510, accuracy: 98.4375%\n",
      "**  Step 3475, val loss = 0.78, val accuracy = 79.69%  **\n",
      "Step: 3480, loss: 0.2340, accuracy: 90.6250%\n",
      "**  Step 3480, val loss = 0.39, val accuracy = 89.06%  **\n",
      "Step: 3485, loss: 0.1996, accuracy: 93.7500%\n",
      "**  Step 3485, val loss = 0.45, val accuracy = 84.38%  **\n",
      "Step: 3490, loss: 0.1506, accuracy: 93.7500%\n",
      "**  Step 3490, val loss = 0.67, val accuracy = 76.56%  **\n",
      "Step: 3495, loss: 0.4017, accuracy: 85.9375%\n",
      "**  Step 3495, val loss = 0.59, val accuracy = 82.81%  **\n",
      "Step: 3500, loss: 0.3669, accuracy: 85.9375%\n",
      "**  Step 3500, val loss = 0.39, val accuracy = 82.81%  **\n",
      "Step: 3505, loss: 0.4499, accuracy: 81.2500%\n",
      "**  Step 3505, val loss = 0.54, val accuracy = 87.50%  **\n",
      "Step: 3510, loss: 0.4415, accuracy: 87.5000%\n",
      "**  Step 3510, val loss = 0.47, val accuracy = 87.50%  **\n",
      "Step: 3515, loss: 0.2661, accuracy: 92.1875%\n",
      "**  Step 3515, val loss = 0.68, val accuracy = 84.38%  **\n",
      "Step: 3520, loss: 0.3051, accuracy: 92.1875%\n",
      "**  Step 3520, val loss = 0.45, val accuracy = 85.94%  **\n",
      "Step: 3525, loss: 0.1249, accuracy: 98.4375%\n",
      "**  Step 3525, val loss = 0.75, val accuracy = 76.56%  **\n",
      "Step: 3530, loss: 0.2317, accuracy: 92.1875%\n",
      "**  Step 3530, val loss = 0.83, val accuracy = 68.75%  **\n",
      "Step: 3535, loss: 0.3330, accuracy: 89.0625%\n",
      "**  Step 3535, val loss = 0.34, val accuracy = 92.19%  **\n",
      "Step: 3540, loss: 0.2167, accuracy: 96.8750%\n",
      "**  Step 3540, val loss = 0.68, val accuracy = 79.69%  **\n",
      "Step: 3545, loss: 0.2439, accuracy: 90.6250%\n",
      "**  Step 3545, val loss = 0.37, val accuracy = 90.62%  **\n",
      "Step: 3550, loss: 0.3100, accuracy: 90.6250%\n",
      "**  Step 3550, val loss = 0.68, val accuracy = 82.81%  **\n",
      "Step: 3555, loss: 0.1544, accuracy: 96.8750%\n",
      "**  Step 3555, val loss = 0.74, val accuracy = 82.81%  **\n",
      "Step: 3560, loss: 0.3602, accuracy: 90.6250%\n",
      "**  Step 3560, val loss = 0.73, val accuracy = 82.81%  **\n",
      "Step: 3565, loss: 0.2194, accuracy: 95.3125%\n",
      "**  Step 3565, val loss = 0.42, val accuracy = 87.50%  **\n",
      "Step: 3570, loss: 0.3275, accuracy: 92.1875%\n",
      "**  Step 3570, val loss = 0.66, val accuracy = 79.69%  **\n",
      "Step: 3575, loss: 0.2010, accuracy: 93.7500%\n",
      "**  Step 3575, val loss = 0.38, val accuracy = 85.94%  **\n",
      "Step: 3580, loss: 0.1863, accuracy: 95.3125%\n",
      "**  Step 3580, val loss = 0.41, val accuracy = 89.06%  **\n",
      "Step: 3585, loss: 0.1808, accuracy: 90.6250%\n",
      "**  Step 3585, val loss = 0.54, val accuracy = 84.38%  **\n",
      "Step: 3590, loss: 0.3387, accuracy: 89.0625%\n",
      "**  Step 3590, val loss = 0.48, val accuracy = 81.25%  **\n",
      "Step: 3595, loss: 0.2092, accuracy: 93.7500%\n",
      "**  Step 3595, val loss = 0.65, val accuracy = 79.69%  **\n",
      "Step: 3600, loss: 0.2195, accuracy: 90.6250%\n",
      "**  Step 3600, val loss = 0.75, val accuracy = 82.81%  **\n",
      "Step: 3605, loss: 0.2110, accuracy: 92.1875%\n",
      "**  Step 3605, val loss = 0.75, val accuracy = 79.69%  **\n",
      "Step: 3610, loss: 0.1814, accuracy: 92.1875%\n",
      "**  Step 3610, val loss = 0.70, val accuracy = 81.25%  **\n",
      "Step: 3615, loss: 0.1603, accuracy: 95.3125%\n",
      "**  Step 3615, val loss = 0.64, val accuracy = 82.81%  **\n",
      "Step: 3620, loss: 0.1955, accuracy: 96.8750%\n",
      "**  Step 3620, val loss = 0.79, val accuracy = 76.56%  **\n",
      "Step: 3625, loss: 0.3737, accuracy: 90.6250%\n",
      "**  Step 3625, val loss = 0.65, val accuracy = 81.25%  **\n",
      "Step: 3630, loss: 0.1656, accuracy: 96.8750%\n",
      "**  Step 3630, val loss = 0.43, val accuracy = 87.50%  **\n",
      "Step: 3635, loss: 0.2964, accuracy: 85.9375%\n",
      "**  Step 3635, val loss = 0.92, val accuracy = 79.69%  **\n",
      "Step: 3640, loss: 0.3172, accuracy: 90.6250%\n",
      "**  Step 3640, val loss = 0.45, val accuracy = 82.81%  **\n",
      "Step: 3645, loss: 0.2397, accuracy: 90.6250%\n",
      "**  Step 3645, val loss = 0.52, val accuracy = 82.81%  **\n",
      "Step: 3650, loss: 0.3157, accuracy: 92.1875%\n",
      "**  Step 3650, val loss = 0.61, val accuracy = 84.38%  **\n",
      "Step: 3655, loss: 0.1611, accuracy: 95.3125%\n",
      "**  Step 3655, val loss = 0.42, val accuracy = 85.94%  **\n",
      "Step: 3660, loss: 0.2744, accuracy: 92.1875%\n",
      "**  Step 3660, val loss = 0.66, val accuracy = 84.38%  **\n",
      "Step: 3665, loss: 0.2417, accuracy: 95.3125%\n",
      "**  Step 3665, val loss = 0.76, val accuracy = 76.56%  **\n",
      "Step: 3670, loss: 0.1889, accuracy: 93.7500%\n",
      "**  Step 3670, val loss = 0.82, val accuracy = 75.00%  **\n",
      "Step: 3675, loss: 0.3497, accuracy: 85.9375%\n",
      "**  Step 3675, val loss = 0.65, val accuracy = 79.69%  **\n",
      "Step: 3680, loss: 0.1770, accuracy: 92.1875%\n",
      "**  Step 3680, val loss = 0.63, val accuracy = 85.94%  **\n",
      "Step: 3685, loss: 0.2580, accuracy: 95.3125%\n",
      "**  Step 3685, val loss = 0.64, val accuracy = 92.19%  **\n",
      "Step: 3690, loss: 0.2331, accuracy: 87.5000%\n",
      "**  Step 3690, val loss = 0.38, val accuracy = 84.38%  **\n",
      "Step: 3695, loss: 0.2557, accuracy: 92.1875%\n",
      "**  Step 3695, val loss = 0.39, val accuracy = 85.94%  **\n",
      "Step: 3700, loss: 0.1370, accuracy: 98.4375%\n",
      "**  Step 3700, val loss = 0.49, val accuracy = 87.50%  **\n",
      "Step: 3705, loss: 0.2355, accuracy: 89.0625%\n",
      "**  Step 3705, val loss = 0.76, val accuracy = 81.25%  **\n",
      "Step: 3710, loss: 0.6320, accuracy: 87.5000%\n",
      "**  Step 3710, val loss = 0.58, val accuracy = 85.94%  **\n",
      "Step: 3715, loss: 0.1632, accuracy: 96.8750%\n",
      "**  Step 3715, val loss = 0.85, val accuracy = 81.25%  **\n",
      "Step: 3720, loss: 0.1948, accuracy: 95.3125%\n",
      "**  Step 3720, val loss = 0.52, val accuracy = 81.25%  **\n",
      "Step: 3725, loss: 0.2460, accuracy: 89.0625%\n",
      "**  Step 3725, val loss = 0.50, val accuracy = 82.81%  **\n",
      "Step: 3730, loss: 0.4821, accuracy: 87.5000%\n",
      "**  Step 3730, val loss = 1.08, val accuracy = 76.56%  **\n",
      "Step: 3735, loss: 0.2676, accuracy: 89.0625%\n",
      "**  Step 3735, val loss = 0.59, val accuracy = 82.81%  **\n",
      "Step: 3740, loss: 0.3500, accuracy: 89.0625%\n",
      "**  Step 3740, val loss = 0.49, val accuracy = 84.38%  **\n",
      "Step: 3745, loss: 0.2140, accuracy: 93.7500%\n",
      "**  Step 3745, val loss = 0.98, val accuracy = 78.12%  **\n",
      "Step: 3750, loss: 0.2546, accuracy: 93.7500%\n",
      "**  Step 3750, val loss = 0.45, val accuracy = 89.06%  **\n",
      "Step: 3755, loss: 0.2664, accuracy: 92.1875%\n",
      "**  Step 3755, val loss = 0.38, val accuracy = 85.94%  **\n",
      "Step: 3760, loss: 0.2863, accuracy: 95.3125%\n",
      "**  Step 3760, val loss = 0.93, val accuracy = 75.00%  **\n",
      "Step: 3765, loss: 0.0973, accuracy: 100.0000%\n",
      "**  Step 3765, val loss = 0.79, val accuracy = 81.25%  **\n",
      "Step: 3770, loss: 0.2743, accuracy: 92.1875%\n",
      "**  Step 3770, val loss = 1.05, val accuracy = 76.56%  **\n",
      "Step: 3775, loss: 0.2527, accuracy: 90.6250%\n",
      "**  Step 3775, val loss = 0.76, val accuracy = 78.12%  **\n",
      "Step: 3780, loss: 0.2458, accuracy: 92.1875%\n",
      "**  Step 3780, val loss = 0.39, val accuracy = 87.50%  **\n",
      "Step: 3785, loss: 0.1954, accuracy: 95.3125%\n",
      "**  Step 3785, val loss = 0.50, val accuracy = 87.50%  **\n",
      "Step: 3790, loss: 0.1487, accuracy: 95.3125%\n",
      "**  Step 3790, val loss = 0.68, val accuracy = 82.81%  **\n",
      "Step: 3795, loss: 0.3460, accuracy: 85.9375%\n",
      "**  Step 3795, val loss = 0.90, val accuracy = 78.12%  **\n",
      "Step: 3800, loss: 0.3015, accuracy: 90.6250%\n",
      "**  Step 3800, val loss = 0.60, val accuracy = 82.81%  **\n",
      "Step: 3805, loss: 0.3185, accuracy: 89.0625%\n",
      "**  Step 3805, val loss = 0.75, val accuracy = 79.69%  **\n",
      "Step: 3810, loss: 0.2197, accuracy: 90.6250%\n",
      "**  Step 3810, val loss = 0.47, val accuracy = 87.50%  **\n",
      "Step: 3815, loss: 0.3514, accuracy: 87.5000%\n",
      "**  Step 3815, val loss = 0.84, val accuracy = 71.88%  **\n",
      "Step: 3820, loss: 0.2269, accuracy: 95.3125%\n",
      "**  Step 3820, val loss = 0.82, val accuracy = 73.44%  **\n",
      "Step: 3825, loss: 0.2738, accuracy: 90.6250%\n",
      "**  Step 3825, val loss = 0.47, val accuracy = 87.50%  **\n",
      "Step: 3830, loss: 0.1923, accuracy: 96.8750%\n",
      "**  Step 3830, val loss = 0.81, val accuracy = 79.69%  **\n",
      "Step: 3835, loss: 0.1559, accuracy: 98.4375%\n",
      "**  Step 3835, val loss = 0.39, val accuracy = 89.06%  **\n",
      "Step: 3840, loss: 0.5877, accuracy: 85.9375%\n",
      "**  Step 3840, val loss = 0.47, val accuracy = 79.69%  **\n",
      "Step: 3845, loss: 0.3099, accuracy: 87.5000%\n",
      "**  Step 3845, val loss = 0.32, val accuracy = 87.50%  **\n",
      "Step: 3850, loss: 0.2011, accuracy: 90.6250%\n",
      "**  Step 3850, val loss = 0.49, val accuracy = 81.25%  **\n",
      "Step: 3855, loss: 0.1820, accuracy: 93.7500%\n",
      "**  Step 3855, val loss = 0.43, val accuracy = 87.50%  **\n",
      "Step: 3860, loss: 0.2072, accuracy: 95.3125%\n",
      "**  Step 3860, val loss = 0.61, val accuracy = 81.25%  **\n",
      "Step: 3865, loss: 0.2138, accuracy: 92.1875%\n",
      "**  Step 3865, val loss = 0.78, val accuracy = 78.12%  **\n",
      "Step: 3870, loss: 0.2487, accuracy: 92.1875%\n",
      "**  Step 3870, val loss = 0.53, val accuracy = 84.38%  **\n",
      "Step: 3875, loss: 0.3384, accuracy: 85.9375%\n",
      "**  Step 3875, val loss = 0.51, val accuracy = 82.81%  **\n",
      "Step: 3880, loss: 0.1450, accuracy: 96.8750%\n",
      "**  Step 3880, val loss = 0.90, val accuracy = 82.81%  **\n",
      "Step: 3885, loss: 0.1673, accuracy: 95.3125%\n",
      "**  Step 3885, val loss = 0.55, val accuracy = 79.69%  **\n",
      "Step: 3890, loss: 0.1751, accuracy: 95.3125%\n",
      "**  Step 3890, val loss = 0.28, val accuracy = 89.06%  **\n",
      "Step: 3895, loss: 0.1473, accuracy: 96.8750%\n",
      "**  Step 3895, val loss = 0.53, val accuracy = 87.50%  **\n",
      "Step: 3900, loss: 0.1399, accuracy: 96.8750%\n",
      "**  Step 3900, val loss = 0.70, val accuracy = 76.56%  **\n",
      "Step: 3905, loss: 0.2268, accuracy: 90.6250%\n",
      "**  Step 3905, val loss = 0.44, val accuracy = 81.25%  **\n",
      "Step: 3910, loss: 0.0821, accuracy: 98.4375%\n",
      "**  Step 3910, val loss = 0.44, val accuracy = 82.81%  **\n",
      "Step: 3915, loss: 0.2807, accuracy: 90.6250%\n",
      "**  Step 3915, val loss = 0.68, val accuracy = 82.81%  **\n",
      "Step: 3920, loss: 0.1786, accuracy: 93.7500%\n",
      "**  Step 3920, val loss = 0.57, val accuracy = 79.69%  **\n",
      "Step: 3925, loss: 0.1566, accuracy: 95.3125%\n",
      "**  Step 3925, val loss = 0.49, val accuracy = 82.81%  **\n",
      "Step: 3930, loss: 0.1175, accuracy: 98.4375%\n",
      "**  Step 3930, val loss = 0.49, val accuracy = 82.81%  **\n",
      "Step: 3935, loss: 0.1827, accuracy: 95.3125%\n",
      "**  Step 3935, val loss = 0.79, val accuracy = 82.81%  **\n",
      "Step: 3940, loss: 0.1988, accuracy: 95.3125%\n",
      "**  Step 3940, val loss = 0.42, val accuracy = 87.50%  **\n",
      "Step: 3945, loss: 0.1655, accuracy: 95.3125%\n",
      "**  Step 3945, val loss = 0.72, val accuracy = 73.44%  **\n",
      "Step: 3950, loss: 0.1725, accuracy: 95.3125%\n",
      "**  Step 3950, val loss = 0.56, val accuracy = 79.69%  **\n",
      "Step: 3955, loss: 0.1205, accuracy: 95.3125%\n",
      "**  Step 3955, val loss = 0.60, val accuracy = 85.94%  **\n",
      "Step: 3960, loss: 0.2509, accuracy: 89.0625%\n",
      "**  Step 3960, val loss = 0.87, val accuracy = 78.12%  **\n",
      "Step: 3965, loss: 0.3513, accuracy: 89.0625%\n",
      "**  Step 3965, val loss = 0.52, val accuracy = 85.94%  **\n",
      "Step: 3970, loss: 0.1559, accuracy: 93.7500%\n",
      "**  Step 3970, val loss = 0.36, val accuracy = 92.19%  **\n",
      "Step: 3975, loss: 0.3022, accuracy: 90.6250%\n",
      "**  Step 3975, val loss = 0.78, val accuracy = 79.69%  **\n",
      "Step: 3980, loss: 0.2904, accuracy: 92.1875%\n",
      "**  Step 3980, val loss = 0.70, val accuracy = 79.69%  **\n",
      "Step: 3985, loss: 0.2147, accuracy: 93.7500%\n",
      "**  Step 3985, val loss = 0.43, val accuracy = 89.06%  **\n",
      "Step: 3990, loss: 0.1360, accuracy: 98.4375%\n",
      "**  Step 3990, val loss = 0.39, val accuracy = 90.62%  **\n",
      "Step: 3995, loss: 0.2837, accuracy: 89.0625%\n",
      "**  Step 3995, val loss = 0.44, val accuracy = 87.50%  **\n",
      "Step: 4000, loss: 0.3739, accuracy: 87.5000%\n",
      "**  Step 4000, val loss = 0.55, val accuracy = 81.25%  **\n",
      "Step: 4005, loss: 0.1952, accuracy: 95.3125%\n",
      "**  Step 4005, val loss = 0.45, val accuracy = 87.50%  **\n",
      "Step: 4010, loss: 0.2825, accuracy: 95.3125%\n",
      "**  Step 4010, val loss = 0.56, val accuracy = 79.69%  **\n",
      "Step: 4015, loss: 0.1556, accuracy: 95.3125%\n",
      "**  Step 4015, val loss = 0.99, val accuracy = 76.56%  **\n",
      "Step: 4020, loss: 0.1827, accuracy: 92.1875%\n",
      "**  Step 4020, val loss = 0.53, val accuracy = 82.81%  **\n",
      "Step: 4025, loss: 0.2695, accuracy: 89.0625%\n",
      "**  Step 4025, val loss = 0.56, val accuracy = 79.69%  **\n",
      "Step: 4030, loss: 0.2046, accuracy: 92.1875%\n",
      "**  Step 4030, val loss = 0.35, val accuracy = 90.62%  **\n",
      "Step: 4035, loss: 0.2185, accuracy: 92.1875%\n",
      "**  Step 4035, val loss = 0.49, val accuracy = 84.38%  **\n",
      "Step: 4040, loss: 0.2006, accuracy: 93.7500%\n",
      "**  Step 4040, val loss = 1.10, val accuracy = 70.31%  **\n",
      "Step: 4045, loss: 0.1931, accuracy: 95.3125%\n",
      "**  Step 4045, val loss = 0.44, val accuracy = 85.94%  **\n",
      "Step: 4050, loss: 0.4209, accuracy: 89.0625%\n",
      "**  Step 4050, val loss = 0.29, val accuracy = 92.19%  **\n",
      "Step: 4055, loss: 0.1998, accuracy: 90.6250%\n",
      "**  Step 4055, val loss = 0.60, val accuracy = 79.69%  **\n",
      "Step: 4060, loss: 0.1235, accuracy: 98.4375%\n",
      "**  Step 4060, val loss = 0.20, val accuracy = 93.75%  **\n",
      "Step: 4065, loss: 0.3985, accuracy: 92.1875%\n",
      "**  Step 4065, val loss = 0.64, val accuracy = 81.25%  **\n",
      "Step: 4070, loss: 0.1332, accuracy: 98.4375%\n",
      "**  Step 4070, val loss = 0.48, val accuracy = 81.25%  **\n",
      "Step: 4075, loss: 0.2337, accuracy: 92.1875%\n",
      "**  Step 4075, val loss = 0.33, val accuracy = 87.50%  **\n",
      "Step: 4080, loss: 0.1459, accuracy: 96.8750%\n",
      "**  Step 4080, val loss = 0.57, val accuracy = 85.94%  **\n",
      "Step: 4085, loss: 0.1504, accuracy: 96.8750%\n",
      "**  Step 4085, val loss = 0.44, val accuracy = 87.50%  **\n",
      "Step: 4090, loss: 0.1413, accuracy: 96.8750%\n",
      "**  Step 4090, val loss = 0.24, val accuracy = 89.06%  **\n",
      "Step: 4095, loss: 0.1701, accuracy: 93.7500%\n",
      "**  Step 4095, val loss = 0.67, val accuracy = 84.38%  **\n",
      "Step: 4100, loss: 0.2608, accuracy: 89.0625%\n",
      "**  Step 4100, val loss = 0.94, val accuracy = 75.00%  **\n",
      "Step: 4105, loss: 0.1994, accuracy: 93.7500%\n",
      "**  Step 4105, val loss = 0.77, val accuracy = 79.69%  **\n",
      "Step: 4110, loss: 0.2272, accuracy: 90.6250%\n",
      "**  Step 4110, val loss = 0.42, val accuracy = 85.94%  **\n",
      "Step: 4115, loss: 0.1956, accuracy: 93.7500%\n",
      "**  Step 4115, val loss = 0.73, val accuracy = 82.81%  **\n",
      "Step: 4120, loss: 0.1901, accuracy: 95.3125%\n",
      "**  Step 4120, val loss = 0.83, val accuracy = 79.69%  **\n",
      "Step: 4125, loss: 0.2607, accuracy: 95.3125%\n",
      "**  Step 4125, val loss = 0.76, val accuracy = 82.81%  **\n",
      "Step: 4130, loss: 0.1988, accuracy: 95.3125%\n",
      "**  Step 4130, val loss = 0.75, val accuracy = 75.00%  **\n",
      "Step: 4135, loss: 0.1844, accuracy: 93.7500%\n",
      "**  Step 4135, val loss = 0.45, val accuracy = 89.06%  **\n",
      "Step: 4140, loss: 0.3066, accuracy: 87.5000%\n",
      "**  Step 4140, val loss = 0.73, val accuracy = 78.12%  **\n",
      "Step: 4145, loss: 0.1459, accuracy: 96.8750%\n",
      "**  Step 4145, val loss = 0.81, val accuracy = 79.69%  **\n",
      "Step: 4150, loss: 0.2268, accuracy: 90.6250%\n",
      "**  Step 4150, val loss = 0.40, val accuracy = 87.50%  **\n",
      "Step: 4155, loss: 0.1741, accuracy: 93.7500%\n",
      "**  Step 4155, val loss = 0.55, val accuracy = 82.81%  **\n",
      "Step: 4160, loss: 0.2077, accuracy: 92.1875%\n",
      "**  Step 4160, val loss = 0.55, val accuracy = 82.81%  **\n",
      "Step: 4165, loss: 0.1646, accuracy: 95.3125%\n",
      "**  Step 4165, val loss = 0.69, val accuracy = 87.50%  **\n",
      "Step: 4170, loss: 0.1875, accuracy: 92.1875%\n",
      "**  Step 4170, val loss = 0.39, val accuracy = 85.94%  **\n",
      "Step: 4175, loss: 0.1653, accuracy: 93.7500%\n",
      "**  Step 4175, val loss = 0.64, val accuracy = 84.38%  **\n",
      "Step: 4180, loss: 0.2245, accuracy: 92.1875%\n",
      "**  Step 4180, val loss = 0.55, val accuracy = 85.94%  **\n",
      "Step: 4185, loss: 0.2001, accuracy: 92.1875%\n",
      "**  Step 4185, val loss = 0.51, val accuracy = 84.38%  **\n",
      "Step: 4190, loss: 0.1191, accuracy: 96.8750%\n",
      "**  Step 4190, val loss = 0.40, val accuracy = 89.06%  **\n",
      "Step: 4195, loss: 0.1224, accuracy: 95.3125%\n",
      "**  Step 4195, val loss = 0.66, val accuracy = 79.69%  **\n",
      "Step: 4200, loss: 0.4592, accuracy: 89.0625%\n",
      "**  Step 4200, val loss = 0.47, val accuracy = 85.94%  **\n",
      "Step: 4205, loss: 0.3138, accuracy: 89.0625%\n",
      "**  Step 4205, val loss = 0.31, val accuracy = 92.19%  **\n",
      "Step: 4210, loss: 0.1067, accuracy: 96.8750%\n",
      "**  Step 4210, val loss = 0.53, val accuracy = 87.50%  **\n",
      "Step: 4215, loss: 0.1828, accuracy: 93.7500%\n",
      "**  Step 4215, val loss = 0.45, val accuracy = 84.38%  **\n",
      "Step: 4220, loss: 0.1959, accuracy: 95.3125%\n",
      "**  Step 4220, val loss = 1.02, val accuracy = 76.56%  **\n",
      "Step: 4225, loss: 0.1754, accuracy: 95.3125%\n",
      "**  Step 4225, val loss = 0.99, val accuracy = 82.81%  **\n",
      "Step: 4230, loss: 0.1040, accuracy: 100.0000%\n",
      "**  Step 4230, val loss = 0.71, val accuracy = 78.12%  **\n",
      "Step: 4235, loss: 0.2263, accuracy: 92.1875%\n",
      "**  Step 4235, val loss = 0.68, val accuracy = 79.69%  **\n",
      "Step: 4240, loss: 0.1672, accuracy: 96.8750%\n",
      "**  Step 4240, val loss = 0.86, val accuracy = 87.50%  **\n",
      "Step: 4245, loss: 0.1891, accuracy: 93.7500%\n",
      "**  Step 4245, val loss = 0.47, val accuracy = 84.38%  **\n",
      "Step: 4250, loss: 0.1324, accuracy: 96.8750%\n",
      "**  Step 4250, val loss = 0.63, val accuracy = 79.69%  **\n",
      "Step: 4255, loss: 0.1322, accuracy: 96.8750%\n",
      "**  Step 4255, val loss = 0.32, val accuracy = 89.06%  **\n",
      "Step: 4260, loss: 0.1864, accuracy: 93.7500%\n",
      "**  Step 4260, val loss = 1.42, val accuracy = 76.56%  **\n",
      "Step: 4265, loss: 0.1594, accuracy: 95.3125%\n",
      "**  Step 4265, val loss = 0.47, val accuracy = 89.06%  **\n",
      "Step: 4270, loss: 0.1912, accuracy: 90.6250%\n",
      "**  Step 4270, val loss = 0.43, val accuracy = 85.94%  **\n",
      "Step: 4275, loss: 0.2933, accuracy: 92.1875%\n",
      "**  Step 4275, val loss = 0.38, val accuracy = 81.25%  **\n",
      "Step: 4280, loss: 0.1828, accuracy: 92.1875%\n",
      "**  Step 4280, val loss = 0.76, val accuracy = 79.69%  **\n",
      "Step: 4285, loss: 0.0934, accuracy: 96.8750%\n",
      "**  Step 4285, val loss = 0.50, val accuracy = 90.62%  **\n",
      "Step: 4290, loss: 0.2137, accuracy: 89.0625%\n",
      "**  Step 4290, val loss = 0.49, val accuracy = 82.81%  **\n",
      "Step: 4295, loss: 0.2340, accuracy: 92.1875%\n",
      "**  Step 4295, val loss = 1.05, val accuracy = 82.81%  **\n",
      "Step: 4300, loss: 0.3555, accuracy: 92.1875%\n",
      "**  Step 4300, val loss = 0.32, val accuracy = 90.62%  **\n",
      "Step: 4305, loss: 0.2668, accuracy: 90.6250%\n",
      "**  Step 4305, val loss = 0.76, val accuracy = 71.88%  **\n",
      "Step: 4310, loss: 0.2011, accuracy: 90.6250%\n",
      "**  Step 4310, val loss = 0.56, val accuracy = 82.81%  **\n",
      "Step: 4315, loss: 0.0821, accuracy: 100.0000%\n",
      "**  Step 4315, val loss = 0.69, val accuracy = 82.81%  **\n",
      "Step: 4320, loss: 0.1593, accuracy: 96.8750%\n",
      "**  Step 4320, val loss = 0.32, val accuracy = 89.06%  **\n",
      "Step: 4325, loss: 0.3355, accuracy: 90.6250%\n",
      "**  Step 4325, val loss = 0.70, val accuracy = 81.25%  **\n",
      "Step: 4330, loss: 0.1975, accuracy: 95.3125%\n",
      "**  Step 4330, val loss = 0.60, val accuracy = 79.69%  **\n",
      "Step: 4335, loss: 0.3353, accuracy: 89.0625%\n",
      "**  Step 4335, val loss = 0.53, val accuracy = 84.38%  **\n",
      "Step: 4340, loss: 0.1701, accuracy: 95.3125%\n",
      "**  Step 4340, val loss = 0.70, val accuracy = 82.81%  **\n",
      "Step: 4345, loss: 0.1458, accuracy: 93.7500%\n",
      "**  Step 4345, val loss = 0.53, val accuracy = 79.69%  **\n",
      "Step: 4350, loss: 0.2090, accuracy: 93.7500%\n",
      "**  Step 4350, val loss = 0.42, val accuracy = 90.62%  **\n",
      "Step: 4355, loss: 0.1477, accuracy: 96.8750%\n",
      "**  Step 4355, val loss = 0.85, val accuracy = 73.44%  **\n",
      "Step: 4360, loss: 0.2547, accuracy: 92.1875%\n",
      "**  Step 4360, val loss = 0.64, val accuracy = 81.25%  **\n",
      "Step: 4365, loss: 0.2381, accuracy: 90.6250%\n",
      "**  Step 4365, val loss = 0.77, val accuracy = 81.25%  **\n",
      "Step: 4370, loss: 0.0819, accuracy: 96.8750%\n",
      "**  Step 4370, val loss = 0.44, val accuracy = 89.06%  **\n",
      "Step: 4375, loss: 0.1645, accuracy: 93.7500%\n",
      "**  Step 4375, val loss = 0.44, val accuracy = 87.50%  **\n",
      "Step: 4380, loss: 0.2073, accuracy: 90.6250%\n",
      "**  Step 4380, val loss = 0.34, val accuracy = 87.50%  **\n",
      "Step: 4385, loss: 0.1031, accuracy: 98.4375%\n",
      "**  Step 4385, val loss = 0.66, val accuracy = 85.94%  **\n",
      "Step: 4390, loss: 0.1306, accuracy: 98.4375%\n",
      "**  Step 4390, val loss = 0.66, val accuracy = 79.69%  **\n",
      "Step: 4395, loss: 0.1496, accuracy: 95.3125%\n",
      "**  Step 4395, val loss = 0.55, val accuracy = 84.38%  **\n",
      "Step: 4400, loss: 0.1459, accuracy: 93.7500%\n",
      "**  Step 4400, val loss = 0.55, val accuracy = 84.38%  **\n",
      "Step: 4405, loss: 0.2730, accuracy: 90.6250%\n",
      "**  Step 4405, val loss = 0.67, val accuracy = 84.38%  **\n",
      "Step: 4410, loss: 0.1767, accuracy: 92.1875%\n",
      "**  Step 4410, val loss = 0.35, val accuracy = 87.50%  **\n",
      "Step: 4415, loss: 0.2039, accuracy: 93.7500%\n",
      "**  Step 4415, val loss = 0.71, val accuracy = 75.00%  **\n",
      "Step: 4420, loss: 0.1188, accuracy: 95.3125%\n",
      "**  Step 4420, val loss = 0.50, val accuracy = 84.38%  **\n",
      "Step: 4425, loss: 0.1537, accuracy: 98.4375%\n",
      "**  Step 4425, val loss = 0.44, val accuracy = 87.50%  **\n",
      "Step: 4430, loss: 0.1631, accuracy: 96.8750%\n",
      "**  Step 4430, val loss = 0.32, val accuracy = 93.75%  **\n",
      "Step: 4435, loss: 0.1911, accuracy: 93.7500%\n",
      "**  Step 4435, val loss = 0.34, val accuracy = 90.62%  **\n",
      "Step: 4440, loss: 0.2579, accuracy: 95.3125%\n",
      "**  Step 4440, val loss = 0.27, val accuracy = 90.62%  **\n",
      "Step: 4445, loss: 0.1859, accuracy: 92.1875%\n",
      "**  Step 4445, val loss = 0.74, val accuracy = 79.69%  **\n",
      "Step: 4450, loss: 0.2885, accuracy: 90.6250%\n",
      "**  Step 4450, val loss = 0.44, val accuracy = 87.50%  **\n",
      "Step: 4455, loss: 0.1781, accuracy: 93.7500%\n",
      "**  Step 4455, val loss = 0.45, val accuracy = 87.50%  **\n",
      "Step: 4460, loss: 0.1426, accuracy: 96.8750%\n",
      "**  Step 4460, val loss = 0.72, val accuracy = 81.25%  **\n",
      "Step: 4465, loss: 0.1607, accuracy: 92.1875%\n",
      "**  Step 4465, val loss = 1.09, val accuracy = 78.12%  **\n",
      "Step: 4470, loss: 0.2083, accuracy: 96.8750%\n",
      "**  Step 4470, val loss = 0.51, val accuracy = 84.38%  **\n",
      "Step: 4475, loss: 0.1253, accuracy: 95.3125%\n",
      "**  Step 4475, val loss = 0.56, val accuracy = 79.69%  **\n",
      "Step: 4480, loss: 0.1472, accuracy: 95.3125%\n",
      "**  Step 4480, val loss = 0.40, val accuracy = 89.06%  **\n",
      "Step: 4485, loss: 0.1441, accuracy: 96.8750%\n",
      "**  Step 4485, val loss = 0.66, val accuracy = 89.06%  **\n",
      "Step: 4490, loss: 0.1558, accuracy: 95.3125%\n",
      "**  Step 4490, val loss = 0.56, val accuracy = 79.69%  **\n",
      "Step: 4495, loss: 0.1364, accuracy: 98.4375%\n",
      "**  Step 4495, val loss = 0.42, val accuracy = 87.50%  **\n",
      "Step: 4500, loss: 0.1633, accuracy: 95.3125%\n",
      "**  Step 4500, val loss = 0.76, val accuracy = 78.12%  **\n",
      "Step: 4505, loss: 0.2467, accuracy: 93.7500%\n",
      "**  Step 4505, val loss = 0.48, val accuracy = 85.94%  **\n",
      "Step: 4510, loss: 0.1318, accuracy: 95.3125%\n",
      "**  Step 4510, val loss = 0.55, val accuracy = 79.69%  **\n",
      "Step: 4515, loss: 0.1269, accuracy: 95.3125%\n",
      "**  Step 4515, val loss = 0.56, val accuracy = 87.50%  **\n",
      "Step: 4520, loss: 0.2974, accuracy: 90.6250%\n",
      "**  Step 4520, val loss = 0.53, val accuracy = 75.00%  **\n",
      "Step: 4525, loss: 0.2530, accuracy: 89.0625%\n",
      "**  Step 4525, val loss = 0.47, val accuracy = 87.50%  **\n",
      "Step: 4530, loss: 0.1797, accuracy: 93.7500%\n",
      "**  Step 4530, val loss = 1.07, val accuracy = 75.00%  **\n",
      "Step: 4535, loss: 0.2660, accuracy: 87.5000%\n",
      "**  Step 4535, val loss = 0.45, val accuracy = 85.94%  **\n",
      "Step: 4540, loss: 0.1013, accuracy: 96.8750%\n",
      "**  Step 4540, val loss = 0.50, val accuracy = 89.06%  **\n",
      "Step: 4545, loss: 0.2543, accuracy: 90.6250%\n",
      "**  Step 4545, val loss = 1.45, val accuracy = 82.81%  **\n",
      "Step: 4550, loss: 0.2139, accuracy: 87.5000%\n",
      "**  Step 4550, val loss = 0.45, val accuracy = 85.94%  **\n",
      "Step: 4555, loss: 0.1647, accuracy: 95.3125%\n",
      "**  Step 4555, val loss = 0.69, val accuracy = 79.69%  **\n",
      "Step: 4560, loss: 0.0881, accuracy: 98.4375%\n",
      "**  Step 4560, val loss = 0.21, val accuracy = 93.75%  **\n",
      "Step: 4565, loss: 0.2276, accuracy: 93.7500%\n",
      "**  Step 4565, val loss = 0.59, val accuracy = 84.38%  **\n",
      "Step: 4570, loss: 0.2723, accuracy: 90.6250%\n",
      "**  Step 4570, val loss = 0.40, val accuracy = 89.06%  **\n",
      "Step: 4575, loss: 0.1082, accuracy: 96.8750%\n",
      "**  Step 4575, val loss = 0.42, val accuracy = 90.62%  **\n",
      "Step: 4580, loss: 0.1250, accuracy: 96.8750%\n",
      "**  Step 4580, val loss = 0.95, val accuracy = 82.81%  **\n",
      "Step: 4585, loss: 0.1890, accuracy: 90.6250%\n",
      "**  Step 4585, val loss = 0.24, val accuracy = 89.06%  **\n",
      "Step: 4590, loss: 0.1856, accuracy: 95.3125%\n",
      "**  Step 4590, val loss = 0.43, val accuracy = 89.06%  **\n",
      "Step: 4595, loss: 0.3112, accuracy: 92.1875%\n",
      "**  Step 4595, val loss = 0.60, val accuracy = 89.06%  **\n",
      "Step: 4600, loss: 0.1021, accuracy: 96.8750%\n",
      "**  Step 4600, val loss = 0.39, val accuracy = 87.50%  **\n",
      "Step: 4605, loss: 0.0965, accuracy: 96.8750%\n",
      "**  Step 4605, val loss = 0.43, val accuracy = 84.38%  **\n",
      "Step: 4610, loss: 0.2053, accuracy: 93.7500%\n",
      "**  Step 4610, val loss = 0.68, val accuracy = 79.69%  **\n",
      "Step: 4615, loss: 0.2439, accuracy: 92.1875%\n",
      "**  Step 4615, val loss = 0.68, val accuracy = 84.38%  **\n",
      "Step: 4620, loss: 0.2240, accuracy: 90.6250%\n",
      "**  Step 4620, val loss = 0.76, val accuracy = 78.12%  **\n",
      "Step: 4625, loss: 0.1569, accuracy: 95.3125%\n",
      "**  Step 4625, val loss = 0.43, val accuracy = 87.50%  **\n",
      "Step: 4630, loss: 0.2461, accuracy: 87.5000%\n",
      "**  Step 4630, val loss = 0.45, val accuracy = 87.50%  **\n",
      "Step: 4635, loss: 0.2199, accuracy: 95.3125%\n",
      "**  Step 4635, val loss = 0.78, val accuracy = 85.94%  **\n",
      "Step: 4640, loss: 0.2833, accuracy: 87.5000%\n",
      "**  Step 4640, val loss = 0.35, val accuracy = 85.94%  **\n",
      "Step: 4645, loss: 0.1310, accuracy: 100.0000%\n",
      "**  Step 4645, val loss = 0.54, val accuracy = 75.00%  **\n",
      "Step: 4650, loss: 0.2498, accuracy: 87.5000%\n",
      "**  Step 4650, val loss = 0.77, val accuracy = 84.38%  **\n",
      "Step: 4655, loss: 0.1473, accuracy: 96.8750%\n",
      "**  Step 4655, val loss = 0.61, val accuracy = 82.81%  **\n",
      "Step: 4660, loss: 0.1483, accuracy: 96.8750%\n",
      "**  Step 4660, val loss = 0.26, val accuracy = 90.62%  **\n",
      "Step: 4665, loss: 0.1167, accuracy: 96.8750%\n",
      "**  Step 4665, val loss = 0.53, val accuracy = 79.69%  **\n",
      "Step: 4670, loss: 0.2626, accuracy: 93.7500%\n",
      "**  Step 4670, val loss = 0.52, val accuracy = 85.94%  **\n",
      "Step: 4675, loss: 0.1980, accuracy: 93.7500%\n",
      "**  Step 4675, val loss = 1.05, val accuracy = 84.38%  **\n",
      "Step: 4680, loss: 0.1608, accuracy: 95.3125%\n",
      "**  Step 4680, val loss = 0.27, val accuracy = 90.62%  **\n",
      "Step: 4685, loss: 0.2363, accuracy: 92.1875%\n",
      "**  Step 4685, val loss = 0.32, val accuracy = 87.50%  **\n",
      "Step: 4690, loss: 0.1017, accuracy: 96.8750%\n",
      "**  Step 4690, val loss = 0.62, val accuracy = 84.38%  **\n",
      "Step: 4695, loss: 0.1155, accuracy: 96.8750%\n",
      "**  Step 4695, val loss = 0.66, val accuracy = 82.81%  **\n",
      "Step: 4700, loss: 0.2576, accuracy: 95.3125%\n",
      "**  Step 4700, val loss = 0.44, val accuracy = 82.81%  **\n",
      "Step: 4705, loss: 0.1481, accuracy: 93.7500%\n",
      "**  Step 4705, val loss = 0.81, val accuracy = 73.44%  **\n",
      "Step: 4710, loss: 0.0982, accuracy: 100.0000%\n",
      "**  Step 4710, val loss = 0.56, val accuracy = 84.38%  **\n",
      "Step: 4715, loss: 0.1373, accuracy: 96.8750%\n",
      "**  Step 4715, val loss = 0.71, val accuracy = 81.25%  **\n",
      "Step: 4720, loss: 0.0990, accuracy: 96.8750%\n",
      "**  Step 4720, val loss = 0.78, val accuracy = 84.38%  **\n",
      "Step: 4725, loss: 0.3212, accuracy: 96.8750%\n",
      "**  Step 4725, val loss = 0.50, val accuracy = 81.25%  **\n",
      "Step: 4730, loss: 0.2221, accuracy: 98.4375%\n",
      "**  Step 4730, val loss = 0.63, val accuracy = 81.25%  **\n",
      "Step: 4735, loss: 0.1678, accuracy: 96.8750%\n",
      "**  Step 4735, val loss = 0.56, val accuracy = 81.25%  **\n",
      "Step: 4740, loss: 0.1860, accuracy: 93.7500%\n",
      "**  Step 4740, val loss = 0.72, val accuracy = 85.94%  **\n",
      "Step: 4745, loss: 0.1595, accuracy: 96.8750%\n",
      "**  Step 4745, val loss = 0.65, val accuracy = 79.69%  **\n",
      "Step: 4750, loss: 0.1888, accuracy: 95.3125%\n",
      "**  Step 4750, val loss = 0.62, val accuracy = 79.69%  **\n",
      "Step: 4755, loss: 0.1670, accuracy: 92.1875%\n",
      "**  Step 4755, val loss = 0.79, val accuracy = 79.69%  **\n",
      "Step: 4760, loss: 0.1463, accuracy: 95.3125%\n",
      "**  Step 4760, val loss = 0.45, val accuracy = 82.81%  **\n",
      "Step: 4765, loss: 0.1827, accuracy: 92.1875%\n",
      "**  Step 4765, val loss = 0.82, val accuracy = 84.38%  **\n",
      "Step: 4770, loss: 0.1092, accuracy: 98.4375%\n",
      "**  Step 4770, val loss = 0.15, val accuracy = 95.31%  **\n",
      "Step: 4775, loss: 0.1468, accuracy: 95.3125%\n",
      "**  Step 4775, val loss = 0.44, val accuracy = 79.69%  **\n",
      "Step: 4780, loss: 0.1228, accuracy: 95.3125%\n",
      "**  Step 4780, val loss = 0.47, val accuracy = 82.81%  **\n",
      "Step: 4785, loss: 0.1621, accuracy: 95.3125%\n",
      "**  Step 4785, val loss = 0.43, val accuracy = 87.50%  **\n",
      "Step: 4790, loss: 0.2365, accuracy: 92.1875%\n",
      "**  Step 4790, val loss = 0.63, val accuracy = 81.25%  **\n",
      "Step: 4795, loss: 0.1089, accuracy: 98.4375%\n",
      "**  Step 4795, val loss = 1.27, val accuracy = 78.12%  **\n",
      "Step: 4800, loss: 0.1411, accuracy: 96.8750%\n",
      "**  Step 4800, val loss = 0.43, val accuracy = 84.38%  **\n",
      "Step: 4805, loss: 0.2746, accuracy: 90.6250%\n",
      "**  Step 4805, val loss = 0.58, val accuracy = 85.94%  **\n",
      "Step: 4810, loss: 0.1142, accuracy: 96.8750%\n",
      "**  Step 4810, val loss = 1.17, val accuracy = 78.12%  **\n",
      "Step: 4815, loss: 0.1265, accuracy: 98.4375%\n",
      "**  Step 4815, val loss = 0.39, val accuracy = 90.62%  **\n",
      "Step: 4820, loss: 0.1600, accuracy: 96.8750%\n",
      "**  Step 4820, val loss = 0.35, val accuracy = 92.19%  **\n",
      "Step: 4825, loss: 0.1418, accuracy: 96.8750%\n",
      "**  Step 4825, val loss = 0.40, val accuracy = 87.50%  **\n",
      "Step: 4830, loss: 0.1598, accuracy: 95.3125%\n",
      "**  Step 4830, val loss = 0.70, val accuracy = 85.94%  **\n",
      "Step: 4835, loss: 0.1876, accuracy: 95.3125%\n",
      "**  Step 4835, val loss = 0.36, val accuracy = 90.62%  **\n",
      "Step: 4840, loss: 0.2657, accuracy: 90.6250%\n",
      "**  Step 4840, val loss = 1.01, val accuracy = 73.44%  **\n",
      "Step: 4845, loss: 0.1756, accuracy: 93.7500%\n",
      "**  Step 4845, val loss = 0.61, val accuracy = 78.12%  **\n",
      "Step: 4850, loss: 0.1341, accuracy: 96.8750%\n",
      "**  Step 4850, val loss = 0.43, val accuracy = 85.94%  **\n",
      "Step: 4855, loss: 0.1525, accuracy: 93.7500%\n",
      "**  Step 4855, val loss = 0.35, val accuracy = 90.62%  **\n",
      "Step: 4860, loss: 0.1699, accuracy: 93.7500%\n",
      "**  Step 4860, val loss = 0.62, val accuracy = 85.94%  **\n",
      "Step: 4865, loss: 0.1562, accuracy: 93.7500%\n",
      "**  Step 4865, val loss = 0.83, val accuracy = 76.56%  **\n",
      "Step: 4870, loss: 0.1359, accuracy: 96.8750%\n",
      "**  Step 4870, val loss = 0.42, val accuracy = 89.06%  **\n",
      "Step: 4875, loss: 0.1203, accuracy: 96.8750%\n",
      "**  Step 4875, val loss = 0.47, val accuracy = 85.94%  **\n",
      "Step: 4880, loss: 0.1556, accuracy: 95.3125%\n",
      "**  Step 4880, val loss = 0.82, val accuracy = 81.25%  **\n",
      "Step: 4885, loss: 0.1683, accuracy: 95.3125%\n",
      "**  Step 4885, val loss = 0.51, val accuracy = 84.38%  **\n",
      "Step: 4890, loss: 0.1464, accuracy: 95.3125%\n",
      "**  Step 4890, val loss = 0.34, val accuracy = 89.06%  **\n",
      "Step: 4895, loss: 0.1249, accuracy: 98.4375%\n",
      "**  Step 4895, val loss = 0.55, val accuracy = 82.81%  **\n",
      "Step: 4900, loss: 0.1088, accuracy: 95.3125%\n",
      "**  Step 4900, val loss = 0.94, val accuracy = 81.25%  **\n",
      "Step: 4905, loss: 0.1360, accuracy: 92.1875%\n",
      "**  Step 4905, val loss = 0.51, val accuracy = 85.94%  **\n",
      "Step: 4910, loss: 0.2294, accuracy: 90.6250%\n",
      "**  Step 4910, val loss = 0.60, val accuracy = 81.25%  **\n",
      "Step: 4915, loss: 0.1824, accuracy: 93.7500%\n",
      "**  Step 4915, val loss = 0.36, val accuracy = 90.62%  **\n",
      "Step: 4920, loss: 0.0829, accuracy: 98.4375%\n",
      "**  Step 4920, val loss = 0.50, val accuracy = 84.38%  **\n",
      "Step: 4925, loss: 0.2399, accuracy: 93.7500%\n",
      "**  Step 4925, val loss = 0.69, val accuracy = 84.38%  **\n",
      "Step: 4930, loss: 0.0935, accuracy: 98.4375%\n",
      "**  Step 4930, val loss = 0.58, val accuracy = 84.38%  **\n",
      "Step: 4935, loss: 0.1298, accuracy: 96.8750%\n",
      "**  Step 4935, val loss = 0.36, val accuracy = 89.06%  **\n",
      "Step: 4940, loss: 0.2077, accuracy: 90.6250%\n",
      "**  Step 4940, val loss = 0.34, val accuracy = 93.75%  **\n",
      "Step: 4945, loss: 0.1117, accuracy: 98.4375%\n",
      "**  Step 4945, val loss = 0.52, val accuracy = 85.94%  **\n",
      "Step: 4950, loss: 0.1554, accuracy: 96.8750%\n",
      "**  Step 4950, val loss = 0.73, val accuracy = 82.81%  **\n",
      "Step: 4955, loss: 0.1714, accuracy: 95.3125%\n",
      "**  Step 4955, val loss = 0.84, val accuracy = 84.38%  **\n",
      "Step: 4960, loss: 0.1869, accuracy: 92.1875%\n",
      "**  Step 4960, val loss = 0.71, val accuracy = 84.38%  **\n",
      "Step: 4965, loss: 0.1141, accuracy: 100.0000%\n",
      "**  Step 4965, val loss = 0.29, val accuracy = 87.50%  **\n",
      "Step: 4970, loss: 0.1495, accuracy: 95.3125%\n",
      "**  Step 4970, val loss = 0.20, val accuracy = 93.75%  **\n",
      "Step: 4975, loss: 0.1881, accuracy: 92.1875%\n",
      "**  Step 4975, val loss = 0.78, val accuracy = 84.38%  **\n",
      "Step: 4980, loss: 0.1848, accuracy: 92.1875%\n",
      "**  Step 4980, val loss = 0.71, val accuracy = 78.12%  **\n",
      "Step: 4985, loss: 0.1425, accuracy: 93.7500%\n",
      "**  Step 4985, val loss = 0.55, val accuracy = 84.38%  **\n",
      "Step: 4990, loss: 0.1524, accuracy: 93.7500%\n",
      "**  Step 4990, val loss = 1.18, val accuracy = 73.44%  **\n",
      "Step: 4995, loss: 0.2106, accuracy: 96.8750%\n",
      "**  Step 4995, val loss = 0.55, val accuracy = 87.50%  **\n",
      "Step: 5000, loss: 0.1580, accuracy: 93.7500%\n",
      "**  Step 5000, val loss = 0.54, val accuracy = 85.94%  **\n",
      "Step: 5005, loss: 0.2479, accuracy: 92.1875%\n",
      "**  Step 5005, val loss = 0.58, val accuracy = 79.69%  **\n",
      "Step: 5010, loss: 0.1256, accuracy: 96.8750%\n",
      "**  Step 5010, val loss = 0.51, val accuracy = 81.25%  **\n",
      "Step: 5015, loss: 0.3496, accuracy: 92.1875%\n",
      "**  Step 5015, val loss = 0.34, val accuracy = 93.75%  **\n",
      "Step: 5020, loss: 0.1717, accuracy: 93.7500%\n",
      "**  Step 5020, val loss = 0.61, val accuracy = 89.06%  **\n",
      "Step: 5025, loss: 0.1486, accuracy: 95.3125%\n",
      "**  Step 5025, val loss = 0.66, val accuracy = 84.38%  **\n",
      "Step: 5030, loss: 0.2612, accuracy: 89.0625%\n",
      "**  Step 5030, val loss = 0.70, val accuracy = 79.69%  **\n",
      "Step: 5035, loss: 0.1655, accuracy: 95.3125%\n",
      "**  Step 5035, val loss = 0.48, val accuracy = 79.69%  **\n",
      "Step: 5040, loss: 0.1309, accuracy: 95.3125%\n",
      "**  Step 5040, val loss = 0.70, val accuracy = 81.25%  **\n",
      "Step: 5045, loss: 0.1601, accuracy: 95.3125%\n",
      "**  Step 5045, val loss = 0.59, val accuracy = 84.38%  **\n",
      "Step: 5050, loss: 0.1011, accuracy: 98.4375%\n",
      "**  Step 5050, val loss = 0.82, val accuracy = 90.62%  **\n",
      "Step: 5055, loss: 0.1853, accuracy: 95.3125%\n",
      "**  Step 5055, val loss = 0.38, val accuracy = 89.06%  **\n",
      "Step: 5060, loss: 0.1054, accuracy: 98.4375%\n",
      "**  Step 5060, val loss = 0.57, val accuracy = 81.25%  **\n",
      "Step: 5065, loss: 0.2004, accuracy: 93.7500%\n",
      "**  Step 5065, val loss = 0.38, val accuracy = 90.62%  **\n",
      "Step: 5070, loss: 0.1217, accuracy: 93.7500%\n",
      "**  Step 5070, val loss = 0.70, val accuracy = 78.12%  **\n",
      "Step: 5075, loss: 0.1941, accuracy: 93.7500%\n",
      "**  Step 5075, val loss = 0.89, val accuracy = 78.12%  **\n",
      "Step: 5080, loss: 0.0892, accuracy: 98.4375%\n",
      "**  Step 5080, val loss = 0.85, val accuracy = 79.69%  **\n",
      "Step: 5085, loss: 0.1614, accuracy: 95.3125%\n",
      "**  Step 5085, val loss = 0.56, val accuracy = 85.94%  **\n",
      "Step: 5090, loss: 0.1136, accuracy: 95.3125%\n",
      "**  Step 5090, val loss = 0.45, val accuracy = 85.94%  **\n",
      "Step: 5095, loss: 0.1351, accuracy: 96.8750%\n",
      "**  Step 5095, val loss = 1.06, val accuracy = 75.00%  **\n",
      "Step: 5100, loss: 0.2036, accuracy: 92.1875%\n",
      "**  Step 5100, val loss = 0.82, val accuracy = 81.25%  **\n",
      "Step: 5105, loss: 0.1317, accuracy: 95.3125%\n",
      "**  Step 5105, val loss = 0.59, val accuracy = 84.38%  **\n",
      "Step: 5110, loss: 0.2765, accuracy: 93.7500%\n",
      "**  Step 5110, val loss = 0.65, val accuracy = 82.81%  **\n",
      "Step: 5115, loss: 0.2158, accuracy: 92.1875%\n",
      "**  Step 5115, val loss = 0.53, val accuracy = 90.62%  **\n",
      "Step: 5120, loss: 0.1204, accuracy: 95.3125%\n",
      "**  Step 5120, val loss = 0.45, val accuracy = 85.94%  **\n",
      "Step: 5125, loss: 0.1320, accuracy: 98.4375%\n",
      "**  Step 5125, val loss = 0.80, val accuracy = 81.25%  **\n",
      "Step: 5130, loss: 0.1511, accuracy: 93.7500%\n",
      "**  Step 5130, val loss = 0.67, val accuracy = 81.25%  **\n",
      "Step: 5135, loss: 0.1484, accuracy: 93.7500%\n",
      "**  Step 5135, val loss = 0.99, val accuracy = 73.44%  **\n",
      "Step: 5140, loss: 0.1781, accuracy: 95.3125%\n",
      "**  Step 5140, val loss = 0.50, val accuracy = 85.94%  **\n",
      "Step: 5145, loss: 0.1014, accuracy: 95.3125%\n",
      "**  Step 5145, val loss = 0.49, val accuracy = 82.81%  **\n",
      "Step: 5150, loss: 0.1549, accuracy: 95.3125%\n",
      "**  Step 5150, val loss = 0.83, val accuracy = 79.69%  **\n",
      "Step: 5155, loss: 0.1667, accuracy: 95.3125%\n",
      "**  Step 5155, val loss = 0.49, val accuracy = 82.81%  **\n",
      "Step: 5160, loss: 0.1840, accuracy: 92.1875%\n",
      "**  Step 5160, val loss = 0.75, val accuracy = 79.69%  **\n",
      "Step: 5165, loss: 0.0791, accuracy: 98.4375%\n",
      "**  Step 5165, val loss = 0.34, val accuracy = 87.50%  **\n",
      "Step: 5170, loss: 0.1081, accuracy: 96.8750%\n",
      "**  Step 5170, val loss = 0.64, val accuracy = 82.81%  **\n",
      "Step: 5175, loss: 0.1329, accuracy: 98.4375%\n",
      "**  Step 5175, val loss = 0.55, val accuracy = 87.50%  **\n",
      "Step: 5180, loss: 0.1854, accuracy: 92.1875%\n",
      "**  Step 5180, val loss = 0.51, val accuracy = 87.50%  **\n",
      "Step: 5185, loss: 0.2515, accuracy: 92.1875%\n",
      "**  Step 5185, val loss = 0.68, val accuracy = 85.94%  **\n",
      "Step: 5190, loss: 0.1465, accuracy: 95.3125%\n",
      "**  Step 5190, val loss = 0.36, val accuracy = 85.94%  **\n",
      "Step: 5195, loss: 0.1476, accuracy: 95.3125%\n",
      "**  Step 5195, val loss = 0.85, val accuracy = 79.69%  **\n",
      "Step: 5200, loss: 0.1471, accuracy: 96.8750%\n",
      "**  Step 5200, val loss = 0.64, val accuracy = 82.81%  **\n",
      "Step: 5205, loss: 0.2324, accuracy: 90.6250%\n",
      "**  Step 5205, val loss = 0.59, val accuracy = 85.94%  **\n",
      "Step: 5210, loss: 0.1492, accuracy: 95.3125%\n",
      "**  Step 5210, val loss = 0.34, val accuracy = 89.06%  **\n",
      "Step: 5215, loss: 0.2298, accuracy: 90.6250%\n",
      "**  Step 5215, val loss = 0.68, val accuracy = 87.50%  **\n",
      "Step: 5220, loss: 0.1605, accuracy: 95.3125%\n",
      "**  Step 5220, val loss = 0.51, val accuracy = 82.81%  **\n",
      "Step: 5225, loss: 0.0968, accuracy: 98.4375%\n",
      "**  Step 5225, val loss = 0.99, val accuracy = 78.12%  **\n",
      "Step: 5230, loss: 0.0791, accuracy: 100.0000%\n",
      "**  Step 5230, val loss = 0.88, val accuracy = 76.56%  **\n",
      "Step: 5235, loss: 0.1971, accuracy: 93.7500%\n",
      "**  Step 5235, val loss = 0.66, val accuracy = 81.25%  **\n",
      "Step: 5240, loss: 0.1579, accuracy: 96.8750%\n",
      "**  Step 5240, val loss = 0.54, val accuracy = 82.81%  **\n",
      "Step: 5245, loss: 0.1779, accuracy: 95.3125%\n",
      "**  Step 5245, val loss = 0.39, val accuracy = 90.62%  **\n",
      "Step: 5250, loss: 0.1431, accuracy: 98.4375%\n",
      "**  Step 5250, val loss = 0.45, val accuracy = 89.06%  **\n",
      "Step: 5255, loss: 0.1858, accuracy: 92.1875%\n",
      "**  Step 5255, val loss = 0.41, val accuracy = 89.06%  **\n",
      "Step: 5260, loss: 0.1621, accuracy: 95.3125%\n",
      "**  Step 5260, val loss = 0.32, val accuracy = 89.06%  **\n",
      "Step: 5265, loss: 0.1199, accuracy: 96.8750%\n",
      "**  Step 5265, val loss = 0.35, val accuracy = 92.19%  **\n",
      "Step: 5270, loss: 0.0996, accuracy: 100.0000%\n",
      "**  Step 5270, val loss = 0.27, val accuracy = 90.62%  **\n",
      "Step: 5275, loss: 0.1468, accuracy: 93.7500%\n",
      "**  Step 5275, val loss = 0.41, val accuracy = 85.94%  **\n",
      "Step: 5280, loss: 0.1174, accuracy: 95.3125%\n",
      "**  Step 5280, val loss = 0.51, val accuracy = 84.38%  **\n",
      "Step: 5285, loss: 0.1449, accuracy: 98.4375%\n",
      "**  Step 5285, val loss = 0.31, val accuracy = 87.50%  **\n",
      "Step: 5290, loss: 0.0898, accuracy: 96.8750%\n",
      "**  Step 5290, val loss = 0.67, val accuracy = 76.56%  **\n",
      "Step: 5295, loss: 0.1282, accuracy: 95.3125%\n",
      "**  Step 5295, val loss = 0.24, val accuracy = 90.62%  **\n",
      "Step: 5300, loss: 0.1632, accuracy: 92.1875%\n",
      "**  Step 5300, val loss = 0.61, val accuracy = 84.38%  **\n",
      "Step: 5305, loss: 0.1725, accuracy: 93.7500%\n",
      "**  Step 5305, val loss = 0.46, val accuracy = 87.50%  **\n",
      "Step: 5310, loss: 0.1206, accuracy: 96.8750%\n",
      "**  Step 5310, val loss = 0.59, val accuracy = 89.06%  **\n",
      "Step: 5315, loss: 0.1880, accuracy: 93.7500%\n",
      "**  Step 5315, val loss = 0.75, val accuracy = 78.12%  **\n",
      "Step: 5320, loss: 0.1030, accuracy: 98.4375%\n",
      "**  Step 5320, val loss = 0.41, val accuracy = 87.50%  **\n",
      "Step: 5325, loss: 0.0958, accuracy: 96.8750%\n",
      "**  Step 5325, val loss = 0.51, val accuracy = 84.38%  **\n",
      "Step: 5330, loss: 0.1858, accuracy: 93.7500%\n",
      "**  Step 5330, val loss = 0.26, val accuracy = 90.62%  **\n",
      "Step: 5335, loss: 0.1499, accuracy: 96.8750%\n",
      "**  Step 5335, val loss = 0.56, val accuracy = 84.38%  **\n",
      "Step: 5340, loss: 0.0938, accuracy: 96.8750%\n",
      "**  Step 5340, val loss = 0.41, val accuracy = 82.81%  **\n",
      "Step: 5345, loss: 0.1404, accuracy: 93.7500%\n",
      "**  Step 5345, val loss = 0.64, val accuracy = 81.25%  **\n",
      "Step: 5350, loss: 0.1570, accuracy: 93.7500%\n",
      "**  Step 5350, val loss = 0.41, val accuracy = 82.81%  **\n",
      "Step: 5355, loss: 0.1191, accuracy: 96.8750%\n",
      "**  Step 5355, val loss = 1.05, val accuracy = 78.12%  **\n",
      "Step: 5360, loss: 0.0922, accuracy: 96.8750%\n",
      "**  Step 5360, val loss = 0.56, val accuracy = 87.50%  **\n",
      "Step: 5365, loss: 0.1524, accuracy: 95.3125%\n",
      "**  Step 5365, val loss = 1.30, val accuracy = 71.88%  **\n",
      "Step: 5370, loss: 0.1369, accuracy: 96.8750%\n",
      "**  Step 5370, val loss = 0.71, val accuracy = 81.25%  **\n",
      "Step: 5375, loss: 0.0783, accuracy: 98.4375%\n",
      "**  Step 5375, val loss = 0.99, val accuracy = 81.25%  **\n",
      "Step: 5380, loss: 0.1678, accuracy: 95.3125%\n",
      "**  Step 5380, val loss = 0.48, val accuracy = 82.81%  **\n",
      "Step: 5385, loss: 0.0979, accuracy: 96.8750%\n",
      "**  Step 5385, val loss = 0.42, val accuracy = 87.50%  **\n",
      "Step: 5390, loss: 0.1421, accuracy: 95.3125%\n",
      "**  Step 5390, val loss = 0.87, val accuracy = 76.56%  **\n",
      "Step: 5395, loss: 0.0838, accuracy: 98.4375%\n",
      "**  Step 5395, val loss = 0.60, val accuracy = 82.81%  **\n",
      "Step: 5400, loss: 0.1465, accuracy: 95.3125%\n",
      "**  Step 5400, val loss = 0.74, val accuracy = 82.81%  **\n",
      "Step: 5405, loss: 0.0643, accuracy: 100.0000%\n",
      "**  Step 5405, val loss = 0.88, val accuracy = 75.00%  **\n",
      "Step: 5410, loss: 0.1311, accuracy: 96.8750%\n",
      "**  Step 5410, val loss = 0.50, val accuracy = 87.50%  **\n",
      "Step: 5415, loss: 0.2202, accuracy: 92.1875%\n",
      "**  Step 5415, val loss = 0.57, val accuracy = 81.25%  **\n",
      "Step: 5420, loss: 0.1264, accuracy: 96.8750%\n",
      "**  Step 5420, val loss = 0.39, val accuracy = 85.94%  **\n",
      "Step: 5425, loss: 0.1745, accuracy: 93.7500%\n",
      "**  Step 5425, val loss = 0.74, val accuracy = 85.94%  **\n",
      "Step: 5430, loss: 0.1231, accuracy: 93.7500%\n",
      "**  Step 5430, val loss = 0.87, val accuracy = 82.81%  **\n",
      "Step: 5435, loss: 0.1313, accuracy: 96.8750%\n",
      "**  Step 5435, val loss = 0.86, val accuracy = 76.56%  **\n",
      "Step: 5440, loss: 0.1712, accuracy: 96.8750%\n",
      "**  Step 5440, val loss = 0.40, val accuracy = 92.19%  **\n",
      "Step: 5445, loss: 0.1031, accuracy: 98.4375%\n",
      "**  Step 5445, val loss = 1.13, val accuracy = 81.25%  **\n",
      "Step: 5450, loss: 0.2070, accuracy: 90.6250%\n",
      "**  Step 5450, val loss = 0.56, val accuracy = 79.69%  **\n",
      "Step: 5455, loss: 0.1292, accuracy: 96.8750%\n",
      "**  Step 5455, val loss = 0.61, val accuracy = 82.81%  **\n",
      "Step: 5460, loss: 0.2041, accuracy: 95.3125%\n",
      "**  Step 5460, val loss = 0.74, val accuracy = 79.69%  **\n",
      "Step: 5465, loss: 0.0948, accuracy: 96.8750%\n",
      "**  Step 5465, val loss = 0.45, val accuracy = 85.94%  **\n",
      "Step: 5470, loss: 0.1221, accuracy: 96.8750%\n",
      "**  Step 5470, val loss = 0.63, val accuracy = 82.81%  **\n",
      "Step: 5475, loss: 0.2479, accuracy: 95.3125%\n",
      "**  Step 5475, val loss = 0.51, val accuracy = 89.06%  **\n",
      "Step: 5480, loss: 0.1108, accuracy: 98.4375%\n",
      "**  Step 5480, val loss = 0.85, val accuracy = 81.25%  **\n",
      "Step: 5485, loss: 0.1326, accuracy: 96.8750%\n",
      "**  Step 5485, val loss = 0.59, val accuracy = 82.81%  **\n",
      "Step: 5490, loss: 0.2459, accuracy: 90.6250%\n",
      "**  Step 5490, val loss = 0.64, val accuracy = 81.25%  **\n",
      "Step: 5495, loss: 0.0590, accuracy: 100.0000%\n",
      "**  Step 5495, val loss = 0.44, val accuracy = 87.50%  **\n",
      "Step: 5500, loss: 0.1739, accuracy: 96.8750%\n",
      "**  Step 5500, val loss = 0.73, val accuracy = 85.94%  **\n",
      "Step: 5505, loss: 0.1675, accuracy: 95.3125%\n",
      "**  Step 5505, val loss = 0.88, val accuracy = 87.50%  **\n",
      "Step: 5510, loss: 0.2131, accuracy: 95.3125%\n",
      "**  Step 5510, val loss = 0.67, val accuracy = 84.38%  **\n",
      "Step: 5515, loss: 0.1542, accuracy: 95.3125%\n",
      "**  Step 5515, val loss = 0.59, val accuracy = 76.56%  **\n",
      "Step: 5520, loss: 0.1590, accuracy: 96.8750%\n",
      "**  Step 5520, val loss = 0.45, val accuracy = 92.19%  **\n",
      "Step: 5525, loss: 0.0828, accuracy: 100.0000%\n",
      "**  Step 5525, val loss = 0.51, val accuracy = 84.38%  **\n",
      "Step: 5530, loss: 0.1137, accuracy: 96.8750%\n",
      "**  Step 5530, val loss = 0.64, val accuracy = 82.81%  **\n",
      "Step: 5535, loss: 0.1319, accuracy: 96.8750%\n",
      "**  Step 5535, val loss = 0.94, val accuracy = 75.00%  **\n",
      "Step: 5540, loss: 0.1050, accuracy: 98.4375%\n",
      "**  Step 5540, val loss = 0.68, val accuracy = 82.81%  **\n",
      "Step: 5545, loss: 0.1030, accuracy: 95.3125%\n",
      "**  Step 5545, val loss = 0.75, val accuracy = 79.69%  **\n",
      "Step: 5550, loss: 0.1274, accuracy: 93.7500%\n",
      "**  Step 5550, val loss = 0.51, val accuracy = 87.50%  **\n",
      "Step: 5555, loss: 0.2007, accuracy: 93.7500%\n",
      "**  Step 5555, val loss = 0.22, val accuracy = 89.06%  **\n",
      "Step: 5560, loss: 0.1173, accuracy: 95.3125%\n",
      "**  Step 5560, val loss = 0.73, val accuracy = 82.81%  **\n",
      "Step: 5565, loss: 0.0896, accuracy: 100.0000%\n",
      "**  Step 5565, val loss = 0.58, val accuracy = 87.50%  **\n",
      "Step: 5570, loss: 0.0846, accuracy: 98.4375%\n",
      "**  Step 5570, val loss = 0.27, val accuracy = 90.62%  **\n",
      "Step: 5575, loss: 0.0830, accuracy: 96.8750%\n",
      "**  Step 5575, val loss = 0.48, val accuracy = 81.25%  **\n",
      "Step: 5580, loss: 0.1097, accuracy: 98.4375%\n",
      "**  Step 5580, val loss = 0.78, val accuracy = 79.69%  **\n",
      "Step: 5585, loss: 0.1582, accuracy: 93.7500%\n",
      "**  Step 5585, val loss = 0.50, val accuracy = 85.94%  **\n",
      "Step: 5590, loss: 0.1028, accuracy: 96.8750%\n",
      "**  Step 5590, val loss = 0.55, val accuracy = 85.94%  **\n",
      "Step: 5595, loss: 0.1754, accuracy: 95.3125%\n",
      "**  Step 5595, val loss = 0.74, val accuracy = 84.38%  **\n",
      "Step: 5600, loss: 0.0709, accuracy: 98.4375%\n",
      "**  Step 5600, val loss = 0.59, val accuracy = 81.25%  **\n",
      "Step: 5605, loss: 0.2421, accuracy: 89.0625%\n",
      "**  Step 5605, val loss = 0.59, val accuracy = 85.94%  **\n",
      "Step: 5610, loss: 0.1305, accuracy: 93.7500%\n",
      "**  Step 5610, val loss = 0.53, val accuracy = 84.38%  **\n",
      "Step: 5615, loss: 0.1315, accuracy: 96.8750%\n",
      "**  Step 5615, val loss = 0.80, val accuracy = 85.94%  **\n",
      "Step: 5620, loss: 0.0806, accuracy: 98.4375%\n",
      "**  Step 5620, val loss = 0.61, val accuracy = 82.81%  **\n",
      "Step: 5625, loss: 0.1916, accuracy: 93.7500%\n",
      "**  Step 5625, val loss = 0.71, val accuracy = 79.69%  **\n",
      "Step: 5630, loss: 0.1599, accuracy: 96.8750%\n",
      "**  Step 5630, val loss = 0.47, val accuracy = 87.50%  **\n",
      "Step: 5635, loss: 0.1792, accuracy: 90.6250%\n",
      "**  Step 5635, val loss = 0.40, val accuracy = 84.38%  **\n",
      "Step: 5640, loss: 0.1802, accuracy: 93.7500%\n",
      "**  Step 5640, val loss = 0.65, val accuracy = 89.06%  **\n",
      "Step: 5645, loss: 0.0955, accuracy: 96.8750%\n",
      "**  Step 5645, val loss = 0.64, val accuracy = 84.38%  **\n",
      "Step: 5650, loss: 0.0791, accuracy: 98.4375%\n",
      "**  Step 5650, val loss = 0.60, val accuracy = 85.94%  **\n",
      "Step: 5655, loss: 0.1491, accuracy: 93.7500%\n",
      "**  Step 5655, val loss = 0.88, val accuracy = 76.56%  **\n",
      "Step: 5660, loss: 0.1149, accuracy: 95.3125%\n",
      "**  Step 5660, val loss = 0.41, val accuracy = 85.94%  **\n",
      "Step: 5665, loss: 0.1905, accuracy: 93.7500%\n",
      "**  Step 5665, val loss = 0.62, val accuracy = 78.12%  **\n",
      "Step: 5670, loss: 0.1229, accuracy: 95.3125%\n",
      "**  Step 5670, val loss = 0.57, val accuracy = 82.81%  **\n",
      "Step: 5675, loss: 0.0840, accuracy: 95.3125%\n",
      "**  Step 5675, val loss = 0.51, val accuracy = 85.94%  **\n",
      "Step: 5680, loss: 0.1371, accuracy: 95.3125%\n",
      "**  Step 5680, val loss = 0.47, val accuracy = 81.25%  **\n",
      "Step: 5685, loss: 0.0567, accuracy: 98.4375%\n",
      "**  Step 5685, val loss = 0.30, val accuracy = 90.62%  **\n",
      "Step: 5690, loss: 0.1818, accuracy: 95.3125%\n",
      "**  Step 5690, val loss = 1.15, val accuracy = 78.12%  **\n",
      "Step: 5695, loss: 0.1494, accuracy: 95.3125%\n",
      "**  Step 5695, val loss = 0.41, val accuracy = 85.94%  **\n",
      "Step: 5700, loss: 0.1987, accuracy: 93.7500%\n",
      "**  Step 5700, val loss = 0.62, val accuracy = 84.38%  **\n",
      "Step: 5705, loss: 0.1403, accuracy: 93.7500%\n",
      "**  Step 5705, val loss = 0.54, val accuracy = 84.38%  **\n",
      "Step: 5710, loss: 0.1370, accuracy: 96.8750%\n",
      "**  Step 5710, val loss = 0.87, val accuracy = 79.69%  **\n",
      "Step: 5715, loss: 0.1621, accuracy: 96.8750%\n",
      "**  Step 5715, val loss = 0.92, val accuracy = 76.56%  **\n",
      "Step: 5720, loss: 0.1906, accuracy: 93.7500%\n",
      "**  Step 5720, val loss = 0.62, val accuracy = 82.81%  **\n",
      "Step: 5725, loss: 0.2143, accuracy: 93.7500%\n",
      "**  Step 5725, val loss = 0.57, val accuracy = 85.94%  **\n",
      "Step: 5730, loss: 0.1455, accuracy: 96.8750%\n",
      "**  Step 5730, val loss = 0.49, val accuracy = 87.50%  **\n",
      "Step: 5735, loss: 0.1421, accuracy: 96.8750%\n",
      "**  Step 5735, val loss = 0.68, val accuracy = 81.25%  **\n",
      "Step: 5740, loss: 0.1541, accuracy: 95.3125%\n",
      "**  Step 5740, val loss = 0.70, val accuracy = 82.81%  **\n",
      "Step: 5745, loss: 0.0832, accuracy: 98.4375%\n",
      "**  Step 5745, val loss = 0.77, val accuracy = 84.38%  **\n",
      "Step: 5750, loss: 0.1039, accuracy: 95.3125%\n",
      "**  Step 5750, val loss = 0.62, val accuracy = 84.38%  **\n",
      "Step: 5755, loss: 0.0980, accuracy: 100.0000%\n",
      "**  Step 5755, val loss = 0.56, val accuracy = 85.94%  **\n",
      "Step: 5760, loss: 0.0638, accuracy: 100.0000%\n",
      "**  Step 5760, val loss = 0.66, val accuracy = 73.44%  **\n",
      "Step: 5765, loss: 0.1050, accuracy: 96.8750%\n",
      "**  Step 5765, val loss = 0.35, val accuracy = 87.50%  **\n",
      "Step: 5770, loss: 0.2307, accuracy: 96.8750%\n",
      "**  Step 5770, val loss = 0.25, val accuracy = 92.19%  **\n",
      "Step: 5775, loss: 0.1172, accuracy: 95.3125%\n",
      "**  Step 5775, val loss = 0.39, val accuracy = 84.38%  **\n",
      "Step: 5780, loss: 0.1373, accuracy: 93.7500%\n",
      "**  Step 5780, val loss = 0.56, val accuracy = 81.25%  **\n",
      "Step: 5785, loss: 0.2258, accuracy: 92.1875%\n",
      "**  Step 5785, val loss = 0.41, val accuracy = 89.06%  **\n",
      "Step: 5790, loss: 0.1184, accuracy: 95.3125%\n",
      "**  Step 5790, val loss = 0.51, val accuracy = 84.38%  **\n",
      "Step: 5795, loss: 0.1946, accuracy: 93.7500%\n",
      "**  Step 5795, val loss = 0.49, val accuracy = 87.50%  **\n",
      "Step: 5800, loss: 0.1126, accuracy: 96.8750%\n",
      "**  Step 5800, val loss = 0.28, val accuracy = 84.38%  **\n",
      "Step: 5805, loss: 0.1095, accuracy: 95.3125%\n",
      "**  Step 5805, val loss = 0.31, val accuracy = 92.19%  **\n",
      "Step: 5810, loss: 0.1441, accuracy: 96.8750%\n",
      "**  Step 5810, val loss = 0.70, val accuracy = 76.56%  **\n",
      "Step: 5815, loss: 0.1508, accuracy: 93.7500%\n",
      "**  Step 5815, val loss = 0.92, val accuracy = 84.38%  **\n",
      "Step: 5820, loss: 0.0740, accuracy: 98.4375%\n",
      "**  Step 5820, val loss = 0.47, val accuracy = 87.50%  **\n",
      "Step: 5825, loss: 0.0859, accuracy: 98.4375%\n",
      "**  Step 5825, val loss = 0.53, val accuracy = 87.50%  **\n",
      "Step: 5830, loss: 0.1060, accuracy: 96.8750%\n",
      "**  Step 5830, val loss = 0.63, val accuracy = 85.94%  **\n",
      "Step: 5835, loss: 0.0727, accuracy: 96.8750%\n",
      "**  Step 5835, val loss = 0.32, val accuracy = 90.62%  **\n",
      "Step: 5840, loss: 0.1594, accuracy: 95.3125%\n",
      "**  Step 5840, val loss = 0.71, val accuracy = 82.81%  **\n",
      "Step: 5845, loss: 0.2042, accuracy: 93.7500%\n",
      "**  Step 5845, val loss = 0.16, val accuracy = 95.31%  **\n",
      "Step: 5850, loss: 0.0837, accuracy: 100.0000%\n",
      "**  Step 5850, val loss = 0.39, val accuracy = 87.50%  **\n",
      "Step: 5855, loss: 0.0895, accuracy: 100.0000%\n",
      "**  Step 5855, val loss = 0.47, val accuracy = 87.50%  **\n",
      "Step: 5860, loss: 0.1584, accuracy: 96.8750%\n",
      "**  Step 5860, val loss = 0.51, val accuracy = 84.38%  **\n",
      "Step: 5865, loss: 0.1138, accuracy: 96.8750%\n",
      "**  Step 5865, val loss = 0.96, val accuracy = 84.38%  **\n",
      "Step: 5870, loss: 0.1366, accuracy: 93.7500%\n",
      "**  Step 5870, val loss = 0.43, val accuracy = 84.38%  **\n",
      "Step: 5875, loss: 0.1235, accuracy: 98.4375%\n",
      "**  Step 5875, val loss = 0.45, val accuracy = 89.06%  **\n",
      "Step: 5880, loss: 0.0844, accuracy: 100.0000%\n",
      "**  Step 5880, val loss = 0.44, val accuracy = 87.50%  **\n",
      "Step: 5885, loss: 0.0944, accuracy: 98.4375%\n",
      "**  Step 5885, val loss = 0.48, val accuracy = 84.38%  **\n",
      "Step: 5890, loss: 0.2640, accuracy: 90.6250%\n",
      "**  Step 5890, val loss = 0.65, val accuracy = 79.69%  **\n",
      "Step: 5895, loss: 0.0839, accuracy: 98.4375%\n",
      "**  Step 5895, val loss = 0.73, val accuracy = 82.81%  **\n",
      "Step: 5900, loss: 0.0886, accuracy: 98.4375%\n",
      "**  Step 5900, val loss = 0.75, val accuracy = 87.50%  **\n",
      "Step: 5905, loss: 0.0816, accuracy: 98.4375%\n",
      "**  Step 5905, val loss = 0.37, val accuracy = 87.50%  **\n",
      "Step: 5910, loss: 0.1372, accuracy: 96.8750%\n",
      "**  Step 5910, val loss = 0.57, val accuracy = 79.69%  **\n",
      "Step: 5915, loss: 0.0808, accuracy: 100.0000%\n",
      "**  Step 5915, val loss = 0.57, val accuracy = 85.94%  **\n",
      "Step: 5920, loss: 0.1123, accuracy: 95.3125%\n",
      "**  Step 5920, val loss = 0.67, val accuracy = 82.81%  **\n",
      "Step: 5925, loss: 0.0860, accuracy: 98.4375%\n",
      "**  Step 5925, val loss = 0.25, val accuracy = 93.75%  **\n",
      "Step: 5930, loss: 0.0792, accuracy: 96.8750%\n",
      "**  Step 5930, val loss = 0.64, val accuracy = 81.25%  **\n",
      "Step: 5935, loss: 0.1096, accuracy: 93.7500%\n",
      "**  Step 5935, val loss = 0.21, val accuracy = 96.88%  **\n",
      "Step: 5940, loss: 0.2520, accuracy: 89.0625%\n",
      "**  Step 5940, val loss = 0.26, val accuracy = 89.06%  **\n",
      "Step: 5945, loss: 0.1193, accuracy: 95.3125%\n",
      "**  Step 5945, val loss = 0.38, val accuracy = 92.19%  **\n",
      "Step: 5950, loss: 0.1015, accuracy: 96.8750%\n",
      "**  Step 5950, val loss = 0.60, val accuracy = 84.38%  **\n",
      "Step: 5955, loss: 0.0943, accuracy: 98.4375%\n",
      "**  Step 5955, val loss = 0.52, val accuracy = 85.94%  **\n",
      "Step: 5960, loss: 0.1581, accuracy: 92.1875%\n",
      "**  Step 5960, val loss = 0.38, val accuracy = 85.94%  **\n",
      "Step: 5965, loss: 0.0755, accuracy: 96.8750%\n",
      "**  Step 5965, val loss = 0.64, val accuracy = 85.94%  **\n",
      "Step: 5970, loss: 0.0979, accuracy: 96.8750%\n",
      "**  Step 5970, val loss = 0.72, val accuracy = 87.50%  **\n",
      "Step: 5975, loss: 0.1264, accuracy: 95.3125%\n",
      "**  Step 5975, val loss = 0.41, val accuracy = 92.19%  **\n",
      "Step: 5980, loss: 0.1281, accuracy: 95.3125%\n",
      "**  Step 5980, val loss = 0.42, val accuracy = 87.50%  **\n",
      "Step: 5985, loss: 0.1235, accuracy: 95.3125%\n",
      "**  Step 5985, val loss = 0.52, val accuracy = 89.06%  **\n",
      "Step: 5990, loss: 0.1281, accuracy: 95.3125%\n",
      "**  Step 5990, val loss = 0.35, val accuracy = 85.94%  **\n",
      "Step: 5995, loss: 0.1114, accuracy: 96.8750%\n",
      "**  Step 5995, val loss = 0.65, val accuracy = 84.38%  **\n",
      "Step: 6000, loss: 0.0905, accuracy: 96.8750%\n",
      "**  Step 6000, val loss = 0.66, val accuracy = 76.56%  **\n",
      "Step: 6005, loss: 0.1517, accuracy: 93.7500%\n",
      "**  Step 6005, val loss = 0.60, val accuracy = 82.81%  **\n",
      "Step: 6010, loss: 0.1829, accuracy: 90.6250%\n",
      "**  Step 6010, val loss = 0.49, val accuracy = 85.94%  **\n",
      "Step: 6015, loss: 0.1181, accuracy: 98.4375%\n",
      "**  Step 6015, val loss = 0.55, val accuracy = 82.81%  **\n",
      "Step: 6020, loss: 0.0662, accuracy: 100.0000%\n",
      "**  Step 6020, val loss = 0.61, val accuracy = 79.69%  **\n",
      "Step: 6025, loss: 0.1563, accuracy: 95.3125%\n",
      "**  Step 6025, val loss = 0.92, val accuracy = 76.56%  **\n",
      "Step: 6030, loss: 0.1634, accuracy: 93.7500%\n",
      "**  Step 6030, val loss = 0.66, val accuracy = 82.81%  **\n",
      "Step: 6035, loss: 0.1275, accuracy: 98.4375%\n",
      "**  Step 6035, val loss = 0.23, val accuracy = 93.75%  **\n",
      "Step: 6040, loss: 0.1095, accuracy: 98.4375%\n",
      "**  Step 6040, val loss = 0.67, val accuracy = 84.38%  **\n",
      "Step: 6045, loss: 0.1955, accuracy: 93.7500%\n",
      "**  Step 6045, val loss = 0.26, val accuracy = 90.62%  **\n",
      "Step: 6050, loss: 0.0775, accuracy: 98.4375%\n",
      "**  Step 6050, val loss = 0.42, val accuracy = 87.50%  **\n",
      "Step: 6055, loss: 0.1963, accuracy: 93.7500%\n",
      "**  Step 6055, val loss = 0.19, val accuracy = 93.75%  **\n",
      "Step: 6060, loss: 0.1954, accuracy: 93.7500%\n",
      "**  Step 6060, val loss = 0.51, val accuracy = 82.81%  **\n",
      "Step: 6065, loss: 0.0894, accuracy: 96.8750%\n",
      "**  Step 6065, val loss = 0.58, val accuracy = 82.81%  **\n",
      "Step: 6070, loss: 0.1081, accuracy: 95.3125%\n",
      "**  Step 6070, val loss = 0.65, val accuracy = 85.94%  **\n",
      "Step: 6075, loss: 0.1834, accuracy: 93.7500%\n",
      "**  Step 6075, val loss = 0.69, val accuracy = 78.12%  **\n",
      "Step: 6080, loss: 0.0947, accuracy: 98.4375%\n",
      "**  Step 6080, val loss = 0.77, val accuracy = 73.44%  **\n",
      "Step: 6085, loss: 0.0838, accuracy: 98.4375%\n",
      "**  Step 6085, val loss = 0.60, val accuracy = 82.81%  **\n",
      "Step: 6090, loss: 0.0844, accuracy: 98.4375%\n",
      "**  Step 6090, val loss = 0.84, val accuracy = 81.25%  **\n",
      "Step: 6095, loss: 0.1174, accuracy: 96.8750%\n",
      "**  Step 6095, val loss = 1.10, val accuracy = 71.88%  **\n",
      "Step: 6100, loss: 0.1020, accuracy: 98.4375%\n",
      "**  Step 6100, val loss = 0.84, val accuracy = 84.38%  **\n",
      "Step: 6105, loss: 0.0741, accuracy: 98.4375%\n",
      "**  Step 6105, val loss = 0.39, val accuracy = 92.19%  **\n",
      "Step: 6110, loss: 0.1807, accuracy: 93.7500%\n",
      "**  Step 6110, val loss = 0.32, val accuracy = 87.50%  **\n",
      "Step: 6115, loss: 0.0655, accuracy: 98.4375%\n",
      "**  Step 6115, val loss = 0.67, val accuracy = 81.25%  **\n",
      "Step: 6120, loss: 0.1822, accuracy: 93.7500%\n",
      "**  Step 6120, val loss = 0.93, val accuracy = 76.56%  **\n",
      "Step: 6125, loss: 0.0854, accuracy: 100.0000%\n",
      "**  Step 6125, val loss = 0.87, val accuracy = 82.81%  **\n",
      "Step: 6130, loss: 0.1161, accuracy: 96.8750%\n",
      "**  Step 6130, val loss = 0.62, val accuracy = 81.25%  **\n",
      "Step: 6135, loss: 0.1167, accuracy: 96.8750%\n",
      "**  Step 6135, val loss = 0.78, val accuracy = 75.00%  **\n",
      "Step: 6140, loss: 0.2429, accuracy: 92.1875%\n",
      "**  Step 6140, val loss = 0.57, val accuracy = 85.94%  **\n",
      "Step: 6145, loss: 0.1658, accuracy: 96.8750%\n",
      "**  Step 6145, val loss = 0.55, val accuracy = 87.50%  **\n",
      "Step: 6150, loss: 0.1155, accuracy: 93.7500%\n",
      "**  Step 6150, val loss = 0.44, val accuracy = 85.94%  **\n",
      "Step: 6155, loss: 0.1649, accuracy: 95.3125%\n",
      "**  Step 6155, val loss = 0.71, val accuracy = 73.44%  **\n",
      "Step: 6160, loss: 0.0654, accuracy: 98.4375%\n",
      "**  Step 6160, val loss = 0.59, val accuracy = 82.81%  **\n",
      "Step: 6165, loss: 0.0639, accuracy: 100.0000%\n",
      "**  Step 6165, val loss = 0.50, val accuracy = 84.38%  **\n",
      "Step: 6170, loss: 0.0956, accuracy: 98.4375%\n",
      "**  Step 6170, val loss = 0.49, val accuracy = 87.50%  **\n",
      "Step: 6175, loss: 0.0644, accuracy: 98.4375%\n",
      "**  Step 6175, val loss = 1.22, val accuracy = 71.88%  **\n",
      "Step: 6180, loss: 0.1331, accuracy: 95.3125%\n",
      "**  Step 6180, val loss = 0.80, val accuracy = 79.69%  **\n",
      "Step: 6185, loss: 0.1555, accuracy: 92.1875%\n",
      "**  Step 6185, val loss = 0.34, val accuracy = 87.50%  **\n",
      "Step: 6190, loss: 0.1145, accuracy: 96.8750%\n",
      "**  Step 6190, val loss = 0.66, val accuracy = 78.12%  **\n",
      "Step: 6195, loss: 0.1818, accuracy: 93.7500%\n",
      "**  Step 6195, val loss = 0.49, val accuracy = 87.50%  **\n",
      "Step: 6200, loss: 0.0733, accuracy: 100.0000%\n",
      "**  Step 6200, val loss = 0.34, val accuracy = 85.94%  **\n",
      "Step: 6205, loss: 0.1643, accuracy: 93.7500%\n",
      "**  Step 6205, val loss = 0.62, val accuracy = 79.69%  **\n",
      "Step: 6210, loss: 0.0865, accuracy: 100.0000%\n",
      "**  Step 6210, val loss = 0.17, val accuracy = 92.19%  **\n",
      "Step: 6215, loss: 0.1154, accuracy: 96.8750%\n",
      "**  Step 6215, val loss = 0.52, val accuracy = 85.94%  **\n",
      "Step: 6220, loss: 0.1773, accuracy: 93.7500%\n",
      "**  Step 6220, val loss = 0.71, val accuracy = 79.69%  **\n",
      "Step: 6225, loss: 0.0801, accuracy: 96.8750%\n",
      "**  Step 6225, val loss = 0.53, val accuracy = 87.50%  **\n",
      "Step: 6230, loss: 0.0764, accuracy: 98.4375%\n",
      "**  Step 6230, val loss = 1.34, val accuracy = 82.81%  **\n",
      "Step: 6235, loss: 0.1073, accuracy: 96.8750%\n",
      "**  Step 6235, val loss = 0.48, val accuracy = 85.94%  **\n",
      "Step: 6240, loss: 0.1674, accuracy: 96.8750%\n",
      "**  Step 6240, val loss = 0.54, val accuracy = 85.94%  **\n",
      "Step: 6245, loss: 0.1564, accuracy: 95.3125%\n",
      "**  Step 6245, val loss = 0.77, val accuracy = 85.94%  **\n",
      "Step: 6250, loss: 0.1000, accuracy: 98.4375%\n",
      "**  Step 6250, val loss = 0.75, val accuracy = 78.12%  **\n",
      "Step: 6255, loss: 0.1550, accuracy: 96.8750%\n",
      "**  Step 6255, val loss = 0.58, val accuracy = 82.81%  **\n",
      "Step: 6260, loss: 0.0520, accuracy: 100.0000%\n",
      "**  Step 6260, val loss = 0.75, val accuracy = 84.38%  **\n",
      "Step: 6265, loss: 0.1128, accuracy: 96.8750%\n",
      "**  Step 6265, val loss = 0.77, val accuracy = 82.81%  **\n",
      "Step: 6270, loss: 0.1694, accuracy: 93.7500%\n",
      "**  Step 6270, val loss = 0.34, val accuracy = 89.06%  **\n",
      "Step: 6275, loss: 0.0546, accuracy: 100.0000%\n",
      "**  Step 6275, val loss = 0.59, val accuracy = 82.81%  **\n",
      "Step: 6280, loss: 0.1036, accuracy: 96.8750%\n",
      "**  Step 6280, val loss = 0.50, val accuracy = 89.06%  **\n",
      "Step: 6285, loss: 0.0425, accuracy: 98.4375%\n",
      "**  Step 6285, val loss = 0.75, val accuracy = 87.50%  **\n",
      "Step: 6290, loss: 0.0876, accuracy: 98.4375%\n",
      "**  Step 6290, val loss = 0.45, val accuracy = 90.62%  **\n",
      "Step: 6295, loss: 0.1143, accuracy: 96.8750%\n",
      "**  Step 6295, val loss = 0.48, val accuracy = 84.38%  **\n",
      "Step: 6300, loss: 0.1074, accuracy: 95.3125%\n",
      "**  Step 6300, val loss = 0.52, val accuracy = 87.50%  **\n",
      "Step: 6305, loss: 0.1374, accuracy: 93.7500%\n",
      "**  Step 6305, val loss = 0.58, val accuracy = 79.69%  **\n",
      "Step: 6310, loss: 0.1694, accuracy: 93.7500%\n",
      "**  Step 6310, val loss = 0.35, val accuracy = 92.19%  **\n",
      "Step: 6315, loss: 0.1401, accuracy: 93.7500%\n",
      "**  Step 6315, val loss = 0.46, val accuracy = 87.50%  **\n",
      "Step: 6320, loss: 0.0645, accuracy: 100.0000%\n",
      "**  Step 6320, val loss = 0.72, val accuracy = 81.25%  **\n",
      "Step: 6325, loss: 0.0990, accuracy: 98.4375%\n",
      "**  Step 6325, val loss = 0.52, val accuracy = 81.25%  **\n",
      "Step: 6330, loss: 0.1695, accuracy: 96.8750%\n",
      "**  Step 6330, val loss = 1.03, val accuracy = 78.12%  **\n",
      "Step: 6335, loss: 0.1300, accuracy: 93.7500%\n",
      "**  Step 6335, val loss = 0.70, val accuracy = 82.81%  **\n",
      "Step: 6340, loss: 0.1340, accuracy: 98.4375%\n",
      "**  Step 6340, val loss = 0.36, val accuracy = 89.06%  **\n",
      "Step: 6345, loss: 0.1870, accuracy: 93.7500%\n",
      "**  Step 6345, val loss = 0.22, val accuracy = 93.75%  **\n",
      "Step: 6350, loss: 0.1351, accuracy: 95.3125%\n",
      "**  Step 6350, val loss = 0.66, val accuracy = 79.69%  **\n",
      "Step: 6355, loss: 0.1866, accuracy: 93.7500%\n",
      "**  Step 6355, val loss = 0.35, val accuracy = 89.06%  **\n",
      "Step: 6360, loss: 0.1093, accuracy: 95.3125%\n",
      "**  Step 6360, val loss = 0.48, val accuracy = 84.38%  **\n",
      "Step: 6365, loss: 0.0627, accuracy: 98.4375%\n",
      "**  Step 6365, val loss = 0.48, val accuracy = 85.94%  **\n",
      "Step: 6370, loss: 0.0531, accuracy: 98.4375%\n",
      "**  Step 6370, val loss = 0.28, val accuracy = 89.06%  **\n",
      "Step: 6375, loss: 0.1146, accuracy: 95.3125%\n",
      "**  Step 6375, val loss = 0.35, val accuracy = 90.62%  **\n",
      "Step: 6380, loss: 0.1306, accuracy: 96.8750%\n",
      "**  Step 6380, val loss = 0.33, val accuracy = 92.19%  **\n",
      "Step: 6385, loss: 0.2588, accuracy: 90.6250%\n",
      "**  Step 6385, val loss = 1.07, val accuracy = 81.25%  **\n",
      "Step: 6390, loss: 0.1159, accuracy: 96.8750%\n",
      "**  Step 6390, val loss = 0.62, val accuracy = 82.81%  **\n",
      "Step: 6395, loss: 0.0605, accuracy: 100.0000%\n",
      "**  Step 6395, val loss = 0.43, val accuracy = 85.94%  **\n",
      "Step: 6400, loss: 0.0741, accuracy: 98.4375%\n",
      "**  Step 6400, val loss = 0.25, val accuracy = 90.62%  **\n",
      "Step: 6405, loss: 0.1048, accuracy: 96.8750%\n",
      "**  Step 6405, val loss = 0.35, val accuracy = 84.38%  **\n",
      "Step: 6410, loss: 0.0748, accuracy: 100.0000%\n",
      "**  Step 6410, val loss = 0.51, val accuracy = 90.62%  **\n",
      "Step: 6415, loss: 0.1258, accuracy: 93.7500%\n",
      "**  Step 6415, val loss = 0.68, val accuracy = 84.38%  **\n",
      "Step: 6420, loss: 0.1025, accuracy: 95.3125%\n",
      "**  Step 6420, val loss = 0.53, val accuracy = 85.94%  **\n",
      "Step: 6425, loss: 0.0931, accuracy: 98.4375%\n",
      "**  Step 6425, val loss = 0.94, val accuracy = 82.81%  **\n",
      "Step: 6430, loss: 0.0626, accuracy: 100.0000%\n",
      "**  Step 6430, val loss = 0.48, val accuracy = 87.50%  **\n",
      "Step: 6435, loss: 0.1799, accuracy: 93.7500%\n",
      "**  Step 6435, val loss = 0.48, val accuracy = 89.06%  **\n",
      "Step: 6440, loss: 0.1127, accuracy: 96.8750%\n",
      "**  Step 6440, val loss = 0.83, val accuracy = 78.12%  **\n",
      "Step: 6445, loss: 0.0859, accuracy: 96.8750%\n",
      "**  Step 6445, val loss = 0.72, val accuracy = 79.69%  **\n",
      "Step: 6450, loss: 0.1066, accuracy: 100.0000%\n",
      "**  Step 6450, val loss = 0.69, val accuracy = 85.94%  **\n",
      "Step: 6455, loss: 0.0537, accuracy: 100.0000%\n",
      "**  Step 6455, val loss = 0.34, val accuracy = 87.50%  **\n",
      "Step: 6460, loss: 0.1315, accuracy: 95.3125%\n",
      "**  Step 6460, val loss = 0.62, val accuracy = 90.62%  **\n",
      "Step: 6465, loss: 0.1325, accuracy: 95.3125%\n",
      "**  Step 6465, val loss = 0.34, val accuracy = 87.50%  **\n",
      "Step: 6470, loss: 0.0736, accuracy: 98.4375%\n",
      "**  Step 6470, val loss = 0.34, val accuracy = 90.62%  **\n",
      "Step: 6475, loss: 0.0970, accuracy: 96.8750%\n",
      "**  Step 6475, val loss = 1.15, val accuracy = 75.00%  **\n",
      "Step: 6480, loss: 0.0830, accuracy: 96.8750%\n",
      "**  Step 6480, val loss = 0.49, val accuracy = 87.50%  **\n",
      "Step: 6485, loss: 0.1206, accuracy: 96.8750%\n",
      "**  Step 6485, val loss = 0.64, val accuracy = 82.81%  **\n",
      "Step: 6490, loss: 0.1381, accuracy: 98.4375%\n",
      "**  Step 6490, val loss = 1.02, val accuracy = 75.00%  **\n",
      "Step: 6495, loss: 0.0928, accuracy: 98.4375%\n",
      "**  Step 6495, val loss = 0.75, val accuracy = 82.81%  **\n",
      "Step: 6500, loss: 0.1335, accuracy: 96.8750%\n",
      "**  Step 6500, val loss = 0.26, val accuracy = 90.62%  **\n",
      "Step: 6505, loss: 0.0645, accuracy: 98.4375%\n",
      "**  Step 6505, val loss = 0.69, val accuracy = 79.69%  **\n",
      "Step: 6510, loss: 0.1260, accuracy: 95.3125%\n",
      "**  Step 6510, val loss = 0.18, val accuracy = 95.31%  **\n",
      "Step: 6515, loss: 0.1336, accuracy: 95.3125%\n",
      "**  Step 6515, val loss = 0.62, val accuracy = 79.69%  **\n",
      "Step: 6520, loss: 0.0790, accuracy: 98.4375%\n",
      "**  Step 6520, val loss = 0.55, val accuracy = 87.50%  **\n",
      "Step: 6525, loss: 0.1600, accuracy: 92.1875%\n",
      "**  Step 6525, val loss = 0.20, val accuracy = 90.62%  **\n",
      "Step: 6530, loss: 0.1131, accuracy: 95.3125%\n",
      "**  Step 6530, val loss = 0.58, val accuracy = 82.81%  **\n",
      "Step: 6535, loss: 0.1013, accuracy: 95.3125%\n",
      "**  Step 6535, val loss = 0.65, val accuracy = 84.38%  **\n",
      "Step: 6540, loss: 0.1511, accuracy: 95.3125%\n",
      "**  Step 6540, val loss = 0.45, val accuracy = 89.06%  **\n",
      "Step: 6545, loss: 0.1420, accuracy: 96.8750%\n",
      "**  Step 6545, val loss = 0.70, val accuracy = 78.12%  **\n",
      "Step: 6550, loss: 0.1361, accuracy: 96.8750%\n",
      "**  Step 6550, val loss = 0.26, val accuracy = 92.19%  **\n",
      "Step: 6555, loss: 0.0887, accuracy: 98.4375%\n",
      "**  Step 6555, val loss = 1.16, val accuracy = 89.06%  **\n",
      "Step: 6560, loss: 0.1713, accuracy: 95.3125%\n",
      "**  Step 6560, val loss = 0.58, val accuracy = 87.50%  **\n",
      "Step: 6565, loss: 0.0502, accuracy: 98.4375%\n",
      "**  Step 6565, val loss = 0.64, val accuracy = 76.56%  **\n",
      "Step: 6570, loss: 0.1191, accuracy: 95.3125%\n",
      "**  Step 6570, val loss = 0.64, val accuracy = 84.38%  **\n",
      "Step: 6575, loss: 0.1036, accuracy: 98.4375%\n",
      "**  Step 6575, val loss = 0.75, val accuracy = 82.81%  **\n",
      "Step: 6580, loss: 0.0698, accuracy: 96.8750%\n",
      "**  Step 6580, val loss = 0.50, val accuracy = 87.50%  **\n",
      "Step: 6585, loss: 0.1132, accuracy: 93.7500%\n",
      "**  Step 6585, val loss = 0.60, val accuracy = 82.81%  **\n",
      "Step: 6590, loss: 0.0572, accuracy: 100.0000%\n",
      "**  Step 6590, val loss = 0.72, val accuracy = 81.25%  **\n",
      "Step: 6595, loss: 0.1418, accuracy: 93.7500%\n",
      "**  Step 6595, val loss = 0.52, val accuracy = 90.62%  **\n",
      "Step: 6600, loss: 0.1141, accuracy: 93.7500%\n",
      "**  Step 6600, val loss = 0.74, val accuracy = 84.38%  **\n",
      "Step: 6605, loss: 0.0722, accuracy: 96.8750%\n",
      "**  Step 6605, val loss = 0.75, val accuracy = 82.81%  **\n",
      "Step: 6610, loss: 0.1049, accuracy: 98.4375%\n",
      "**  Step 6610, val loss = 0.54, val accuracy = 85.94%  **\n",
      "Step: 6615, loss: 0.0844, accuracy: 98.4375%\n",
      "**  Step 6615, val loss = 0.27, val accuracy = 90.62%  **\n",
      "Step: 6620, loss: 0.0338, accuracy: 100.0000%\n",
      "**  Step 6620, val loss = 0.95, val accuracy = 81.25%  **\n",
      "Step: 6625, loss: 0.1627, accuracy: 95.3125%\n",
      "**  Step 6625, val loss = 0.50, val accuracy = 87.50%  **\n",
      "Step: 6630, loss: 0.1324, accuracy: 96.8750%\n",
      "**  Step 6630, val loss = 0.88, val accuracy = 76.56%  **\n",
      "Step: 6635, loss: 0.0583, accuracy: 98.4375%\n",
      "**  Step 6635, val loss = 0.62, val accuracy = 79.69%  **\n",
      "Step: 6640, loss: 0.1211, accuracy: 96.8750%\n",
      "**  Step 6640, val loss = 0.76, val accuracy = 87.50%  **\n",
      "Step: 6645, loss: 0.0748, accuracy: 96.8750%\n",
      "**  Step 6645, val loss = 0.33, val accuracy = 87.50%  **\n",
      "Step: 6650, loss: 0.0681, accuracy: 96.8750%\n",
      "**  Step 6650, val loss = 0.66, val accuracy = 84.38%  **\n",
      "Step: 6655, loss: 0.0989, accuracy: 98.4375%\n",
      "**  Step 6655, val loss = 0.89, val accuracy = 87.50%  **\n",
      "Step: 6660, loss: 0.1063, accuracy: 93.7500%\n",
      "**  Step 6660, val loss = 0.57, val accuracy = 82.81%  **\n",
      "Step: 6665, loss: 0.1662, accuracy: 95.3125%\n",
      "**  Step 6665, val loss = 0.66, val accuracy = 79.69%  **\n",
      "Step: 6670, loss: 0.1022, accuracy: 96.8750%\n",
      "**  Step 6670, val loss = 0.52, val accuracy = 87.50%  **\n",
      "Step: 6675, loss: 0.0452, accuracy: 100.0000%\n",
      "**  Step 6675, val loss = 0.88, val accuracy = 87.50%  **\n",
      "Step: 6680, loss: 0.0975, accuracy: 96.8750%\n",
      "**  Step 6680, val loss = 0.79, val accuracy = 81.25%  **\n",
      "Step: 6685, loss: 0.2320, accuracy: 89.0625%\n",
      "**  Step 6685, val loss = 0.44, val accuracy = 84.38%  **\n",
      "Step: 6690, loss: 0.1163, accuracy: 95.3125%\n",
      "**  Step 6690, val loss = 0.54, val accuracy = 85.94%  **\n",
      "Step: 6695, loss: 0.0590, accuracy: 98.4375%\n",
      "**  Step 6695, val loss = 0.68, val accuracy = 78.12%  **\n",
      "Step: 6700, loss: 0.1226, accuracy: 96.8750%\n",
      "**  Step 6700, val loss = 0.25, val accuracy = 90.62%  **\n",
      "Step: 6705, loss: 0.0643, accuracy: 98.4375%\n",
      "**  Step 6705, val loss = 0.70, val accuracy = 79.69%  **\n",
      "Step: 6710, loss: 0.1568, accuracy: 95.3125%\n",
      "**  Step 6710, val loss = 0.92, val accuracy = 79.69%  **\n",
      "Step: 6715, loss: 0.0811, accuracy: 98.4375%\n",
      "**  Step 6715, val loss = 0.60, val accuracy = 84.38%  **\n",
      "Step: 6720, loss: 0.0730, accuracy: 96.8750%\n",
      "**  Step 6720, val loss = 0.47, val accuracy = 84.38%  **\n",
      "Step: 6725, loss: 0.1000, accuracy: 95.3125%\n",
      "**  Step 6725, val loss = 0.79, val accuracy = 76.56%  **\n",
      "Step: 6730, loss: 0.0864, accuracy: 98.4375%\n",
      "**  Step 6730, val loss = 0.74, val accuracy = 87.50%  **\n",
      "Step: 6735, loss: 0.1327, accuracy: 96.8750%\n",
      "**  Step 6735, val loss = 0.52, val accuracy = 84.38%  **\n",
      "Step: 6740, loss: 0.1011, accuracy: 96.8750%\n",
      "**  Step 6740, val loss = 0.59, val accuracy = 84.38%  **\n",
      "Step: 6745, loss: 0.0759, accuracy: 96.8750%\n",
      "**  Step 6745, val loss = 0.79, val accuracy = 81.25%  **\n",
      "Step: 6750, loss: 0.1076, accuracy: 96.8750%\n",
      "**  Step 6750, val loss = 1.50, val accuracy = 71.88%  **\n",
      "Step: 6755, loss: 0.0685, accuracy: 98.4375%\n",
      "**  Step 6755, val loss = 0.32, val accuracy = 89.06%  **\n",
      "Step: 6760, loss: 0.1634, accuracy: 95.3125%\n",
      "**  Step 6760, val loss = 0.67, val accuracy = 85.94%  **\n",
      "Step: 6765, loss: 0.0670, accuracy: 98.4375%\n",
      "**  Step 6765, val loss = 0.58, val accuracy = 89.06%  **\n",
      "Step: 6770, loss: 0.1266, accuracy: 95.3125%\n",
      "**  Step 6770, val loss = 0.46, val accuracy = 82.81%  **\n",
      "Step: 6775, loss: 0.0414, accuracy: 100.0000%\n",
      "**  Step 6775, val loss = 1.66, val accuracy = 78.12%  **\n",
      "Step: 6780, loss: 0.1375, accuracy: 93.7500%\n",
      "**  Step 6780, val loss = 0.44, val accuracy = 90.62%  **\n",
      "Step: 6785, loss: 0.1026, accuracy: 95.3125%\n",
      "**  Step 6785, val loss = 0.81, val accuracy = 79.69%  **\n",
      "Step: 6790, loss: 0.1186, accuracy: 95.3125%\n",
      "**  Step 6790, val loss = 0.36, val accuracy = 89.06%  **\n",
      "Step: 6795, loss: 0.2085, accuracy: 90.6250%\n",
      "**  Step 6795, val loss = 0.74, val accuracy = 87.50%  **\n",
      "Step: 6800, loss: 0.1894, accuracy: 93.7500%\n",
      "**  Step 6800, val loss = 0.67, val accuracy = 82.81%  **\n",
      "Step: 6805, loss: 0.0872, accuracy: 96.8750%\n",
      "**  Step 6805, val loss = 0.64, val accuracy = 82.81%  **\n",
      "Step: 6810, loss: 0.1405, accuracy: 93.7500%\n",
      "**  Step 6810, val loss = 0.52, val accuracy = 81.25%  **\n",
      "Step: 6815, loss: 0.0799, accuracy: 98.4375%\n",
      "**  Step 6815, val loss = 0.72, val accuracy = 85.94%  **\n",
      "Step: 6820, loss: 0.0676, accuracy: 98.4375%\n",
      "**  Step 6820, val loss = 0.54, val accuracy = 82.81%  **\n",
      "Step: 6825, loss: 0.0965, accuracy: 100.0000%\n",
      "**  Step 6825, val loss = 0.61, val accuracy = 87.50%  **\n",
      "Step: 6830, loss: 0.1181, accuracy: 96.8750%\n",
      "**  Step 6830, val loss = 0.44, val accuracy = 87.50%  **\n",
      "Step: 6835, loss: 0.0888, accuracy: 98.4375%\n",
      "**  Step 6835, val loss = 0.30, val accuracy = 90.62%  **\n",
      "Step: 6840, loss: 0.1035, accuracy: 98.4375%\n",
      "**  Step 6840, val loss = 0.88, val accuracy = 84.38%  **\n",
      "Step: 6845, loss: 0.1004, accuracy: 96.8750%\n",
      "**  Step 6845, val loss = 0.53, val accuracy = 89.06%  **\n",
      "Step: 6850, loss: 0.1090, accuracy: 98.4375%\n",
      "**  Step 6850, val loss = 0.23, val accuracy = 89.06%  **\n",
      "Step: 6855, loss: 0.0694, accuracy: 98.4375%\n",
      "**  Step 6855, val loss = 0.56, val accuracy = 81.25%  **\n",
      "Step: 6860, loss: 0.0689, accuracy: 98.4375%\n",
      "**  Step 6860, val loss = 0.79, val accuracy = 79.69%  **\n",
      "Step: 6865, loss: 0.0937, accuracy: 95.3125%\n",
      "**  Step 6865, val loss = 0.56, val accuracy = 82.81%  **\n",
      "Step: 6870, loss: 0.0770, accuracy: 98.4375%\n",
      "**  Step 6870, val loss = 0.43, val accuracy = 89.06%  **\n",
      "Step: 6875, loss: 0.0664, accuracy: 100.0000%\n",
      "**  Step 6875, val loss = 0.29, val accuracy = 90.62%  **\n",
      "Step: 6880, loss: 0.1286, accuracy: 95.3125%\n",
      "**  Step 6880, val loss = 0.97, val accuracy = 81.25%  **\n",
      "Step: 6885, loss: 0.0516, accuracy: 100.0000%\n",
      "**  Step 6885, val loss = 0.44, val accuracy = 84.38%  **\n",
      "Step: 6890, loss: 0.1252, accuracy: 96.8750%\n",
      "**  Step 6890, val loss = 0.57, val accuracy = 78.12%  **\n",
      "Step: 6895, loss: 0.0636, accuracy: 98.4375%\n",
      "**  Step 6895, val loss = 0.55, val accuracy = 87.50%  **\n",
      "Step: 6900, loss: 0.1082, accuracy: 95.3125%\n",
      "**  Step 6900, val loss = 0.58, val accuracy = 82.81%  **\n",
      "Step: 6905, loss: 0.1542, accuracy: 96.8750%\n",
      "**  Step 6905, val loss = 0.31, val accuracy = 89.06%  **\n",
      "Step: 6910, loss: 0.0761, accuracy: 96.8750%\n",
      "**  Step 6910, val loss = 0.74, val accuracy = 79.69%  **\n",
      "Step: 6915, loss: 0.0616, accuracy: 98.4375%\n",
      "**  Step 6915, val loss = 0.79, val accuracy = 79.69%  **\n",
      "Step: 6920, loss: 0.0829, accuracy: 100.0000%\n",
      "**  Step 6920, val loss = 0.66, val accuracy = 81.25%  **\n",
      "Step: 6925, loss: 0.1196, accuracy: 95.3125%\n",
      "**  Step 6925, val loss = 0.49, val accuracy = 90.62%  **\n",
      "Step: 6930, loss: 0.1026, accuracy: 96.8750%\n",
      "**  Step 6930, val loss = 0.47, val accuracy = 84.38%  **\n",
      "Step: 6935, loss: 0.1628, accuracy: 95.3125%\n",
      "**  Step 6935, val loss = 0.30, val accuracy = 90.62%  **\n",
      "Step: 6940, loss: 0.1862, accuracy: 92.1875%\n",
      "**  Step 6940, val loss = 0.48, val accuracy = 85.94%  **\n",
      "Step: 6945, loss: 0.0929, accuracy: 98.4375%\n",
      "**  Step 6945, val loss = 0.98, val accuracy = 75.00%  **\n",
      "Step: 6950, loss: 0.0772, accuracy: 98.4375%\n",
      "**  Step 6950, val loss = 0.44, val accuracy = 90.62%  **\n",
      "Step: 6955, loss: 0.0962, accuracy: 95.3125%\n",
      "**  Step 6955, val loss = 0.73, val accuracy = 85.94%  **\n",
      "Step: 6960, loss: 0.0525, accuracy: 98.4375%\n",
      "**  Step 6960, val loss = 0.41, val accuracy = 84.38%  **\n",
      "Step: 6965, loss: 0.1621, accuracy: 96.8750%\n",
      "**  Step 6965, val loss = 0.67, val accuracy = 82.81%  **\n",
      "Step: 6970, loss: 0.1229, accuracy: 96.8750%\n",
      "**  Step 6970, val loss = 0.19, val accuracy = 96.88%  **\n",
      "Step: 6975, loss: 0.1108, accuracy: 96.8750%\n",
      "**  Step 6975, val loss = 0.45, val accuracy = 92.19%  **\n",
      "Step: 6980, loss: 0.0433, accuracy: 100.0000%\n",
      "**  Step 6980, val loss = 0.68, val accuracy = 82.81%  **\n",
      "Step: 6985, loss: 0.0311, accuracy: 100.0000%\n",
      "**  Step 6985, val loss = 0.61, val accuracy = 82.81%  **\n",
      "Step: 6990, loss: 0.1139, accuracy: 98.4375%\n",
      "**  Step 6990, val loss = 0.41, val accuracy = 85.94%  **\n",
      "Step: 6995, loss: 0.1222, accuracy: 95.3125%\n",
      "**  Step 6995, val loss = 0.47, val accuracy = 82.81%  **\n",
      "Step: 7000, loss: 0.0369, accuracy: 98.4375%\n",
      "**  Step 7000, val loss = 0.87, val accuracy = 70.31%  **\n",
      "Step: 7005, loss: 0.1013, accuracy: 95.3125%\n",
      "**  Step 7005, val loss = 0.36, val accuracy = 89.06%  **\n",
      "Step: 7010, loss: 0.1695, accuracy: 93.7500%\n",
      "**  Step 7010, val loss = 0.76, val accuracy = 84.38%  **\n",
      "Step: 7015, loss: 0.1454, accuracy: 95.3125%\n",
      "**  Step 7015, val loss = 0.88, val accuracy = 81.25%  **\n",
      "Step: 7020, loss: 0.1668, accuracy: 92.1875%\n",
      "**  Step 7020, val loss = 0.33, val accuracy = 90.62%  **\n",
      "Step: 7025, loss: 0.1059, accuracy: 96.8750%\n",
      "**  Step 7025, val loss = 0.58, val accuracy = 82.81%  **\n",
      "Step: 7030, loss: 0.1216, accuracy: 96.8750%\n",
      "**  Step 7030, val loss = 0.37, val accuracy = 85.94%  **\n",
      "Step: 7035, loss: 0.1865, accuracy: 90.6250%\n",
      "**  Step 7035, val loss = 0.43, val accuracy = 87.50%  **\n",
      "Step: 7040, loss: 0.1364, accuracy: 96.8750%\n",
      "**  Step 7040, val loss = 0.36, val accuracy = 90.62%  **\n",
      "Step: 7045, loss: 0.0864, accuracy: 98.4375%\n",
      "**  Step 7045, val loss = 0.70, val accuracy = 82.81%  **\n",
      "Step: 7050, loss: 0.1108, accuracy: 93.7500%\n",
      "**  Step 7050, val loss = 0.30, val accuracy = 92.19%  **\n",
      "Step: 7055, loss: 0.0889, accuracy: 96.8750%\n",
      "**  Step 7055, val loss = 0.80, val accuracy = 82.81%  **\n",
      "Step: 7060, loss: 0.0813, accuracy: 96.8750%\n",
      "**  Step 7060, val loss = 0.65, val accuracy = 81.25%  **\n",
      "Step: 7065, loss: 0.0800, accuracy: 98.4375%\n",
      "**  Step 7065, val loss = 0.46, val accuracy = 87.50%  **\n",
      "Step: 7070, loss: 0.0945, accuracy: 98.4375%\n",
      "**  Step 7070, val loss = 0.39, val accuracy = 89.06%  **\n",
      "Step: 7075, loss: 0.0726, accuracy: 100.0000%\n",
      "**  Step 7075, val loss = 0.31, val accuracy = 89.06%  **\n",
      "Step: 7080, loss: 0.1251, accuracy: 95.3125%\n",
      "**  Step 7080, val loss = 0.37, val accuracy = 87.50%  **\n",
      "Step: 7085, loss: 0.1182, accuracy: 96.8750%\n",
      "**  Step 7085, val loss = 0.68, val accuracy = 79.69%  **\n",
      "Step: 7090, loss: 0.0877, accuracy: 100.0000%\n",
      "**  Step 7090, val loss = 0.70, val accuracy = 87.50%  **\n",
      "Step: 7095, loss: 0.0621, accuracy: 98.4375%\n",
      "**  Step 7095, val loss = 0.34, val accuracy = 90.62%  **\n",
      "Step: 7100, loss: 0.1119, accuracy: 96.8750%\n",
      "**  Step 7100, val loss = 0.92, val accuracy = 89.06%  **\n",
      "Step: 7105, loss: 0.1398, accuracy: 93.7500%\n",
      "**  Step 7105, val loss = 0.77, val accuracy = 75.00%  **\n",
      "Step: 7110, loss: 0.0719, accuracy: 98.4375%\n",
      "**  Step 7110, val loss = 0.66, val accuracy = 87.50%  **\n",
      "Step: 7115, loss: 0.1060, accuracy: 96.8750%\n",
      "**  Step 7115, val loss = 0.61, val accuracy = 84.38%  **\n",
      "Step: 7120, loss: 0.0699, accuracy: 98.4375%\n",
      "**  Step 7120, val loss = 0.91, val accuracy = 82.81%  **\n",
      "Step: 7125, loss: 0.0623, accuracy: 98.4375%\n",
      "**  Step 7125, val loss = 0.32, val accuracy = 87.50%  **\n",
      "Step: 7130, loss: 0.0406, accuracy: 100.0000%\n",
      "**  Step 7130, val loss = 0.64, val accuracy = 81.25%  **\n",
      "Step: 7135, loss: 0.0662, accuracy: 100.0000%\n",
      "**  Step 7135, val loss = 0.52, val accuracy = 87.50%  **\n",
      "Step: 7140, loss: 0.0930, accuracy: 100.0000%\n",
      "**  Step 7140, val loss = 0.63, val accuracy = 76.56%  **\n",
      "Step: 7145, loss: 0.0459, accuracy: 100.0000%\n",
      "**  Step 7145, val loss = 1.01, val accuracy = 78.12%  **\n",
      "Step: 7150, loss: 0.1189, accuracy: 93.7500%\n",
      "**  Step 7150, val loss = 0.32, val accuracy = 90.62%  **\n",
      "Step: 7155, loss: 0.0311, accuracy: 100.0000%\n",
      "**  Step 7155, val loss = 0.53, val accuracy = 82.81%  **\n",
      "Step: 7160, loss: 0.0483, accuracy: 98.4375%\n",
      "**  Step 7160, val loss = 0.38, val accuracy = 89.06%  **\n",
      "Step: 7165, loss: 0.0750, accuracy: 100.0000%\n",
      "**  Step 7165, val loss = 0.62, val accuracy = 81.25%  **\n",
      "Step: 7170, loss: 0.0828, accuracy: 96.8750%\n",
      "**  Step 7170, val loss = 0.40, val accuracy = 90.62%  **\n",
      "Step: 7175, loss: 0.0742, accuracy: 98.4375%\n",
      "**  Step 7175, val loss = 0.63, val accuracy = 87.50%  **\n",
      "Step: 7180, loss: 0.1130, accuracy: 95.3125%\n",
      "**  Step 7180, val loss = 0.51, val accuracy = 87.50%  **\n",
      "Step: 7185, loss: 0.1098, accuracy: 98.4375%\n",
      "**  Step 7185, val loss = 0.34, val accuracy = 87.50%  **\n",
      "Step: 7190, loss: 0.1078, accuracy: 96.8750%\n",
      "**  Step 7190, val loss = 0.54, val accuracy = 82.81%  **\n",
      "Step: 7195, loss: 0.0770, accuracy: 96.8750%\n",
      "**  Step 7195, val loss = 0.47, val accuracy = 84.38%  **\n",
      "Step: 7200, loss: 0.0874, accuracy: 98.4375%\n",
      "**  Step 7200, val loss = 0.54, val accuracy = 82.81%  **\n",
      "Step: 7205, loss: 0.1056, accuracy: 96.8750%\n",
      "**  Step 7205, val loss = 0.41, val accuracy = 89.06%  **\n",
      "Step: 7210, loss: 0.0783, accuracy: 98.4375%\n",
      "**  Step 7210, val loss = 0.81, val accuracy = 84.38%  **\n",
      "Step: 7215, loss: 0.0361, accuracy: 98.4375%\n",
      "**  Step 7215, val loss = 0.67, val accuracy = 82.81%  **\n",
      "Step: 7220, loss: 0.1633, accuracy: 93.7500%\n",
      "**  Step 7220, val loss = 0.71, val accuracy = 82.81%  **\n",
      "Step: 7225, loss: 0.0937, accuracy: 96.8750%\n",
      "**  Step 7225, val loss = 0.61, val accuracy = 84.38%  **\n",
      "Step: 7230, loss: 0.0745, accuracy: 98.4375%\n",
      "**  Step 7230, val loss = 0.70, val accuracy = 76.56%  **\n",
      "Step: 7235, loss: 0.0969, accuracy: 98.4375%\n",
      "**  Step 7235, val loss = 0.72, val accuracy = 82.81%  **\n",
      "Step: 7240, loss: 0.1127, accuracy: 96.8750%\n",
      "**  Step 7240, val loss = 0.93, val accuracy = 85.94%  **\n",
      "Step: 7245, loss: 0.1754, accuracy: 95.3125%\n",
      "**  Step 7245, val loss = 0.80, val accuracy = 82.81%  **\n",
      "Step: 7250, loss: 0.1200, accuracy: 95.3125%\n",
      "**  Step 7250, val loss = 0.43, val accuracy = 85.94%  **\n",
      "Step: 7255, loss: 0.1118, accuracy: 95.3125%\n",
      "**  Step 7255, val loss = 0.43, val accuracy = 85.94%  **\n",
      "Step: 7260, loss: 0.1053, accuracy: 98.4375%\n",
      "**  Step 7260, val loss = 0.46, val accuracy = 84.38%  **\n",
      "Step: 7265, loss: 0.1382, accuracy: 95.3125%\n",
      "**  Step 7265, val loss = 0.56, val accuracy = 81.25%  **\n",
      "Step: 7270, loss: 0.1497, accuracy: 93.7500%\n",
      "**  Step 7270, val loss = 1.02, val accuracy = 79.69%  **\n",
      "Step: 7275, loss: 0.1155, accuracy: 95.3125%\n",
      "**  Step 7275, val loss = 0.45, val accuracy = 89.06%  **\n",
      "Step: 7280, loss: 0.0635, accuracy: 100.0000%\n",
      "**  Step 7280, val loss = 0.70, val accuracy = 85.94%  **\n",
      "Step: 7285, loss: 0.0640, accuracy: 100.0000%\n",
      "**  Step 7285, val loss = 0.92, val accuracy = 75.00%  **\n",
      "Step: 7290, loss: 0.0932, accuracy: 95.3125%\n",
      "**  Step 7290, val loss = 0.71, val accuracy = 84.38%  **\n",
      "Step: 7295, loss: 0.0370, accuracy: 100.0000%\n",
      "**  Step 7295, val loss = 0.53, val accuracy = 87.50%  **\n",
      "Step: 7300, loss: 0.1180, accuracy: 93.7500%\n",
      "**  Step 7300, val loss = 0.65, val accuracy = 79.69%  **\n",
      "Step: 7305, loss: 0.0629, accuracy: 100.0000%\n",
      "**  Step 7305, val loss = 0.50, val accuracy = 84.38%  **\n",
      "Step: 7310, loss: 0.0284, accuracy: 100.0000%\n",
      "**  Step 7310, val loss = 0.81, val accuracy = 82.81%  **\n",
      "Step: 7315, loss: 0.0859, accuracy: 98.4375%\n",
      "**  Step 7315, val loss = 0.65, val accuracy = 79.69%  **\n",
      "Step: 7320, loss: 0.1357, accuracy: 95.3125%\n",
      "**  Step 7320, val loss = 0.58, val accuracy = 79.69%  **\n",
      "Step: 7325, loss: 0.1300, accuracy: 96.8750%\n",
      "**  Step 7325, val loss = 0.78, val accuracy = 85.94%  **\n",
      "Step: 7330, loss: 0.0843, accuracy: 96.8750%\n",
      "**  Step 7330, val loss = 0.69, val accuracy = 84.38%  **\n",
      "Step: 7335, loss: 0.0396, accuracy: 100.0000%\n",
      "**  Step 7335, val loss = 0.60, val accuracy = 89.06%  **\n",
      "Step: 7340, loss: 0.0848, accuracy: 98.4375%\n",
      "**  Step 7340, val loss = 0.25, val accuracy = 90.62%  **\n",
      "Step: 7345, loss: 0.0909, accuracy: 96.8750%\n",
      "**  Step 7345, val loss = 0.80, val accuracy = 84.38%  **\n",
      "Step: 7350, loss: 0.1216, accuracy: 96.8750%\n",
      "**  Step 7350, val loss = 0.49, val accuracy = 82.81%  **\n",
      "Step: 7355, loss: 0.1294, accuracy: 95.3125%\n",
      "**  Step 7355, val loss = 0.53, val accuracy = 81.25%  **\n",
      "Step: 7360, loss: 0.0721, accuracy: 96.8750%\n",
      "**  Step 7360, val loss = 0.68, val accuracy = 78.12%  **\n",
      "Step: 7365, loss: 0.0925, accuracy: 98.4375%\n",
      "**  Step 7365, val loss = 0.41, val accuracy = 89.06%  **\n",
      "Step: 7370, loss: 0.1146, accuracy: 95.3125%\n",
      "**  Step 7370, val loss = 0.81, val accuracy = 85.94%  **\n",
      "Step: 7375, loss: 0.1003, accuracy: 96.8750%\n",
      "**  Step 7375, val loss = 0.57, val accuracy = 82.81%  **\n",
      "Step: 7380, loss: 0.1527, accuracy: 93.7500%\n",
      "**  Step 7380, val loss = 0.56, val accuracy = 85.94%  **\n",
      "Step: 7385, loss: 0.0832, accuracy: 96.8750%\n",
      "**  Step 7385, val loss = 0.70, val accuracy = 78.12%  **\n",
      "Step: 7390, loss: 0.1443, accuracy: 95.3125%\n",
      "**  Step 7390, val loss = 0.31, val accuracy = 90.62%  **\n",
      "Step: 7395, loss: 0.0784, accuracy: 96.8750%\n",
      "**  Step 7395, val loss = 0.35, val accuracy = 92.19%  **\n",
      "Step: 7400, loss: 0.0845, accuracy: 96.8750%\n",
      "**  Step 7400, val loss = 0.62, val accuracy = 82.81%  **\n",
      "Step: 7405, loss: 0.0918, accuracy: 98.4375%\n",
      "**  Step 7405, val loss = 0.64, val accuracy = 85.94%  **\n",
      "Step: 7410, loss: 0.0624, accuracy: 98.4375%\n",
      "**  Step 7410, val loss = 0.47, val accuracy = 87.50%  **\n",
      "Step: 7415, loss: 0.0768, accuracy: 98.4375%\n",
      "**  Step 7415, val loss = 0.57, val accuracy = 82.81%  **\n",
      "Step: 7420, loss: 0.0959, accuracy: 98.4375%\n",
      "**  Step 7420, val loss = 0.90, val accuracy = 78.12%  **\n",
      "Step: 7425, loss: 0.0657, accuracy: 98.4375%\n",
      "**  Step 7425, val loss = 0.49, val accuracy = 85.94%  **\n",
      "Step: 7430, loss: 0.0861, accuracy: 98.4375%\n",
      "**  Step 7430, val loss = 0.49, val accuracy = 78.12%  **\n",
      "Step: 7435, loss: 0.0663, accuracy: 100.0000%\n",
      "**  Step 7435, val loss = 0.97, val accuracy = 85.94%  **\n",
      "Step: 7440, loss: 0.1600, accuracy: 90.6250%\n",
      "**  Step 7440, val loss = 0.66, val accuracy = 87.50%  **\n",
      "Step: 7445, loss: 0.0900, accuracy: 95.3125%\n",
      "**  Step 7445, val loss = 0.72, val accuracy = 82.81%  **\n",
      "Step: 7450, loss: 0.1153, accuracy: 96.8750%\n",
      "**  Step 7450, val loss = 0.59, val accuracy = 82.81%  **\n",
      "Step: 7455, loss: 0.1044, accuracy: 96.8750%\n",
      "**  Step 7455, val loss = 0.85, val accuracy = 84.38%  **\n",
      "Step: 7460, loss: 0.0316, accuracy: 100.0000%\n",
      "**  Step 7460, val loss = 0.37, val accuracy = 89.06%  **\n",
      "Step: 7465, loss: 0.0728, accuracy: 98.4375%\n",
      "**  Step 7465, val loss = 0.62, val accuracy = 84.38%  **\n",
      "Step: 7470, loss: 0.0780, accuracy: 98.4375%\n",
      "**  Step 7470, val loss = 0.53, val accuracy = 90.62%  **\n",
      "Step: 7475, loss: 0.1461, accuracy: 93.7500%\n",
      "**  Step 7475, val loss = 0.63, val accuracy = 87.50%  **\n",
      "Step: 7480, loss: 0.1325, accuracy: 96.8750%\n",
      "**  Step 7480, val loss = 0.61, val accuracy = 82.81%  **\n",
      "Step: 7485, loss: 0.0761, accuracy: 95.3125%\n",
      "**  Step 7485, val loss = 0.58, val accuracy = 84.38%  **\n",
      "Step: 7490, loss: 0.0820, accuracy: 98.4375%\n",
      "**  Step 7490, val loss = 0.29, val accuracy = 90.62%  **\n",
      "Step: 7495, loss: 0.1401, accuracy: 95.3125%\n",
      "**  Step 7495, val loss = 0.50, val accuracy = 87.50%  **\n",
      "Step: 7500, loss: 0.1298, accuracy: 96.8750%\n",
      "**  Step 7500, val loss = 0.36, val accuracy = 89.06%  **\n",
      "Step: 7505, loss: 0.0566, accuracy: 96.8750%\n",
      "**  Step 7505, val loss = 1.11, val accuracy = 78.12%  **\n",
      "Step: 7510, loss: 0.1222, accuracy: 96.8750%\n",
      "**  Step 7510, val loss = 0.32, val accuracy = 90.62%  **\n",
      "Step: 7515, loss: 0.0616, accuracy: 98.4375%\n",
      "**  Step 7515, val loss = 0.50, val accuracy = 82.81%  **\n",
      "Step: 7520, loss: 0.0628, accuracy: 98.4375%\n",
      "**  Step 7520, val loss = 1.13, val accuracy = 75.00%  **\n",
      "Step: 7525, loss: 0.0934, accuracy: 96.8750%\n",
      "**  Step 7525, val loss = 1.29, val accuracy = 76.56%  **\n",
      "Step: 7530, loss: 0.0866, accuracy: 98.4375%\n",
      "**  Step 7530, val loss = 0.67, val accuracy = 82.81%  **\n",
      "Step: 7535, loss: 0.1495, accuracy: 98.4375%\n",
      "**  Step 7535, val loss = 0.26, val accuracy = 92.19%  **\n",
      "Step: 7540, loss: 0.0955, accuracy: 96.8750%\n",
      "**  Step 7540, val loss = 0.68, val accuracy = 85.94%  **\n",
      "Step: 7545, loss: 0.1048, accuracy: 96.8750%\n",
      "**  Step 7545, val loss = 0.48, val accuracy = 81.25%  **\n",
      "Step: 7550, loss: 0.1156, accuracy: 96.8750%\n",
      "**  Step 7550, val loss = 0.34, val accuracy = 84.38%  **\n",
      "Step: 7555, loss: 0.0593, accuracy: 98.4375%\n",
      "**  Step 7555, val loss = 0.60, val accuracy = 89.06%  **\n",
      "Step: 7560, loss: 0.0709, accuracy: 96.8750%\n",
      "**  Step 7560, val loss = 0.37, val accuracy = 84.38%  **\n",
      "Step: 7565, loss: 0.1151, accuracy: 95.3125%\n",
      "**  Step 7565, val loss = 0.44, val accuracy = 90.62%  **\n",
      "Step: 7570, loss: 0.0354, accuracy: 100.0000%\n",
      "**  Step 7570, val loss = 0.74, val accuracy = 82.81%  **\n",
      "Step: 7575, loss: 0.0784, accuracy: 96.8750%\n",
      "**  Step 7575, val loss = 0.78, val accuracy = 82.81%  **\n",
      "Step: 7580, loss: 0.0775, accuracy: 98.4375%\n",
      "**  Step 7580, val loss = 0.47, val accuracy = 89.06%  **\n",
      "Step: 7585, loss: 0.0762, accuracy: 98.4375%\n",
      "**  Step 7585, val loss = 0.35, val accuracy = 87.50%  **\n",
      "Step: 7590, loss: 0.0489, accuracy: 100.0000%\n",
      "**  Step 7590, val loss = 0.47, val accuracy = 89.06%  **\n",
      "Step: 7595, loss: 0.0923, accuracy: 100.0000%\n",
      "**  Step 7595, val loss = 0.43, val accuracy = 78.12%  **\n",
      "Step: 7600, loss: 0.1075, accuracy: 96.8750%\n",
      "**  Step 7600, val loss = 0.32, val accuracy = 85.94%  **\n",
      "Step: 7605, loss: 0.1412, accuracy: 95.3125%\n",
      "**  Step 7605, val loss = 0.76, val accuracy = 85.94%  **\n",
      "Step: 7610, loss: 0.0889, accuracy: 98.4375%\n",
      "**  Step 7610, val loss = 0.63, val accuracy = 84.38%  **\n",
      "Step: 7615, loss: 0.0647, accuracy: 100.0000%\n",
      "**  Step 7615, val loss = 0.27, val accuracy = 87.50%  **\n",
      "Step: 7620, loss: 0.0230, accuracy: 100.0000%\n",
      "**  Step 7620, val loss = 0.60, val accuracy = 87.50%  **\n",
      "Step: 7625, loss: 0.1249, accuracy: 95.3125%\n",
      "**  Step 7625, val loss = 0.54, val accuracy = 82.81%  **\n",
      "Step: 7630, loss: 0.1043, accuracy: 96.8750%\n",
      "**  Step 7630, val loss = 0.68, val accuracy = 81.25%  **\n",
      "Step: 7635, loss: 0.0592, accuracy: 98.4375%\n",
      "**  Step 7635, val loss = 0.20, val accuracy = 89.06%  **\n",
      "Step: 7640, loss: 0.0685, accuracy: 98.4375%\n",
      "**  Step 7640, val loss = 1.05, val accuracy = 84.38%  **\n",
      "Step: 7645, loss: 0.1245, accuracy: 96.8750%\n",
      "**  Step 7645, val loss = 0.56, val accuracy = 82.81%  **\n",
      "Step: 7650, loss: 0.0775, accuracy: 98.4375%\n",
      "**  Step 7650, val loss = 0.50, val accuracy = 85.94%  **\n",
      "Step: 7655, loss: 0.0806, accuracy: 98.4375%\n",
      "**  Step 7655, val loss = 0.57, val accuracy = 84.38%  **\n",
      "Step: 7660, loss: 0.1065, accuracy: 95.3125%\n",
      "**  Step 7660, val loss = 0.52, val accuracy = 89.06%  **\n",
      "Step: 7665, loss: 0.0502, accuracy: 100.0000%\n",
      "**  Step 7665, val loss = 0.59, val accuracy = 82.81%  **\n",
      "Step: 7670, loss: 0.0675, accuracy: 98.4375%\n",
      "**  Step 7670, val loss = 0.56, val accuracy = 85.94%  **\n",
      "Step: 7675, loss: 0.1065, accuracy: 96.8750%\n",
      "**  Step 7675, val loss = 0.36, val accuracy = 90.62%  **\n",
      "Step: 7680, loss: 0.0642, accuracy: 98.4375%\n",
      "**  Step 7680, val loss = 0.30, val accuracy = 93.75%  **\n",
      "Step: 7685, loss: 0.1330, accuracy: 96.8750%\n",
      "**  Step 7685, val loss = 0.30, val accuracy = 89.06%  **\n",
      "Step: 7690, loss: 0.0450, accuracy: 98.4375%\n",
      "**  Step 7690, val loss = 0.33, val accuracy = 95.31%  **\n",
      "Step: 7695, loss: 0.0676, accuracy: 98.4375%\n",
      "**  Step 7695, val loss = 0.61, val accuracy = 84.38%  **\n",
      "Step: 7700, loss: 0.1185, accuracy: 96.8750%\n",
      "**  Step 7700, val loss = 0.60, val accuracy = 87.50%  **\n",
      "Step: 7705, loss: 0.0463, accuracy: 100.0000%\n",
      "**  Step 7705, val loss = 1.02, val accuracy = 75.00%  **\n",
      "Step: 7710, loss: 0.0791, accuracy: 98.4375%\n",
      "**  Step 7710, val loss = 0.51, val accuracy = 84.38%  **\n",
      "Step: 7715, loss: 0.0647, accuracy: 100.0000%\n",
      "**  Step 7715, val loss = 0.28, val accuracy = 90.62%  **\n",
      "Step: 7720, loss: 0.1556, accuracy: 95.3125%\n",
      "**  Step 7720, val loss = 0.84, val accuracy = 84.38%  **\n",
      "Step: 7725, loss: 0.0641, accuracy: 96.8750%\n",
      "**  Step 7725, val loss = 0.54, val accuracy = 84.38%  **\n",
      "Step: 7730, loss: 0.0934, accuracy: 96.8750%\n",
      "**  Step 7730, val loss = 0.69, val accuracy = 84.38%  **\n",
      "Step: 7735, loss: 0.1632, accuracy: 93.7500%\n",
      "**  Step 7735, val loss = 0.65, val accuracy = 85.94%  **\n",
      "Step: 7740, loss: 0.1699, accuracy: 93.7500%\n",
      "**  Step 7740, val loss = 0.42, val accuracy = 90.62%  **\n",
      "Step: 7745, loss: 0.0767, accuracy: 98.4375%\n",
      "**  Step 7745, val loss = 0.29, val accuracy = 90.62%  **\n",
      "Step: 7750, loss: 0.0678, accuracy: 96.8750%\n",
      "**  Step 7750, val loss = 0.36, val accuracy = 90.62%  **\n",
      "Step: 7755, loss: 0.0860, accuracy: 100.0000%\n",
      "**  Step 7755, val loss = 0.32, val accuracy = 85.94%  **\n",
      "Step: 7760, loss: 0.0517, accuracy: 100.0000%\n",
      "**  Step 7760, val loss = 0.88, val accuracy = 82.81%  **\n",
      "Step: 7765, loss: 0.1107, accuracy: 96.8750%\n",
      "**  Step 7765, val loss = 0.40, val accuracy = 90.62%  **\n",
      "Step: 7770, loss: 0.1058, accuracy: 95.3125%\n",
      "**  Step 7770, val loss = 0.30, val accuracy = 92.19%  **\n",
      "Step: 7775, loss: 0.1037, accuracy: 96.8750%\n",
      "**  Step 7775, val loss = 0.58, val accuracy = 84.38%  **\n",
      "Step: 7780, loss: 0.0516, accuracy: 98.4375%\n",
      "**  Step 7780, val loss = 0.81, val accuracy = 85.94%  **\n",
      "Step: 7785, loss: 0.1536, accuracy: 96.8750%\n",
      "**  Step 7785, val loss = 0.93, val accuracy = 84.38%  **\n",
      "Step: 7790, loss: 0.0740, accuracy: 98.4375%\n",
      "**  Step 7790, val loss = 0.54, val accuracy = 82.81%  **\n",
      "Step: 7795, loss: 0.0850, accuracy: 96.8750%\n",
      "**  Step 7795, val loss = 0.32, val accuracy = 90.62%  **\n",
      "Step: 7800, loss: 0.0848, accuracy: 98.4375%\n",
      "**  Step 7800, val loss = 0.59, val accuracy = 87.50%  **\n",
      "Step: 7805, loss: 0.0789, accuracy: 98.4375%\n",
      "**  Step 7805, val loss = 0.51, val accuracy = 82.81%  **\n",
      "Step: 7810, loss: 0.0282, accuracy: 100.0000%\n",
      "**  Step 7810, val loss = 0.71, val accuracy = 87.50%  **\n",
      "Step: 7815, loss: 0.1199, accuracy: 95.3125%\n",
      "**  Step 7815, val loss = 0.88, val accuracy = 75.00%  **\n",
      "Step: 7820, loss: 0.0763, accuracy: 95.3125%\n",
      "**  Step 7820, val loss = 0.42, val accuracy = 89.06%  **\n",
      "Step: 7825, loss: 0.0744, accuracy: 100.0000%\n",
      "**  Step 7825, val loss = 0.54, val accuracy = 85.94%  **\n",
      "Step: 7830, loss: 0.0986, accuracy: 96.8750%\n",
      "**  Step 7830, val loss = 0.57, val accuracy = 84.38%  **\n",
      "Step: 7835, loss: 0.1532, accuracy: 95.3125%\n",
      "**  Step 7835, val loss = 0.67, val accuracy = 84.38%  **\n",
      "Step: 7840, loss: 0.0443, accuracy: 100.0000%\n",
      "**  Step 7840, val loss = 0.58, val accuracy = 85.94%  **\n",
      "Step: 7845, loss: 0.1220, accuracy: 95.3125%\n",
      "**  Step 7845, val loss = 0.38, val accuracy = 87.50%  **\n",
      "Step: 7850, loss: 0.0911, accuracy: 96.8750%\n",
      "**  Step 7850, val loss = 0.31, val accuracy = 89.06%  **\n",
      "Step: 7855, loss: 0.0988, accuracy: 95.3125%\n",
      "**  Step 7855, val loss = 0.29, val accuracy = 90.62%  **\n",
      "Step: 7860, loss: 0.1007, accuracy: 96.8750%\n",
      "**  Step 7860, val loss = 0.69, val accuracy = 85.94%  **\n",
      "Step: 7865, loss: 0.1694, accuracy: 93.7500%\n",
      "**  Step 7865, val loss = 0.78, val accuracy = 85.94%  **\n",
      "Step: 7870, loss: 0.0934, accuracy: 98.4375%\n",
      "**  Step 7870, val loss = 0.50, val accuracy = 82.81%  **\n",
      "Step: 7875, loss: 0.0549, accuracy: 100.0000%\n",
      "**  Step 7875, val loss = 0.42, val accuracy = 85.94%  **\n",
      "Step: 7880, loss: 0.0699, accuracy: 100.0000%\n",
      "**  Step 7880, val loss = 0.73, val accuracy = 81.25%  **\n",
      "Step: 7885, loss: 0.0938, accuracy: 96.8750%\n",
      "**  Step 7885, val loss = 0.33, val accuracy = 87.50%  **\n",
      "Step: 7890, loss: 0.1167, accuracy: 96.8750%\n",
      "**  Step 7890, val loss = 0.65, val accuracy = 82.81%  **\n",
      "Step: 7895, loss: 0.1285, accuracy: 93.7500%\n",
      "**  Step 7895, val loss = 0.99, val accuracy = 75.00%  **\n",
      "Step: 7900, loss: 0.1294, accuracy: 98.4375%\n",
      "**  Step 7900, val loss = 0.31, val accuracy = 87.50%  **\n",
      "Step: 7905, loss: 0.1301, accuracy: 96.8750%\n",
      "**  Step 7905, val loss = 0.40, val accuracy = 90.62%  **\n",
      "Step: 7910, loss: 0.0970, accuracy: 98.4375%\n",
      "**  Step 7910, val loss = 0.31, val accuracy = 92.19%  **\n",
      "Step: 7915, loss: 0.1372, accuracy: 95.3125%\n",
      "**  Step 7915, val loss = 0.40, val accuracy = 90.62%  **\n",
      "Step: 7920, loss: 0.0401, accuracy: 100.0000%\n",
      "**  Step 7920, val loss = 0.49, val accuracy = 87.50%  **\n",
      "Step: 7925, loss: 0.1551, accuracy: 92.1875%\n",
      "**  Step 7925, val loss = 0.35, val accuracy = 95.31%  **\n",
      "Step: 7930, loss: 0.0415, accuracy: 100.0000%\n",
      "**  Step 7930, val loss = 0.92, val accuracy = 82.81%  **\n",
      "Step: 7935, loss: 0.1002, accuracy: 98.4375%\n",
      "**  Step 7935, val loss = 0.34, val accuracy = 93.75%  **\n",
      "Step: 7940, loss: 0.0622, accuracy: 100.0000%\n",
      "**  Step 7940, val loss = 0.48, val accuracy = 85.94%  **\n",
      "Step: 7945, loss: 0.0493, accuracy: 98.4375%\n",
      "**  Step 7945, val loss = 0.50, val accuracy = 82.81%  **\n",
      "Step: 7950, loss: 0.0896, accuracy: 96.8750%\n",
      "**  Step 7950, val loss = 0.33, val accuracy = 84.38%  **\n",
      "Step: 7955, loss: 0.1025, accuracy: 96.8750%\n",
      "**  Step 7955, val loss = 0.75, val accuracy = 84.38%  **\n",
      "Step: 7960, loss: 0.0633, accuracy: 96.8750%\n",
      "**  Step 7960, val loss = 0.56, val accuracy = 82.81%  **\n",
      "Step: 7965, loss: 0.0671, accuracy: 98.4375%\n",
      "**  Step 7965, val loss = 0.88, val accuracy = 82.81%  **\n",
      "Step: 7970, loss: 0.0506, accuracy: 100.0000%\n",
      "**  Step 7970, val loss = 0.44, val accuracy = 87.50%  **\n",
      "Step: 7975, loss: 0.0673, accuracy: 98.4375%\n",
      "**  Step 7975, val loss = 0.42, val accuracy = 89.06%  **\n",
      "Step: 7980, loss: 0.0735, accuracy: 98.4375%\n",
      "**  Step 7980, val loss = 0.30, val accuracy = 87.50%  **\n",
      "Step: 7985, loss: 0.0818, accuracy: 96.8750%\n",
      "**  Step 7985, val loss = 0.61, val accuracy = 90.62%  **\n",
      "Step: 7990, loss: 0.1037, accuracy: 96.8750%\n",
      "**  Step 7990, val loss = 0.40, val accuracy = 89.06%  **\n",
      "Step: 7995, loss: 0.0544, accuracy: 100.0000%\n",
      "**  Step 7995, val loss = 0.45, val accuracy = 87.50%  **\n",
      "Step: 8000, loss: 0.0483, accuracy: 100.0000%\n",
      "**  Step 8000, val loss = 0.65, val accuracy = 79.69%  **\n",
      "Step: 8005, loss: 0.0764, accuracy: 96.8750%\n",
      "**  Step 8005, val loss = 0.33, val accuracy = 89.06%  **\n",
      "Step: 8010, loss: 0.0755, accuracy: 98.4375%\n",
      "**  Step 8010, val loss = 0.57, val accuracy = 87.50%  **\n",
      "Step: 8015, loss: 0.0734, accuracy: 100.0000%\n",
      "**  Step 8015, val loss = 0.69, val accuracy = 82.81%  **\n",
      "Step: 8020, loss: 0.1064, accuracy: 95.3125%\n",
      "**  Step 8020, val loss = 0.44, val accuracy = 84.38%  **\n",
      "Step: 8025, loss: 0.2042, accuracy: 92.1875%\n",
      "**  Step 8025, val loss = 0.57, val accuracy = 84.38%  **\n",
      "Step: 8030, loss: 0.0480, accuracy: 98.4375%\n",
      "**  Step 8030, val loss = 0.28, val accuracy = 87.50%  **\n",
      "Step: 8035, loss: 0.1243, accuracy: 93.7500%\n",
      "**  Step 8035, val loss = 0.43, val accuracy = 85.94%  **\n",
      "Step: 8040, loss: 0.1172, accuracy: 95.3125%\n",
      "**  Step 8040, val loss = 0.51, val accuracy = 85.94%  **\n",
      "Step: 8045, loss: 0.0938, accuracy: 98.4375%\n",
      "**  Step 8045, val loss = 0.34, val accuracy = 93.75%  **\n",
      "Step: 8050, loss: 0.0766, accuracy: 98.4375%\n",
      "**  Step 8050, val loss = 0.90, val accuracy = 87.50%  **\n",
      "Step: 8055, loss: 0.1239, accuracy: 95.3125%\n",
      "**  Step 8055, val loss = 1.04, val accuracy = 79.69%  **\n",
      "Step: 8060, loss: 0.0663, accuracy: 98.4375%\n",
      "**  Step 8060, val loss = 0.61, val accuracy = 85.94%  **\n",
      "Step: 8065, loss: 0.0718, accuracy: 100.0000%\n",
      "**  Step 8065, val loss = 0.30, val accuracy = 93.75%  **\n",
      "Step: 8070, loss: 0.1443, accuracy: 96.8750%\n",
      "**  Step 8070, val loss = 0.40, val accuracy = 89.06%  **\n",
      "Step: 8075, loss: 0.1004, accuracy: 93.7500%\n",
      "**  Step 8075, val loss = 0.60, val accuracy = 82.81%  **\n",
      "Step: 8080, loss: 0.0475, accuracy: 100.0000%\n",
      "**  Step 8080, val loss = 0.28, val accuracy = 92.19%  **\n",
      "Step: 8085, loss: 0.0430, accuracy: 98.4375%\n",
      "**  Step 8085, val loss = 0.53, val accuracy = 82.81%  **\n",
      "Step: 8090, loss: 0.1271, accuracy: 98.4375%\n",
      "**  Step 8090, val loss = 0.53, val accuracy = 87.50%  **\n",
      "Step: 8095, loss: 0.0705, accuracy: 100.0000%\n",
      "**  Step 8095, val loss = 0.47, val accuracy = 93.75%  **\n",
      "Step: 8100, loss: 0.1302, accuracy: 95.3125%\n",
      "**  Step 8100, val loss = 0.60, val accuracy = 81.25%  **\n",
      "Step: 8105, loss: 0.0789, accuracy: 96.8750%\n",
      "**  Step 8105, val loss = 0.50, val accuracy = 90.62%  **\n",
      "Step: 8110, loss: 0.0686, accuracy: 96.8750%\n",
      "**  Step 8110, val loss = 0.84, val accuracy = 78.12%  **\n",
      "Step: 8115, loss: 0.1077, accuracy: 96.8750%\n",
      "**  Step 8115, val loss = 0.67, val accuracy = 79.69%  **\n",
      "Step: 8120, loss: 0.1302, accuracy: 95.3125%\n",
      "**  Step 8120, val loss = 0.81, val accuracy = 79.69%  **\n",
      "Step: 8125, loss: 0.0744, accuracy: 98.4375%\n",
      "**  Step 8125, val loss = 0.50, val accuracy = 87.50%  **\n",
      "Step: 8130, loss: 0.0488, accuracy: 100.0000%\n",
      "**  Step 8130, val loss = 0.66, val accuracy = 85.94%  **\n",
      "Step: 8135, loss: 0.1152, accuracy: 95.3125%\n",
      "**  Step 8135, val loss = 0.53, val accuracy = 82.81%  **\n",
      "Step: 8140, loss: 0.1077, accuracy: 95.3125%\n",
      "**  Step 8140, val loss = 0.78, val accuracy = 81.25%  **\n",
      "Step: 8145, loss: 0.0995, accuracy: 96.8750%\n",
      "**  Step 8145, val loss = 0.26, val accuracy = 92.19%  **\n",
      "Step: 8150, loss: 0.0730, accuracy: 98.4375%\n",
      "**  Step 8150, val loss = 0.77, val accuracy = 82.81%  **\n",
      "Step: 8155, loss: 0.0717, accuracy: 98.4375%\n",
      "**  Step 8155, val loss = 0.42, val accuracy = 82.81%  **\n",
      "Step: 8160, loss: 0.0392, accuracy: 100.0000%\n",
      "**  Step 8160, val loss = 0.68, val accuracy = 79.69%  **\n",
      "Step: 8165, loss: 0.0976, accuracy: 96.8750%\n",
      "**  Step 8165, val loss = 0.61, val accuracy = 79.69%  **\n",
      "Step: 8170, loss: 0.1071, accuracy: 96.8750%\n",
      "**  Step 8170, val loss = 0.87, val accuracy = 84.38%  **\n",
      "Step: 8175, loss: 0.0979, accuracy: 96.8750%\n",
      "**  Step 8175, val loss = 0.46, val accuracy = 92.19%  **\n",
      "Step: 8180, loss: 0.0858, accuracy: 96.8750%\n",
      "**  Step 8180, val loss = 0.77, val accuracy = 79.69%  **\n",
      "Step: 8185, loss: 0.1065, accuracy: 98.4375%\n",
      "**  Step 8185, val loss = 0.51, val accuracy = 84.38%  **\n",
      "Step: 8190, loss: 0.1652, accuracy: 93.7500%\n",
      "**  Step 8190, val loss = 0.25, val accuracy = 95.31%  **\n",
      "Step: 8195, loss: 0.0941, accuracy: 96.8750%\n",
      "**  Step 8195, val loss = 0.32, val accuracy = 92.19%  **\n",
      "Step: 8200, loss: 0.0462, accuracy: 98.4375%\n",
      "**  Step 8200, val loss = 0.70, val accuracy = 81.25%  **\n",
      "Step: 8205, loss: 0.1514, accuracy: 93.7500%\n",
      "**  Step 8205, val loss = 1.28, val accuracy = 82.81%  **\n",
      "Step: 8210, loss: 0.1136, accuracy: 96.8750%\n",
      "**  Step 8210, val loss = 0.80, val accuracy = 75.00%  **\n",
      "Step: 8215, loss: 0.0928, accuracy: 98.4375%\n",
      "**  Step 8215, val loss = 0.58, val accuracy = 84.38%  **\n",
      "Step: 8220, loss: 0.0823, accuracy: 98.4375%\n",
      "**  Step 8220, val loss = 0.54, val accuracy = 85.94%  **\n",
      "Step: 8225, loss: 0.1308, accuracy: 93.7500%\n",
      "**  Step 8225, val loss = 0.35, val accuracy = 92.19%  **\n",
      "Step: 8230, loss: 0.0992, accuracy: 96.8750%\n",
      "**  Step 8230, val loss = 1.27, val accuracy = 81.25%  **\n",
      "Step: 8235, loss: 0.0630, accuracy: 96.8750%\n",
      "**  Step 8235, val loss = 0.20, val accuracy = 92.19%  **\n",
      "Step: 8240, loss: 0.0843, accuracy: 96.8750%\n",
      "**  Step 8240, val loss = 0.95, val accuracy = 78.12%  **\n",
      "Step: 8245, loss: 0.0781, accuracy: 96.8750%\n",
      "**  Step 8245, val loss = 0.45, val accuracy = 87.50%  **\n",
      "Step: 8250, loss: 0.0984, accuracy: 96.8750%\n",
      "**  Step 8250, val loss = 0.66, val accuracy = 82.81%  **\n",
      "Step: 8255, loss: 0.0765, accuracy: 98.4375%\n",
      "**  Step 8255, val loss = 0.82, val accuracy = 82.81%  **\n",
      "Step: 8260, loss: 0.0723, accuracy: 100.0000%\n",
      "**  Step 8260, val loss = 0.39, val accuracy = 84.38%  **\n",
      "Step: 8265, loss: 0.1127, accuracy: 98.4375%\n",
      "**  Step 8265, val loss = 0.31, val accuracy = 92.19%  **\n",
      "Step: 8270, loss: 0.1442, accuracy: 95.3125%\n",
      "**  Step 8270, val loss = 0.71, val accuracy = 90.62%  **\n",
      "Step: 8275, loss: 0.1717, accuracy: 93.7500%\n",
      "**  Step 8275, val loss = 0.37, val accuracy = 89.06%  **\n",
      "Step: 8280, loss: 0.0574, accuracy: 100.0000%\n",
      "**  Step 8280, val loss = 0.27, val accuracy = 90.62%  **\n",
      "Step: 8285, loss: 0.1301, accuracy: 95.3125%\n",
      "**  Step 8285, val loss = 0.26, val accuracy = 92.19%  **\n",
      "Step: 8290, loss: 0.0847, accuracy: 98.4375%\n",
      "**  Step 8290, val loss = 0.67, val accuracy = 81.25%  **\n",
      "Step: 8295, loss: 0.0620, accuracy: 98.4375%\n",
      "**  Step 8295, val loss = 0.60, val accuracy = 81.25%  **\n",
      "Step: 8300, loss: 0.0296, accuracy: 100.0000%\n",
      "**  Step 8300, val loss = 0.41, val accuracy = 92.19%  **\n",
      "Step: 8305, loss: 0.0568, accuracy: 98.4375%\n",
      "**  Step 8305, val loss = 0.80, val accuracy = 82.81%  **\n",
      "Step: 8310, loss: 0.1206, accuracy: 95.3125%\n",
      "**  Step 8310, val loss = 0.43, val accuracy = 90.62%  **\n",
      "Step: 8315, loss: 0.0749, accuracy: 98.4375%\n",
      "**  Step 8315, val loss = 0.71, val accuracy = 79.69%  **\n",
      "Step: 8320, loss: 0.1123, accuracy: 98.4375%\n",
      "**  Step 8320, val loss = 0.42, val accuracy = 87.50%  **\n",
      "Step: 8325, loss: 0.1068, accuracy: 96.8750%\n",
      "**  Step 8325, val loss = 0.55, val accuracy = 82.81%  **\n",
      "Step: 8330, loss: 0.1166, accuracy: 96.8750%\n",
      "**  Step 8330, val loss = 0.47, val accuracy = 92.19%  **\n",
      "Step: 8335, loss: 0.0602, accuracy: 96.8750%\n",
      "**  Step 8335, val loss = 0.48, val accuracy = 75.00%  **\n",
      "Step: 8340, loss: 0.0313, accuracy: 100.0000%\n",
      "**  Step 8340, val loss = 0.66, val accuracy = 87.50%  **\n",
      "Step: 8345, loss: 0.0520, accuracy: 100.0000%\n",
      "**  Step 8345, val loss = 0.45, val accuracy = 84.38%  **\n",
      "Step: 8350, loss: 0.0382, accuracy: 98.4375%\n",
      "**  Step 8350, val loss = 0.45, val accuracy = 87.50%  **\n",
      "Step: 8355, loss: 0.0863, accuracy: 98.4375%\n",
      "**  Step 8355, val loss = 0.43, val accuracy = 85.94%  **\n",
      "Step: 8360, loss: 0.1408, accuracy: 93.7500%\n",
      "**  Step 8360, val loss = 0.36, val accuracy = 90.62%  **\n",
      "Step: 8365, loss: 0.0466, accuracy: 98.4375%\n",
      "**  Step 8365, val loss = 0.51, val accuracy = 82.81%  **\n",
      "Step: 8370, loss: 0.0502, accuracy: 100.0000%\n",
      "**  Step 8370, val loss = 0.73, val accuracy = 79.69%  **\n",
      "Step: 8375, loss: 0.0927, accuracy: 98.4375%\n",
      "**  Step 8375, val loss = 0.32, val accuracy = 87.50%  **\n",
      "Step: 8380, loss: 0.0199, accuracy: 100.0000%\n",
      "**  Step 8380, val loss = 0.42, val accuracy = 92.19%  **\n",
      "Step: 8385, loss: 0.0666, accuracy: 98.4375%\n",
      "**  Step 8385, val loss = 0.72, val accuracy = 87.50%  **\n",
      "Step: 8390, loss: 0.0891, accuracy: 96.8750%\n",
      "**  Step 8390, val loss = 0.37, val accuracy = 89.06%  **\n",
      "Step: 8395, loss: 0.0360, accuracy: 100.0000%\n",
      "**  Step 8395, val loss = 0.47, val accuracy = 87.50%  **\n",
      "Step: 8400, loss: 0.0626, accuracy: 100.0000%\n",
      "**  Step 8400, val loss = 0.46, val accuracy = 85.94%  **\n",
      "Step: 8405, loss: 0.0977, accuracy: 96.8750%\n",
      "**  Step 8405, val loss = 0.68, val accuracy = 75.00%  **\n",
      "Step: 8410, loss: 0.1455, accuracy: 92.1875%\n",
      "**  Step 8410, val loss = 0.84, val accuracy = 78.12%  **\n",
      "Step: 8415, loss: 0.0775, accuracy: 96.8750%\n",
      "**  Step 8415, val loss = 0.85, val accuracy = 79.69%  **\n",
      "Step: 8420, loss: 0.0933, accuracy: 96.8750%\n",
      "**  Step 8420, val loss = 0.76, val accuracy = 85.94%  **\n",
      "Step: 8425, loss: 0.0698, accuracy: 98.4375%\n",
      "**  Step 8425, val loss = 0.34, val accuracy = 85.94%  **\n",
      "Step: 8430, loss: 0.1303, accuracy: 93.7500%\n",
      "**  Step 8430, val loss = 1.03, val accuracy = 85.94%  **\n",
      "Step: 8435, loss: 0.0952, accuracy: 96.8750%\n",
      "**  Step 8435, val loss = 0.39, val accuracy = 90.62%  **\n",
      "Step: 8440, loss: 0.0954, accuracy: 95.3125%\n",
      "**  Step 8440, val loss = 0.41, val accuracy = 87.50%  **\n",
      "Step: 8445, loss: 0.1010, accuracy: 95.3125%\n",
      "**  Step 8445, val loss = 0.60, val accuracy = 78.12%  **\n",
      "Step: 8450, loss: 0.0836, accuracy: 96.8750%\n",
      "**  Step 8450, val loss = 0.85, val accuracy = 81.25%  **\n",
      "Step: 8455, loss: 0.0772, accuracy: 98.4375%\n",
      "**  Step 8455, val loss = 0.61, val accuracy = 79.69%  **\n",
      "Step: 8460, loss: 0.0707, accuracy: 96.8750%\n",
      "**  Step 8460, val loss = 0.65, val accuracy = 87.50%  **\n",
      "Step: 8465, loss: 0.0717, accuracy: 98.4375%\n",
      "**  Step 8465, val loss = 0.51, val accuracy = 85.94%  **\n",
      "Step: 8470, loss: 0.0549, accuracy: 98.4375%\n",
      "**  Step 8470, val loss = 0.62, val accuracy = 79.69%  **\n",
      "Step: 8475, loss: 0.0596, accuracy: 98.4375%\n",
      "**  Step 8475, val loss = 0.37, val accuracy = 89.06%  **\n",
      "Step: 8480, loss: 0.1632, accuracy: 93.7500%\n",
      "**  Step 8480, val loss = 0.61, val accuracy = 81.25%  **\n",
      "Step: 8485, loss: 0.0480, accuracy: 98.4375%\n",
      "**  Step 8485, val loss = 0.41, val accuracy = 87.50%  **\n",
      "Step: 8490, loss: 0.1190, accuracy: 98.4375%\n",
      "**  Step 8490, val loss = 0.57, val accuracy = 78.12%  **\n",
      "Step: 8495, loss: 0.0880, accuracy: 96.8750%\n",
      "**  Step 8495, val loss = 0.43, val accuracy = 89.06%  **\n",
      "Step: 8500, loss: 0.0607, accuracy: 98.4375%\n",
      "**  Step 8500, val loss = 0.73, val accuracy = 84.38%  **\n",
      "Step: 8505, loss: 0.0925, accuracy: 96.8750%\n",
      "**  Step 8505, val loss = 0.66, val accuracy = 87.50%  **\n",
      "Step: 8510, loss: 0.0992, accuracy: 96.8750%\n",
      "**  Step 8510, val loss = 0.59, val accuracy = 79.69%  **\n",
      "Step: 8515, loss: 0.0316, accuracy: 100.0000%\n",
      "**  Step 8515, val loss = 0.34, val accuracy = 92.19%  **\n",
      "Step: 8520, loss: 0.0800, accuracy: 98.4375%\n",
      "**  Step 8520, val loss = 0.58, val accuracy = 84.38%  **\n",
      "Step: 8525, loss: 0.0628, accuracy: 98.4375%\n",
      "**  Step 8525, val loss = 0.54, val accuracy = 92.19%  **\n",
      "Step: 8530, loss: 0.1231, accuracy: 95.3125%\n",
      "**  Step 8530, val loss = 0.54, val accuracy = 89.06%  **\n",
      "Step: 8535, loss: 0.0745, accuracy: 95.3125%\n",
      "**  Step 8535, val loss = 0.72, val accuracy = 84.38%  **\n",
      "Step: 8540, loss: 0.1146, accuracy: 96.8750%\n",
      "**  Step 8540, val loss = 1.06, val accuracy = 81.25%  **\n",
      "Step: 8545, loss: 0.1420, accuracy: 93.7500%\n",
      "**  Step 8545, val loss = 0.75, val accuracy = 81.25%  **\n",
      "Step: 8550, loss: 0.0375, accuracy: 100.0000%\n",
      "**  Step 8550, val loss = 0.67, val accuracy = 87.50%  **\n",
      "Step: 8555, loss: 0.0831, accuracy: 96.8750%\n",
      "**  Step 8555, val loss = 0.88, val accuracy = 84.38%  **\n",
      "Step: 8560, loss: 0.0626, accuracy: 100.0000%\n",
      "**  Step 8560, val loss = 0.39, val accuracy = 90.62%  **\n",
      "Step: 8565, loss: 0.0766, accuracy: 95.3125%\n",
      "**  Step 8565, val loss = 0.72, val accuracy = 79.69%  **\n",
      "Step: 8570, loss: 0.0879, accuracy: 96.8750%\n",
      "**  Step 8570, val loss = 0.61, val accuracy = 79.69%  **\n",
      "Step: 8575, loss: 0.1527, accuracy: 95.3125%\n",
      "**  Step 8575, val loss = 0.72, val accuracy = 78.12%  **\n",
      "Step: 8580, loss: 0.0767, accuracy: 98.4375%\n",
      "**  Step 8580, val loss = 0.39, val accuracy = 89.06%  **\n",
      "Step: 8585, loss: 0.0460, accuracy: 98.4375%\n",
      "**  Step 8585, val loss = 0.55, val accuracy = 85.94%  **\n",
      "Step: 8590, loss: 0.1140, accuracy: 96.8750%\n",
      "**  Step 8590, val loss = 0.69, val accuracy = 81.25%  **\n",
      "Step: 8595, loss: 0.1126, accuracy: 95.3125%\n",
      "**  Step 8595, val loss = 0.82, val accuracy = 84.38%  **\n",
      "Step: 8600, loss: 0.1034, accuracy: 95.3125%\n",
      "**  Step 8600, val loss = 0.91, val accuracy = 81.25%  **\n",
      "Step: 8605, loss: 0.0759, accuracy: 98.4375%\n",
      "**  Step 8605, val loss = 0.56, val accuracy = 89.06%  **\n",
      "Step: 8610, loss: 0.0806, accuracy: 98.4375%\n",
      "**  Step 8610, val loss = 0.66, val accuracy = 81.25%  **\n",
      "Step: 8615, loss: 0.1201, accuracy: 96.8750%\n",
      "**  Step 8615, val loss = 0.65, val accuracy = 82.81%  **\n",
      "Step: 8620, loss: 0.1063, accuracy: 93.7500%\n",
      "**  Step 8620, val loss = 0.49, val accuracy = 89.06%  **\n",
      "Step: 8625, loss: 0.0934, accuracy: 95.3125%\n",
      "**  Step 8625, val loss = 0.55, val accuracy = 81.25%  **\n",
      "Step: 8630, loss: 0.0605, accuracy: 100.0000%\n",
      "**  Step 8630, val loss = 0.23, val accuracy = 93.75%  **\n",
      "Step: 8635, loss: 0.0758, accuracy: 98.4375%\n",
      "**  Step 8635, val loss = 0.64, val accuracy = 82.81%  **\n",
      "Step: 8640, loss: 0.0626, accuracy: 98.4375%\n",
      "**  Step 8640, val loss = 0.92, val accuracy = 73.44%  **\n",
      "Step: 8645, loss: 0.0583, accuracy: 98.4375%\n",
      "**  Step 8645, val loss = 0.28, val accuracy = 92.19%  **\n",
      "Step: 8650, loss: 0.1005, accuracy: 96.8750%\n",
      "**  Step 8650, val loss = 0.97, val accuracy = 75.00%  **\n",
      "Step: 8655, loss: 0.0600, accuracy: 98.4375%\n",
      "**  Step 8655, val loss = 0.43, val accuracy = 89.06%  **\n",
      "Step: 8660, loss: 0.0805, accuracy: 95.3125%\n",
      "**  Step 8660, val loss = 0.77, val accuracy = 84.38%  **\n",
      "Step: 8665, loss: 0.0361, accuracy: 98.4375%\n",
      "**  Step 8665, val loss = 0.52, val accuracy = 85.94%  **\n",
      "Step: 8670, loss: 0.0662, accuracy: 98.4375%\n",
      "**  Step 8670, val loss = 0.59, val accuracy = 84.38%  **\n",
      "Step: 8675, loss: 0.0411, accuracy: 100.0000%\n",
      "**  Step 8675, val loss = 0.18, val accuracy = 93.75%  **\n",
      "Step: 8680, loss: 0.0733, accuracy: 96.8750%\n",
      "**  Step 8680, val loss = 0.82, val accuracy = 82.81%  **\n",
      "Step: 8685, loss: 0.0463, accuracy: 98.4375%\n",
      "**  Step 8685, val loss = 0.88, val accuracy = 85.94%  **\n",
      "Step: 8690, loss: 0.0726, accuracy: 100.0000%\n",
      "**  Step 8690, val loss = 0.82, val accuracy = 87.50%  **\n",
      "Step: 8695, loss: 0.0576, accuracy: 100.0000%\n",
      "**  Step 8695, val loss = 0.83, val accuracy = 81.25%  **\n",
      "Step: 8700, loss: 0.1171, accuracy: 95.3125%\n",
      "**  Step 8700, val loss = 0.52, val accuracy = 87.50%  **\n",
      "Step: 8705, loss: 0.0667, accuracy: 96.8750%\n",
      "**  Step 8705, val loss = 0.48, val accuracy = 84.38%  **\n",
      "Step: 8710, loss: 0.1213, accuracy: 96.8750%\n",
      "**  Step 8710, val loss = 0.43, val accuracy = 85.94%  **\n",
      "Step: 8715, loss: 0.0441, accuracy: 98.4375%\n",
      "**  Step 8715, val loss = 0.44, val accuracy = 89.06%  **\n",
      "Step: 8720, loss: 0.0594, accuracy: 100.0000%\n",
      "**  Step 8720, val loss = 0.48, val accuracy = 82.81%  **\n",
      "Step: 8725, loss: 0.0950, accuracy: 96.8750%\n",
      "**  Step 8725, val loss = 0.66, val accuracy = 89.06%  **\n",
      "Step: 8730, loss: 0.0676, accuracy: 98.4375%\n",
      "**  Step 8730, val loss = 0.60, val accuracy = 82.81%  **\n",
      "Step: 8735, loss: 0.0651, accuracy: 98.4375%\n",
      "**  Step 8735, val loss = 0.28, val accuracy = 89.06%  **\n",
      "Step: 8740, loss: 0.0892, accuracy: 98.4375%\n",
      "**  Step 8740, val loss = 0.62, val accuracy = 84.38%  **\n",
      "Step: 8745, loss: 0.1053, accuracy: 96.8750%\n",
      "**  Step 8745, val loss = 0.71, val accuracy = 84.38%  **\n",
      "Step: 8750, loss: 0.0789, accuracy: 96.8750%\n",
      "**  Step 8750, val loss = 0.75, val accuracy = 78.12%  **\n",
      "Step: 8755, loss: 0.0324, accuracy: 100.0000%\n",
      "**  Step 8755, val loss = 0.58, val accuracy = 76.56%  **\n",
      "Step: 8760, loss: 0.0693, accuracy: 98.4375%\n",
      "**  Step 8760, val loss = 0.74, val accuracy = 87.50%  **\n",
      "Step: 8765, loss: 0.0834, accuracy: 98.4375%\n",
      "**  Step 8765, val loss = 1.44, val accuracy = 84.38%  **\n",
      "Step: 8770, loss: 0.0787, accuracy: 96.8750%\n",
      "**  Step 8770, val loss = 0.53, val accuracy = 84.38%  **\n",
      "Step: 8775, loss: 0.0513, accuracy: 100.0000%\n",
      "**  Step 8775, val loss = 0.52, val accuracy = 85.94%  **\n",
      "Step: 8780, loss: 0.0904, accuracy: 96.8750%\n",
      "**  Step 8780, val loss = 0.45, val accuracy = 82.81%  **\n",
      "Step: 8785, loss: 0.1251, accuracy: 95.3125%\n",
      "**  Step 8785, val loss = 0.49, val accuracy = 89.06%  **\n",
      "Step: 8790, loss: 0.0685, accuracy: 98.4375%\n",
      "**  Step 8790, val loss = 0.50, val accuracy = 85.94%  **\n",
      "Step: 8795, loss: 0.1591, accuracy: 93.7500%\n",
      "**  Step 8795, val loss = 0.91, val accuracy = 79.69%  **\n",
      "Step: 8800, loss: 0.0773, accuracy: 100.0000%\n",
      "**  Step 8800, val loss = 0.58, val accuracy = 87.50%  **\n",
      "Step: 8805, loss: 0.0617, accuracy: 100.0000%\n",
      "**  Step 8805, val loss = 0.80, val accuracy = 84.38%  **\n",
      "Step: 8810, loss: 0.0865, accuracy: 96.8750%\n",
      "**  Step 8810, val loss = 0.32, val accuracy = 89.06%  **\n",
      "Step: 8815, loss: 0.0540, accuracy: 98.4375%\n",
      "**  Step 8815, val loss = 0.57, val accuracy = 89.06%  **\n",
      "Step: 8820, loss: 0.0430, accuracy: 100.0000%\n",
      "**  Step 8820, val loss = 0.44, val accuracy = 84.38%  **\n",
      "Step: 8825, loss: 0.0782, accuracy: 96.8750%\n",
      "**  Step 8825, val loss = 1.01, val accuracy = 75.00%  **\n",
      "Step: 8830, loss: 0.1244, accuracy: 95.3125%\n",
      "**  Step 8830, val loss = 0.34, val accuracy = 89.06%  **\n",
      "Step: 8835, loss: 0.0250, accuracy: 100.0000%\n",
      "**  Step 8835, val loss = 0.56, val accuracy = 82.81%  **\n",
      "Step: 8840, loss: 0.0782, accuracy: 96.8750%\n",
      "**  Step 8840, val loss = 0.46, val accuracy = 82.81%  **\n",
      "Step: 8845, loss: 0.0492, accuracy: 100.0000%\n",
      "**  Step 8845, val loss = 0.48, val accuracy = 84.38%  **\n",
      "Step: 8850, loss: 0.0628, accuracy: 100.0000%\n",
      "**  Step 8850, val loss = 0.41, val accuracy = 85.94%  **\n",
      "Step: 8855, loss: 0.0532, accuracy: 100.0000%\n",
      "**  Step 8855, val loss = 0.42, val accuracy = 85.94%  **\n",
      "Step: 8860, loss: 0.0429, accuracy: 100.0000%\n",
      "**  Step 8860, val loss = 0.59, val accuracy = 82.81%  **\n",
      "Step: 8865, loss: 0.0747, accuracy: 98.4375%\n",
      "**  Step 8865, val loss = 0.26, val accuracy = 93.75%  **\n",
      "Step: 8870, loss: 0.0343, accuracy: 100.0000%\n",
      "**  Step 8870, val loss = 0.63, val accuracy = 79.69%  **\n",
      "Step: 8875, loss: 0.0371, accuracy: 100.0000%\n",
      "**  Step 8875, val loss = 0.58, val accuracy = 90.62%  **\n",
      "Step: 8880, loss: 0.0914, accuracy: 96.8750%\n",
      "**  Step 8880, val loss = 0.39, val accuracy = 85.94%  **\n",
      "Step: 8885, loss: 0.0633, accuracy: 100.0000%\n",
      "**  Step 8885, val loss = 0.48, val accuracy = 87.50%  **\n",
      "Step: 8890, loss: 0.0725, accuracy: 100.0000%\n",
      "**  Step 8890, val loss = 0.45, val accuracy = 90.62%  **\n",
      "Step: 8895, loss: 0.0722, accuracy: 98.4375%\n",
      "**  Step 8895, val loss = 0.60, val accuracy = 85.94%  **\n",
      "Step: 8900, loss: 0.1279, accuracy: 95.3125%\n",
      "**  Step 8900, val loss = 0.42, val accuracy = 89.06%  **\n",
      "Step: 8905, loss: 0.1052, accuracy: 98.4375%\n",
      "**  Step 8905, val loss = 0.60, val accuracy = 89.06%  **\n",
      "Step: 8910, loss: 0.0476, accuracy: 98.4375%\n",
      "**  Step 8910, val loss = 0.65, val accuracy = 87.50%  **\n",
      "Step: 8915, loss: 0.0492, accuracy: 100.0000%\n",
      "**  Step 8915, val loss = 0.55, val accuracy = 84.38%  **\n",
      "Step: 8920, loss: 0.0387, accuracy: 100.0000%\n",
      "**  Step 8920, val loss = 0.39, val accuracy = 87.50%  **\n",
      "Step: 8925, loss: 0.0728, accuracy: 96.8750%\n",
      "**  Step 8925, val loss = 0.50, val accuracy = 87.50%  **\n",
      "Step: 8930, loss: 0.1126, accuracy: 96.8750%\n",
      "**  Step 8930, val loss = 0.92, val accuracy = 82.81%  **\n",
      "Step: 8935, loss: 0.0930, accuracy: 96.8750%\n",
      "**  Step 8935, val loss = 0.81, val accuracy = 85.94%  **\n",
      "Step: 8940, loss: 0.0362, accuracy: 98.4375%\n",
      "**  Step 8940, val loss = 0.73, val accuracy = 82.81%  **\n",
      "Step: 8945, loss: 0.0765, accuracy: 98.4375%\n",
      "**  Step 8945, val loss = 0.98, val accuracy = 82.81%  **\n",
      "Step: 8950, loss: 0.0720, accuracy: 98.4375%\n",
      "**  Step 8950, val loss = 0.43, val accuracy = 92.19%  **\n",
      "Step: 8955, loss: 0.0668, accuracy: 100.0000%\n",
      "**  Step 8955, val loss = 0.84, val accuracy = 81.25%  **\n",
      "Step: 8960, loss: 0.0362, accuracy: 100.0000%\n",
      "**  Step 8960, val loss = 0.91, val accuracy = 76.56%  **\n",
      "Step: 8965, loss: 0.0788, accuracy: 96.8750%\n",
      "**  Step 8965, val loss = 0.37, val accuracy = 85.94%  **\n",
      "Step: 8970, loss: 0.1020, accuracy: 98.4375%\n",
      "**  Step 8970, val loss = 0.33, val accuracy = 89.06%  **\n",
      "Step: 8975, loss: 0.0609, accuracy: 96.8750%\n",
      "**  Step 8975, val loss = 0.64, val accuracy = 87.50%  **\n",
      "Step: 8980, loss: 0.1006, accuracy: 96.8750%\n",
      "**  Step 8980, val loss = 0.37, val accuracy = 89.06%  **\n",
      "Step: 8985, loss: 0.1065, accuracy: 98.4375%\n",
      "**  Step 8985, val loss = 0.56, val accuracy = 85.94%  **\n",
      "Step: 8990, loss: 0.0539, accuracy: 98.4375%\n",
      "**  Step 8990, val loss = 0.82, val accuracy = 79.69%  **\n",
      "Step: 8995, loss: 0.0627, accuracy: 100.0000%\n",
      "**  Step 8995, val loss = 0.54, val accuracy = 85.94%  **\n",
      "Step: 9000, loss: 0.0705, accuracy: 95.3125%\n",
      "**  Step 9000, val loss = 0.63, val accuracy = 84.38%  **\n",
      "Step: 9005, loss: 0.0348, accuracy: 100.0000%\n",
      "**  Step 9005, val loss = 0.79, val accuracy = 85.94%  **\n",
      "Step: 9010, loss: 0.0343, accuracy: 100.0000%\n",
      "**  Step 9010, val loss = 0.85, val accuracy = 81.25%  **\n",
      "Step: 9015, loss: 0.1055, accuracy: 96.8750%\n",
      "**  Step 9015, val loss = 0.48, val accuracy = 84.38%  **\n",
      "Step: 9020, loss: 0.0820, accuracy: 98.4375%\n",
      "**  Step 9020, val loss = 0.44, val accuracy = 84.38%  **\n",
      "Step: 9025, loss: 0.0660, accuracy: 98.4375%\n",
      "**  Step 9025, val loss = 0.18, val accuracy = 92.19%  **\n",
      "Step: 9030, loss: 0.0869, accuracy: 95.3125%\n",
      "**  Step 9030, val loss = 1.06, val accuracy = 78.12%  **\n",
      "Step: 9035, loss: 0.0353, accuracy: 100.0000%\n",
      "**  Step 9035, val loss = 0.40, val accuracy = 87.50%  **\n",
      "Step: 9040, loss: 0.0879, accuracy: 98.4375%\n",
      "**  Step 9040, val loss = 0.50, val accuracy = 87.50%  **\n",
      "Step: 9045, loss: 0.0844, accuracy: 96.8750%\n",
      "**  Step 9045, val loss = 0.73, val accuracy = 84.38%  **\n",
      "Step: 9050, loss: 0.0761, accuracy: 98.4375%\n",
      "**  Step 9050, val loss = 0.75, val accuracy = 82.81%  **\n",
      "Step: 9055, loss: 0.0381, accuracy: 100.0000%\n",
      "**  Step 9055, val loss = 0.80, val accuracy = 78.12%  **\n",
      "Step: 9060, loss: 0.0556, accuracy: 98.4375%\n",
      "**  Step 9060, val loss = 0.46, val accuracy = 82.81%  **\n",
      "Step: 9065, loss: 0.1134, accuracy: 95.3125%\n",
      "**  Step 9065, val loss = 0.80, val accuracy = 81.25%  **\n",
      "Step: 9070, loss: 0.0384, accuracy: 100.0000%\n",
      "**  Step 9070, val loss = 0.70, val accuracy = 89.06%  **\n",
      "Step: 9075, loss: 0.0728, accuracy: 96.8750%\n",
      "**  Step 9075, val loss = 0.70, val accuracy = 85.94%  **\n",
      "Step: 9080, loss: 0.0231, accuracy: 100.0000%\n",
      "**  Step 9080, val loss = 0.79, val accuracy = 85.94%  **\n",
      "Step: 9085, loss: 0.0666, accuracy: 98.4375%\n",
      "**  Step 9085, val loss = 0.15, val accuracy = 90.62%  **\n",
      "Step: 9090, loss: 0.0637, accuracy: 98.4375%\n",
      "**  Step 9090, val loss = 1.10, val accuracy = 79.69%  **\n",
      "Step: 9095, loss: 0.0293, accuracy: 100.0000%\n",
      "**  Step 9095, val loss = 0.89, val accuracy = 78.12%  **\n",
      "Step: 9100, loss: 0.0747, accuracy: 96.8750%\n",
      "**  Step 9100, val loss = 0.69, val accuracy = 85.94%  **\n",
      "Step: 9105, loss: 0.0988, accuracy: 96.8750%\n",
      "**  Step 9105, val loss = 0.25, val accuracy = 89.06%  **\n",
      "Step: 9110, loss: 0.0650, accuracy: 100.0000%\n",
      "**  Step 9110, val loss = 0.60, val accuracy = 85.94%  **\n",
      "Step: 9115, loss: 0.0729, accuracy: 98.4375%\n",
      "**  Step 9115, val loss = 0.70, val accuracy = 84.38%  **\n",
      "Step: 9120, loss: 0.0417, accuracy: 96.8750%\n",
      "**  Step 9120, val loss = 0.52, val accuracy = 84.38%  **\n",
      "Step: 9125, loss: 0.1177, accuracy: 95.3125%\n",
      "**  Step 9125, val loss = 1.03, val accuracy = 78.12%  **\n",
      "Step: 9130, loss: 0.0415, accuracy: 100.0000%\n",
      "**  Step 9130, val loss = 0.67, val accuracy = 85.94%  **\n",
      "Step: 9135, loss: 0.0895, accuracy: 98.4375%\n",
      "**  Step 9135, val loss = 0.55, val accuracy = 79.69%  **\n",
      "Step: 9140, loss: 0.0634, accuracy: 98.4375%\n",
      "**  Step 9140, val loss = 0.34, val accuracy = 89.06%  **\n",
      "Step: 9145, loss: 0.0591, accuracy: 98.4375%\n",
      "**  Step 9145, val loss = 0.68, val accuracy = 79.69%  **\n",
      "Step: 9150, loss: 0.1171, accuracy: 98.4375%\n",
      "**  Step 9150, val loss = 0.48, val accuracy = 89.06%  **\n",
      "Step: 9155, loss: 0.0618, accuracy: 96.8750%\n",
      "**  Step 9155, val loss = 0.50, val accuracy = 76.56%  **\n",
      "Step: 9160, loss: 0.1142, accuracy: 96.8750%\n",
      "**  Step 9160, val loss = 0.67, val accuracy = 81.25%  **\n",
      "Step: 9165, loss: 0.0125, accuracy: 100.0000%\n",
      "**  Step 9165, val loss = 0.72, val accuracy = 82.81%  **\n",
      "Step: 9170, loss: 0.0841, accuracy: 96.8750%\n",
      "**  Step 9170, val loss = 0.61, val accuracy = 79.69%  **\n",
      "Step: 9175, loss: 0.1294, accuracy: 95.3125%\n",
      "**  Step 9175, val loss = 0.69, val accuracy = 87.50%  **\n",
      "Step: 9180, loss: 0.0780, accuracy: 98.4375%\n",
      "**  Step 9180, val loss = 0.82, val accuracy = 79.69%  **\n",
      "Step: 9185, loss: 0.0483, accuracy: 98.4375%\n",
      "**  Step 9185, val loss = 0.66, val accuracy = 89.06%  **\n",
      "Step: 9190, loss: 0.0893, accuracy: 96.8750%\n",
      "**  Step 9190, val loss = 0.90, val accuracy = 81.25%  **\n",
      "Step: 9195, loss: 0.1127, accuracy: 96.8750%\n",
      "**  Step 9195, val loss = 0.63, val accuracy = 82.81%  **\n",
      "Step: 9200, loss: 0.0488, accuracy: 98.4375%\n",
      "**  Step 9200, val loss = 0.68, val accuracy = 87.50%  **\n",
      "Step: 9205, loss: 0.0416, accuracy: 98.4375%\n",
      "**  Step 9205, val loss = 0.59, val accuracy = 89.06%  **\n",
      "Step: 9210, loss: 0.0336, accuracy: 100.0000%\n",
      "**  Step 9210, val loss = 0.84, val accuracy = 76.56%  **\n",
      "Step: 9215, loss: 0.0939, accuracy: 96.8750%\n",
      "**  Step 9215, val loss = 0.74, val accuracy = 87.50%  **\n",
      "Step: 9220, loss: 0.1637, accuracy: 95.3125%\n",
      "**  Step 9220, val loss = 1.29, val accuracy = 81.25%  **\n",
      "Step: 9225, loss: 0.1106, accuracy: 96.8750%\n",
      "**  Step 9225, val loss = 0.27, val accuracy = 90.62%  **\n",
      "Step: 9230, loss: 0.0621, accuracy: 100.0000%\n",
      "**  Step 9230, val loss = 0.76, val accuracy = 79.69%  **\n",
      "Step: 9235, loss: 0.0823, accuracy: 93.7500%\n",
      "**  Step 9235, val loss = 0.17, val accuracy = 93.75%  **\n",
      "Step: 9240, loss: 0.1139, accuracy: 96.8750%\n",
      "**  Step 9240, val loss = 0.56, val accuracy = 92.19%  **\n",
      "Step: 9245, loss: 0.0635, accuracy: 100.0000%\n",
      "**  Step 9245, val loss = 0.83, val accuracy = 90.62%  **\n",
      "Step: 9250, loss: 0.1068, accuracy: 96.8750%\n",
      "**  Step 9250, val loss = 0.68, val accuracy = 79.69%  **\n",
      "Step: 9255, loss: 0.0334, accuracy: 100.0000%\n",
      "**  Step 9255, val loss = 0.40, val accuracy = 92.19%  **\n",
      "Step: 9260, loss: 0.0499, accuracy: 100.0000%\n",
      "**  Step 9260, val loss = 0.71, val accuracy = 87.50%  **\n",
      "Step: 9265, loss: 0.0834, accuracy: 98.4375%\n",
      "**  Step 9265, val loss = 0.47, val accuracy = 93.75%  **\n",
      "Step: 9270, loss: 0.0499, accuracy: 100.0000%\n",
      "**  Step 9270, val loss = 0.62, val accuracy = 84.38%  **\n",
      "Step: 9275, loss: 0.0292, accuracy: 100.0000%\n",
      "**  Step 9275, val loss = 0.45, val accuracy = 89.06%  **\n",
      "Step: 9280, loss: 0.0711, accuracy: 98.4375%\n",
      "**  Step 9280, val loss = 0.90, val accuracy = 81.25%  **\n",
      "Step: 9285, loss: 0.0614, accuracy: 98.4375%\n",
      "**  Step 9285, val loss = 0.51, val accuracy = 87.50%  **\n",
      "Step: 9290, loss: 0.0756, accuracy: 98.4375%\n",
      "**  Step 9290, val loss = 0.83, val accuracy = 89.06%  **\n",
      "Step: 9295, loss: 0.0505, accuracy: 100.0000%\n",
      "**  Step 9295, val loss = 0.65, val accuracy = 84.38%  **\n",
      "Step: 9300, loss: 0.0973, accuracy: 95.3125%\n",
      "**  Step 9300, val loss = 0.93, val accuracy = 81.25%  **\n",
      "Step: 9305, loss: 0.0450, accuracy: 100.0000%\n",
      "**  Step 9305, val loss = 1.05, val accuracy = 78.12%  **\n",
      "Step: 9310, loss: 0.0656, accuracy: 98.4375%\n",
      "**  Step 9310, val loss = 0.49, val accuracy = 89.06%  **\n",
      "Step: 9315, loss: 0.0201, accuracy: 100.0000%\n",
      "**  Step 9315, val loss = 0.42, val accuracy = 87.50%  **\n",
      "Step: 9320, loss: 0.0313, accuracy: 98.4375%\n",
      "**  Step 9320, val loss = 0.32, val accuracy = 90.62%  **\n",
      "Step: 9325, loss: 0.0615, accuracy: 98.4375%\n",
      "**  Step 9325, val loss = 0.88, val accuracy = 78.12%  **\n",
      "Step: 9330, loss: 0.0771, accuracy: 100.0000%\n",
      "**  Step 9330, val loss = 0.42, val accuracy = 85.94%  **\n",
      "Step: 9335, loss: 0.1102, accuracy: 98.4375%\n",
      "**  Step 9335, val loss = 0.33, val accuracy = 90.62%  **\n",
      "Step: 9340, loss: 0.0759, accuracy: 96.8750%\n",
      "**  Step 9340, val loss = 0.92, val accuracy = 79.69%  **\n",
      "Step: 9345, loss: 0.0661, accuracy: 96.8750%\n",
      "**  Step 9345, val loss = 0.26, val accuracy = 93.75%  **\n",
      "Step: 9350, loss: 0.1305, accuracy: 95.3125%\n",
      "**  Step 9350, val loss = 0.70, val accuracy = 82.81%  **\n",
      "Step: 9355, loss: 0.1115, accuracy: 95.3125%\n",
      "**  Step 9355, val loss = 0.79, val accuracy = 82.81%  **\n",
      "Step: 9360, loss: 0.1454, accuracy: 95.3125%\n",
      "**  Step 9360, val loss = 0.97, val accuracy = 81.25%  **\n",
      "Step: 9365, loss: 0.0532, accuracy: 100.0000%\n",
      "**  Step 9365, val loss = 0.81, val accuracy = 82.81%  **\n",
      "Step: 9370, loss: 0.0598, accuracy: 98.4375%\n",
      "**  Step 9370, val loss = 0.40, val accuracy = 87.50%  **\n",
      "Step: 9375, loss: 0.0574, accuracy: 98.4375%\n",
      "**  Step 9375, val loss = 0.57, val accuracy = 87.50%  **\n",
      "Step: 9380, loss: 0.0471, accuracy: 100.0000%\n",
      "**  Step 9380, val loss = 0.59, val accuracy = 85.94%  **\n",
      "Step: 9385, loss: 0.0365, accuracy: 98.4375%\n",
      "**  Step 9385, val loss = 0.65, val accuracy = 84.38%  **\n",
      "Step: 9390, loss: 0.0465, accuracy: 98.4375%\n",
      "**  Step 9390, val loss = 0.65, val accuracy = 87.50%  **\n",
      "Step: 9395, loss: 0.0949, accuracy: 96.8750%\n",
      "**  Step 9395, val loss = 0.58, val accuracy = 84.38%  **\n",
      "Step: 9400, loss: 0.0474, accuracy: 98.4375%\n",
      "**  Step 9400, val loss = 0.41, val accuracy = 89.06%  **\n",
      "Step: 9405, loss: 0.0326, accuracy: 100.0000%\n",
      "**  Step 9405, val loss = 0.53, val accuracy = 84.38%  **\n",
      "Step: 9410, loss: 0.1022, accuracy: 98.4375%\n",
      "**  Step 9410, val loss = 0.34, val accuracy = 87.50%  **\n",
      "Step: 9415, loss: 0.0346, accuracy: 100.0000%\n",
      "**  Step 9415, val loss = 0.30, val accuracy = 89.06%  **\n",
      "Step: 9420, loss: 0.0987, accuracy: 95.3125%\n",
      "**  Step 9420, val loss = 0.50, val accuracy = 85.94%  **\n",
      "Step: 9425, loss: 0.0869, accuracy: 96.8750%\n",
      "**  Step 9425, val loss = 0.37, val accuracy = 90.62%  **\n",
      "Step: 9430, loss: 0.0480, accuracy: 100.0000%\n",
      "**  Step 9430, val loss = 0.40, val accuracy = 93.75%  **\n",
      "Step: 9435, loss: 0.0824, accuracy: 100.0000%\n",
      "**  Step 9435, val loss = 0.51, val accuracy = 84.38%  **\n",
      "Step: 9440, loss: 0.1011, accuracy: 96.8750%\n",
      "**  Step 9440, val loss = 0.71, val accuracy = 85.94%  **\n",
      "Step: 9445, loss: 0.0742, accuracy: 98.4375%\n",
      "**  Step 9445, val loss = 0.80, val accuracy = 78.12%  **\n",
      "Step: 9450, loss: 0.0410, accuracy: 100.0000%\n",
      "**  Step 9450, val loss = 0.42, val accuracy = 87.50%  **\n",
      "Step: 9455, loss: 0.0797, accuracy: 98.4375%\n",
      "**  Step 9455, val loss = 0.91, val accuracy = 87.50%  **\n",
      "Step: 9460, loss: 0.1550, accuracy: 93.7500%\n",
      "**  Step 9460, val loss = 0.59, val accuracy = 85.94%  **\n",
      "Step: 9465, loss: 0.0787, accuracy: 98.4375%\n",
      "**  Step 9465, val loss = 0.43, val accuracy = 92.19%  **\n",
      "Step: 9470, loss: 0.0311, accuracy: 96.8750%\n",
      "**  Step 9470, val loss = 0.32, val accuracy = 93.75%  **\n",
      "Step: 9475, loss: 0.0491, accuracy: 98.4375%\n",
      "**  Step 9475, val loss = 0.54, val accuracy = 84.38%  **\n",
      "Step: 9480, loss: 0.0567, accuracy: 100.0000%\n",
      "**  Step 9480, val loss = 0.40, val accuracy = 89.06%  **\n",
      "Step: 9485, loss: 0.0768, accuracy: 96.8750%\n",
      "**  Step 9485, val loss = 0.57, val accuracy = 85.94%  **\n",
      "Step: 9490, loss: 0.0357, accuracy: 100.0000%\n",
      "**  Step 9490, val loss = 0.26, val accuracy = 89.06%  **\n",
      "Step: 9495, loss: 0.0550, accuracy: 98.4375%\n",
      "**  Step 9495, val loss = 0.15, val accuracy = 92.19%  **\n",
      "Step: 9500, loss: 0.0838, accuracy: 96.8750%\n",
      "**  Step 9500, val loss = 0.50, val accuracy = 89.06%  **\n",
      "Step: 9505, loss: 0.0590, accuracy: 98.4375%\n",
      "**  Step 9505, val loss = 0.83, val accuracy = 84.38%  **\n",
      "Step: 9510, loss: 0.1970, accuracy: 95.3125%\n",
      "**  Step 9510, val loss = 0.76, val accuracy = 85.94%  **\n",
      "Step: 9515, loss: 0.1224, accuracy: 96.8750%\n",
      "**  Step 9515, val loss = 0.28, val accuracy = 92.19%  **\n",
      "Step: 9520, loss: 0.0302, accuracy: 100.0000%\n",
      "**  Step 9520, val loss = 1.25, val accuracy = 81.25%  **\n",
      "Step: 9525, loss: 0.0508, accuracy: 100.0000%\n",
      "**  Step 9525, val loss = 1.07, val accuracy = 85.94%  **\n",
      "Step: 9530, loss: 0.0773, accuracy: 96.8750%\n",
      "**  Step 9530, val loss = 0.41, val accuracy = 87.50%  **\n",
      "Step: 9535, loss: 0.1449, accuracy: 93.7500%\n",
      "**  Step 9535, val loss = 0.36, val accuracy = 87.50%  **\n",
      "Step: 9540, loss: 0.1336, accuracy: 93.7500%\n",
      "**  Step 9540, val loss = 0.49, val accuracy = 87.50%  **\n",
      "Step: 9545, loss: 0.0939, accuracy: 96.8750%\n",
      "**  Step 9545, val loss = 0.31, val accuracy = 85.94%  **\n",
      "Step: 9550, loss: 0.0628, accuracy: 98.4375%\n",
      "**  Step 9550, val loss = 0.86, val accuracy = 82.81%  **\n",
      "Step: 9555, loss: 0.0828, accuracy: 96.8750%\n",
      "**  Step 9555, val loss = 0.13, val accuracy = 93.75%  **\n",
      "Step: 9560, loss: 0.0514, accuracy: 98.4375%\n",
      "**  Step 9560, val loss = 1.08, val accuracy = 78.12%  **\n",
      "Step: 9565, loss: 0.1141, accuracy: 95.3125%\n",
      "**  Step 9565, val loss = 0.52, val accuracy = 85.94%  **\n",
      "Step: 9570, loss: 0.0879, accuracy: 96.8750%\n",
      "**  Step 9570, val loss = 0.29, val accuracy = 90.62%  **\n",
      "Step: 9575, loss: 0.0547, accuracy: 100.0000%\n",
      "**  Step 9575, val loss = 0.84, val accuracy = 82.81%  **\n",
      "Step: 9580, loss: 0.0800, accuracy: 98.4375%\n",
      "**  Step 9580, val loss = 0.15, val accuracy = 93.75%  **\n",
      "Step: 9585, loss: 0.0192, accuracy: 98.4375%\n",
      "**  Step 9585, val loss = 0.74, val accuracy = 87.50%  **\n",
      "Step: 9590, loss: 0.0965, accuracy: 96.8750%\n",
      "**  Step 9590, val loss = 0.57, val accuracy = 84.38%  **\n",
      "Step: 9595, loss: 0.0740, accuracy: 96.8750%\n",
      "**  Step 9595, val loss = 0.71, val accuracy = 85.94%  **\n",
      "Step: 9600, loss: 0.0480, accuracy: 98.4375%\n",
      "**  Step 9600, val loss = 0.52, val accuracy = 84.38%  **\n",
      "Step: 9605, loss: 0.0376, accuracy: 100.0000%\n",
      "**  Step 9605, val loss = 0.67, val accuracy = 84.38%  **\n",
      "Step: 9610, loss: 0.0685, accuracy: 98.4375%\n",
      "**  Step 9610, val loss = 0.61, val accuracy = 81.25%  **\n",
      "Step: 9615, loss: 0.0238, accuracy: 100.0000%\n",
      "**  Step 9615, val loss = 0.60, val accuracy = 84.38%  **\n",
      "Step: 9620, loss: 0.0606, accuracy: 96.8750%\n",
      "**  Step 9620, val loss = 0.45, val accuracy = 85.94%  **\n",
      "Step: 9625, loss: 0.0269, accuracy: 100.0000%\n",
      "**  Step 9625, val loss = 0.54, val accuracy = 82.81%  **\n",
      "Step: 9630, loss: 0.0730, accuracy: 98.4375%\n",
      "**  Step 9630, val loss = 0.76, val accuracy = 84.38%  **\n",
      "Step: 9635, loss: 0.0535, accuracy: 98.4375%\n",
      "**  Step 9635, val loss = 0.83, val accuracy = 84.38%  **\n",
      "Step: 9640, loss: 0.0317, accuracy: 100.0000%\n",
      "**  Step 9640, val loss = 0.33, val accuracy = 89.06%  **\n",
      "Step: 9645, loss: 0.0732, accuracy: 98.4375%\n",
      "**  Step 9645, val loss = 0.53, val accuracy = 87.50%  **\n",
      "Step: 9650, loss: 0.0537, accuracy: 98.4375%\n",
      "**  Step 9650, val loss = 0.61, val accuracy = 87.50%  **\n",
      "Step: 9655, loss: 0.0541, accuracy: 98.4375%\n",
      "**  Step 9655, val loss = 0.26, val accuracy = 89.06%  **\n",
      "Step: 9660, loss: 0.0713, accuracy: 98.4375%\n",
      "**  Step 9660, val loss = 0.46, val accuracy = 89.06%  **\n",
      "Step: 9665, loss: 0.0832, accuracy: 98.4375%\n",
      "**  Step 9665, val loss = 0.64, val accuracy = 81.25%  **\n",
      "Step: 9670, loss: 0.0808, accuracy: 96.8750%\n",
      "**  Step 9670, val loss = 0.69, val accuracy = 82.81%  **\n",
      "Step: 9675, loss: 0.0436, accuracy: 100.0000%\n",
      "**  Step 9675, val loss = 0.65, val accuracy = 82.81%  **\n",
      "Step: 9680, loss: 0.0756, accuracy: 96.8750%\n",
      "**  Step 9680, val loss = 0.38, val accuracy = 89.06%  **\n",
      "Step: 9685, loss: 0.1002, accuracy: 96.8750%\n",
      "**  Step 9685, val loss = 0.66, val accuracy = 79.69%  **\n",
      "Step: 9690, loss: 0.0497, accuracy: 98.4375%\n",
      "**  Step 9690, val loss = 0.66, val accuracy = 79.69%  **\n",
      "Step: 9695, loss: 0.0302, accuracy: 100.0000%\n",
      "**  Step 9695, val loss = 0.25, val accuracy = 89.06%  **\n",
      "Step: 9700, loss: 0.0767, accuracy: 98.4375%\n",
      "**  Step 9700, val loss = 0.42, val accuracy = 87.50%  **\n",
      "Step: 9705, loss: 0.0749, accuracy: 96.8750%\n",
      "**  Step 9705, val loss = 0.73, val accuracy = 87.50%  **\n",
      "Step: 9710, loss: 0.0787, accuracy: 98.4375%\n",
      "**  Step 9710, val loss = 0.72, val accuracy = 84.38%  **\n",
      "Step: 9715, loss: 0.0355, accuracy: 100.0000%\n",
      "**  Step 9715, val loss = 0.36, val accuracy = 87.50%  **\n",
      "Step: 9720, loss: 0.0431, accuracy: 100.0000%\n",
      "**  Step 9720, val loss = 0.40, val accuracy = 92.19%  **\n",
      "Step: 9725, loss: 0.2236, accuracy: 93.7500%\n",
      "**  Step 9725, val loss = 1.04, val accuracy = 81.25%  **\n",
      "Step: 9730, loss: 0.0271, accuracy: 100.0000%\n",
      "**  Step 9730, val loss = 0.31, val accuracy = 90.62%  **\n",
      "Step: 9735, loss: 0.0856, accuracy: 96.8750%\n",
      "**  Step 9735, val loss = 0.84, val accuracy = 84.38%  **\n",
      "Step: 9740, loss: 0.0764, accuracy: 96.8750%\n",
      "**  Step 9740, val loss = 0.26, val accuracy = 90.62%  **\n",
      "Step: 9745, loss: 0.0830, accuracy: 98.4375%\n",
      "**  Step 9745, val loss = 0.74, val accuracy = 82.81%  **\n",
      "Step: 9750, loss: 0.0783, accuracy: 100.0000%\n",
      "**  Step 9750, val loss = 0.76, val accuracy = 84.38%  **\n",
      "Step: 9755, loss: 0.1113, accuracy: 95.3125%\n",
      "**  Step 9755, val loss = 0.57, val accuracy = 87.50%  **\n",
      "Step: 9760, loss: 0.0318, accuracy: 100.0000%\n",
      "**  Step 9760, val loss = 0.80, val accuracy = 82.81%  **\n",
      "Step: 9765, loss: 0.0899, accuracy: 100.0000%\n",
      "**  Step 9765, val loss = 0.68, val accuracy = 82.81%  **\n",
      "Step: 9770, loss: 0.1424, accuracy: 95.3125%\n",
      "**  Step 9770, val loss = 0.48, val accuracy = 87.50%  **\n",
      "Step: 9775, loss: 0.1010, accuracy: 93.7500%\n",
      "**  Step 9775, val loss = 0.47, val accuracy = 87.50%  **\n",
      "Step: 9780, loss: 0.0323, accuracy: 100.0000%\n",
      "**  Step 9780, val loss = 0.53, val accuracy = 90.62%  **\n",
      "Step: 9785, loss: 0.0785, accuracy: 98.4375%\n",
      "**  Step 9785, val loss = 0.44, val accuracy = 90.62%  **\n",
      "Step: 9790, loss: 0.0542, accuracy: 100.0000%\n",
      "**  Step 9790, val loss = 0.69, val accuracy = 84.38%  **\n",
      "Step: 9795, loss: 0.0577, accuracy: 98.4375%\n",
      "**  Step 9795, val loss = 0.67, val accuracy = 85.94%  **\n",
      "Step: 9800, loss: 0.1174, accuracy: 96.8750%\n",
      "**  Step 9800, val loss = 0.74, val accuracy = 82.81%  **\n",
      "Step: 9805, loss: 0.0614, accuracy: 100.0000%\n",
      "**  Step 9805, val loss = 0.38, val accuracy = 89.06%  **\n",
      "Step: 9810, loss: 0.1581, accuracy: 95.3125%\n",
      "**  Step 9810, val loss = 0.50, val accuracy = 84.38%  **\n",
      "Step: 9815, loss: 0.0647, accuracy: 98.4375%\n",
      "**  Step 9815, val loss = 0.42, val accuracy = 89.06%  **\n",
      "Step: 9820, loss: 0.0697, accuracy: 98.4375%\n",
      "**  Step 9820, val loss = 0.51, val accuracy = 85.94%  **\n",
      "Step: 9825, loss: 0.0202, accuracy: 100.0000%\n",
      "**  Step 9825, val loss = 0.62, val accuracy = 82.81%  **\n",
      "Step: 9830, loss: 0.0743, accuracy: 98.4375%\n",
      "**  Step 9830, val loss = 0.32, val accuracy = 87.50%  **\n",
      "Step: 9835, loss: 0.0966, accuracy: 96.8750%\n",
      "**  Step 9835, val loss = 0.16, val accuracy = 93.75%  **\n",
      "Step: 9840, loss: 0.0646, accuracy: 98.4375%\n",
      "**  Step 9840, val loss = 0.39, val accuracy = 85.94%  **\n",
      "Step: 9845, loss: 0.0956, accuracy: 98.4375%\n",
      "**  Step 9845, val loss = 0.40, val accuracy = 85.94%  **\n",
      "Step: 9850, loss: 0.0549, accuracy: 100.0000%\n",
      "**  Step 9850, val loss = 0.66, val accuracy = 87.50%  **\n",
      "Step: 9855, loss: 0.0750, accuracy: 98.4375%\n",
      "**  Step 9855, val loss = 0.53, val accuracy = 87.50%  **\n",
      "Step: 9860, loss: 0.0199, accuracy: 100.0000%\n",
      "**  Step 9860, val loss = 0.41, val accuracy = 89.06%  **\n",
      "Step: 9865, loss: 0.0759, accuracy: 100.0000%\n",
      "**  Step 9865, val loss = 0.84, val accuracy = 76.56%  **\n",
      "Step: 9870, loss: 0.0258, accuracy: 98.4375%\n",
      "**  Step 9870, val loss = 0.73, val accuracy = 87.50%  **\n",
      "Step: 9875, loss: 0.0762, accuracy: 96.8750%\n",
      "**  Step 9875, val loss = 0.45, val accuracy = 87.50%  **\n",
      "Step: 9880, loss: 0.0512, accuracy: 100.0000%\n",
      "**  Step 9880, val loss = 0.76, val accuracy = 84.38%  **\n",
      "Step: 9885, loss: 0.1093, accuracy: 96.8750%\n",
      "**  Step 9885, val loss = 0.38, val accuracy = 87.50%  **\n",
      "Step: 9890, loss: 0.0576, accuracy: 98.4375%\n",
      "**  Step 9890, val loss = 0.30, val accuracy = 93.75%  **\n",
      "Step: 9895, loss: 0.0756, accuracy: 96.8750%\n",
      "**  Step 9895, val loss = 0.62, val accuracy = 84.38%  **\n",
      "Step: 9900, loss: 0.0586, accuracy: 100.0000%\n",
      "**  Step 9900, val loss = 0.90, val accuracy = 84.38%  **\n",
      "Step: 9905, loss: 0.0768, accuracy: 96.8750%\n",
      "**  Step 9905, val loss = 0.29, val accuracy = 89.06%  **\n",
      "Step: 9910, loss: 0.1144, accuracy: 96.8750%\n",
      "**  Step 9910, val loss = 0.71, val accuracy = 85.94%  **\n",
      "Step: 9915, loss: 0.0722, accuracy: 98.4375%\n",
      "**  Step 9915, val loss = 0.35, val accuracy = 87.50%  **\n",
      "Step: 9920, loss: 0.0467, accuracy: 100.0000%\n",
      "**  Step 9920, val loss = 1.05, val accuracy = 81.25%  **\n",
      "Step: 9925, loss: 0.0545, accuracy: 98.4375%\n",
      "**  Step 9925, val loss = 0.71, val accuracy = 82.81%  **\n",
      "Step: 9930, loss: 0.0744, accuracy: 100.0000%\n",
      "**  Step 9930, val loss = 0.48, val accuracy = 87.50%  **\n",
      "Step: 9935, loss: 0.1256, accuracy: 93.7500%\n",
      "**  Step 9935, val loss = 0.47, val accuracy = 85.94%  **\n",
      "Step: 9940, loss: 0.0587, accuracy: 100.0000%\n",
      "**  Step 9940, val loss = 0.53, val accuracy = 89.06%  **\n",
      "Step: 9945, loss: 0.0425, accuracy: 100.0000%\n",
      "**  Step 9945, val loss = 0.56, val accuracy = 89.06%  **\n",
      "Step: 9950, loss: 0.0819, accuracy: 98.4375%\n",
      "**  Step 9950, val loss = 0.26, val accuracy = 92.19%  **\n",
      "Step: 9955, loss: 0.0945, accuracy: 98.4375%\n",
      "**  Step 9955, val loss = 0.35, val accuracy = 85.94%  **\n",
      "Step: 9960, loss: 0.0720, accuracy: 96.8750%\n",
      "**  Step 9960, val loss = 0.37, val accuracy = 84.38%  **\n",
      "Step: 9965, loss: 0.1485, accuracy: 93.7500%\n",
      "**  Step 9965, val loss = 0.55, val accuracy = 87.50%  **\n",
      "Step: 9970, loss: 0.0311, accuracy: 100.0000%\n",
      "**  Step 9970, val loss = 0.82, val accuracy = 87.50%  **\n",
      "Step: 9975, loss: 0.0411, accuracy: 98.4375%\n",
      "**  Step 9975, val loss = 0.37, val accuracy = 89.06%  **\n",
      "Step: 9980, loss: 0.0491, accuracy: 100.0000%\n",
      "**  Step 9980, val loss = 0.65, val accuracy = 84.38%  **\n",
      "Step: 9985, loss: 0.0698, accuracy: 96.8750%\n",
      "**  Step 9985, val loss = 0.31, val accuracy = 93.75%  **\n",
      "Step: 9990, loss: 0.0391, accuracy: 98.4375%\n",
      "**  Step 9990, val loss = 0.40, val accuracy = 87.50%  **\n",
      "Step: 9995, loss: 0.0362, accuracy: 98.4375%\n",
      "**  Step 9995, val loss = 0.30, val accuracy = 90.62%  **\n",
      "Step: 9999, loss: 0.0734, accuracy: 98.4375%\n",
      "**  Step 9999, val loss = 0.34, val accuracy = 85.94%  **\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope('input'):\n",
    "\n",
    "    image_batch, label_batch  = read_SVHN(data_dir = data_dir,\n",
    "                                                        ratio = 0.1,\n",
    "                                                        batch_size = 64)\n",
    "    tra_image_batch = image_batch[0]\n",
    "    tra_label_batch = label_batch[0]\n",
    "\n",
    "    val_image_batch = image_batch[1]\n",
    "    val_label_batch = label_batch[1]\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[BATCH_SIZE, IMG_W, IMG_H, 1])\n",
    "y_ = tf.placeholder(tf.int16, shape=[BATCH_SIZE,N_CLASSES])\n",
    "\n",
    "\n",
    "c1 = conv('conv1', x, 16)\n",
    "p1 = pool('pool1', c1)\n",
    "\n",
    "c2 = conv('conv2', p1, 64)\n",
    "p2 = pool('pool2', c2)\n",
    "\n",
    "c3 = conv('conv3', p2, 128)\n",
    "p3 = pool('pool3', c3)\n",
    "\n",
    "fc = FC_layer('fc4', p3, out_nodes = 64)\n",
    "do = drop_out('drop_out', fc, keep_prob = 0.5)\n",
    "logits = final_layer('softmax', do, out_nodes=N_CLASSES)\n",
    "\n",
    "loss = lossFn(logits, y_)\n",
    "accuracy = accuracyFn(logits, y_)\n",
    "\n",
    "my_global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "train_op = optimize(loss, learning_rate, my_global_step)\n",
    "summary_op = tf.summary.merge_all()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "    tra_summary_writer = tf.summary.FileWriter(train_log_dir, sess.graph)\n",
    "    val_summary_writer = tf.summary.FileWriter(val_log_dir, sess.graph)\n",
    "\n",
    "\n",
    "    try:\n",
    "        for step in np.arange(MAX_STEP):\n",
    "            if coord.should_stop():\n",
    "                break\n",
    "            tra_images,tra_labels = sess.run([tra_image_batch, tra_label_batch])\n",
    "            _, tra_loss, tra_acc = sess.run([train_op, loss, accuracy], feed_dict={x:tra_images, y_:tra_labels})\n",
    "            \n",
    "            if step % 5 == 0 or (step + 1) == MAX_STEP:\n",
    "\n",
    "                print ('Step: %d, loss: %.4f, accuracy: %.4f%%' % (step, tra_loss, tra_acc))\n",
    "                _, summary_str = sess.run([train_op, summary_op], feed_dict={x: tra_images, y_: tra_labels})\n",
    "                tra_summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "            if step % 5 == 0 or (step + 1) == MAX_STEP:\n",
    "                val_images, val_labels = sess.run([val_image_batch, val_label_batch])\n",
    "                val_loss, val_acc = sess.run([loss, accuracy], feed_dict={x:val_images,y_:val_labels})\n",
    "\n",
    "                print('**  Step %d, val loss = %.2f, val accuracy = %.2f%%  **' %(step, val_loss, val_acc))\n",
    "                _, summary_str = sess.run([train_op, summary_op], feed_dict={x: val_images, y_: val_labels})\n",
    "                val_summary_writer.add_summary(summary_str, step)\n",
    "            \n",
    "            if step == (MAX_STEP - 1):\n",
    "                c1_v = sess.run(c1, feed_dict={x:tra_images})\n",
    "                p1_v = sess.run(p1, feed_dict={c1:c1_v})\n",
    "                c2_v = sess.run(c2, feed_dict={p1:p1_v})\n",
    "                p2_v = sess.run(p2, feed_dict={c2:c2_v})\n",
    "                c3_v = sess.run(c3, feed_dict={p2:p2_v})\n",
    "                p3_v = sess.run(p3, feed_dict={c3:c3_v})\n",
    "\n",
    "    except tf.errors.OutOfRangeError:\n",
    "        print('Done training -- epoch limit reached')\n",
    "\n",
    "    finally:\n",
    "        coord.request_stop()\n",
    "    coord.join(threads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image1](./fig/SVHN_uncov_train_acc.png)\n",
    "![image1](./fig/SVHN_uncov_train_loss.png)\n",
    "![image1](./fig/SVHN_uncov_val_acc.png)\n",
    "![image1](./fig/SVHN_uncov_val_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpool(value, name='unpool'):\n",
    "    \"\"\"N-dimensional version of the unpooling operation from\n",
    "    https://www.robots.ox.ac.uk/~vgg/rg/papers/Dosovitskiy_Learning_to_Generate_2015_CVPR_paper.pdf\n",
    "    :param value: A Tensor of shape [b, d0, d1, ..., dn, ch]\n",
    "    :return: A Tensor of shape [b, 2*d0, 2*d1, ..., 2*dn, ch]\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name) as scope:\n",
    "        shape = value.get_shape().as_list()\n",
    "        dim = len(shape[1:-1])\n",
    "        out = (tf.reshape(value, [-1] + shape[-dim:]))\n",
    "        for i in range(dim, 0, -1):\n",
    "            out = tf.concat([out, out],i)\n",
    "        out_size = [-1] + [s * 2 for s in shape[1:-1]] + [shape[-1]]\n",
    "        out = tf.reshape(out, out_size, name=scope)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def display_features(v_name, pool_size, out_shape, p_v, figs, col_mp):\n",
    "    \n",
    "    w1 = [v for v in tf.trainable_variables() if v.name == v_name][0]\n",
    "    \n",
    "    #Reconstruction from the layer CNV-1\n",
    "    featuresReLu = tf.placeholder(tf.float32, pool_size)\n",
    "    unPool = unpool(featuresReLu)\n",
    "    unReLu = tf.nn.relu(unPool)\n",
    "    unBias  = unReLu\n",
    "    unConv = tf.nn.conv2d_transpose(unBias, w1, output_shape=out_shape , strides=[1,1,1,1], padding=\"SAME\")\n",
    "    \n",
    "    plt.figure(figsize=figs)\n",
    "    for i in range(figs[0]*figs[1]):\n",
    "        isolated = p_v.copy()\n",
    "        isolated[:,:,:,:i] = 0\n",
    "        isolated[:,:,:,i+1:] = 0#These two lines set all other features other than feature i to 0\n",
    "        totals = np.sum(isolated,axis=(1,2,3))\n",
    "        best = np.argmax(totals,axis=0)\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            pixelactive = sess.run(unConv, feed_dict={featuresReLu: isolated})\n",
    "        #print(pixelactive.shape) \n",
    "        plt.subplot(figs[0],figs[1],i+1)    \n",
    "        plt.imshow(pixelactive[best,:,:,0],cmap = col_mp)\n",
    "        plt.axis('off') \n",
    "    plt.subplots_adjust(top=0.5, bottom=0.08, left=0.10, right=0.95, hspace=0.1, wspace=0.35)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQYAAACbCAYAAACAujUVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1514be2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_features(\n",
    "                v_name = \"conv1/weights:0\", \n",
    "                pool_size = [None, 16,16,16],\n",
    "                out_shape = [64,32,32,1],\n",
    "                p_v = p1_v,\n",
    "                figs = (4,4), \n",
    "                col_mp=\"gray\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfwAAAEUCAYAAADORYT8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x15187fc88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_features(\n",
    "                v_name = \"conv2/weights:0\", \n",
    "                pool_size = [None, 8, 8, 64],\n",
    "                out_shape = [64,16,16,16],\n",
    "                p_v = p2_v,\n",
    "                figs = (8,8), \n",
    "                col_mp=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x150fbdd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAggAAAHmCAYAAAAWWnYMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x151228198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_features(\n",
    "                v_name = \"conv3/weights:0\", \n",
    "                pool_size = [None, 4, 4, 128],\n",
    "                out_shape = [64,8,8,64],\n",
    "                p_v = p3_v,\n",
    "                figs = (8,16), \n",
    "                col_mp=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
