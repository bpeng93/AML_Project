{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.platform import gfile\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import scipy.misc\n",
    "import math\n",
    "import pickle\n",
    "from random import randint\n",
    "import os\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GCN(data_dir, ratio):\n",
    "    train_data = sio.loadmat(data_dir)\n",
    "    x_train = train_data['X']\n",
    "    y_train = train_data['y']\n",
    "    y_train[y_train == 10] = 0\n",
    "    x_train = x_train.transpose((3,0,1,2))\n",
    "    x_train.astype(float)\n",
    "    x_gray = np.dot(x_train, [[0.2989],[0.5870],[0.1140]])\n",
    "\n",
    "    imsize = x_gray.shape[0]\n",
    "    mean = np.mean(x_gray, axis=(1,2), dtype=float)\n",
    "    std = np.std(x_gray, axis=(1,2), dtype=float, ddof=1)\n",
    "    std[std < 1e-4] = 1\n",
    "    x_GCN = np.zeros(x_gray.shape, dtype=float)\n",
    "    for i in np.arange(imsize):\n",
    "        x_GCN[i,:,:] = (x_gray[i,:,:] - mean[i]) / std[i]\n",
    "    nums = x_GCN.shape[0]\n",
    "    x_GCN = x_GCN.reshape((nums,-1))\n",
    "    data = np.hstack((y_train,x_GCN))\n",
    "    np.random.shuffle(data)\n",
    "    cut=math.floor(nums*ratio)\n",
    "    train,val = data[:cut,:], data[cut:,:]\n",
    "\n",
    "    print(\"\\n------- GCN done -------\")\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_SVHN(data_dir, ratio, batch_size):\n",
    "\n",
    "    train,val = GCN(data_dir, ratio)\n",
    "    img_width = 32\n",
    "    img_height = 32\n",
    "    img_depth = 1\n",
    "    label_bytes = 1\n",
    "    image_bytes = 1024\n",
    "    record_bytes = 1025\n",
    "\n",
    "    with tf.name_scope('input'):\n",
    "        images_list = []\n",
    "        label_batch_list = []\n",
    "        for train_val in [train, val]:\n",
    "            q = tf.train.input_producer(train_val)\n",
    "            input_data = q.dequeue()\n",
    "\n",
    "            label = tf.slice(input_data , [0], [1])\n",
    "            label = tf.cast(label, tf.int32)\n",
    "\n",
    "            image_raw = tf.slice(input_data , [1], [1024])\n",
    "            image_raw = tf.reshape(image_raw, [1, 32, 32])\n",
    "            image = tf.transpose(image_raw, (1,2,0))\n",
    "            image = tf.cast(image, tf.float32)\n",
    "            images, label_batch = tf.train.batch([image, label],\n",
    "                                                batch_size = batch_size,\n",
    "                                                num_threads = 16,\n",
    "                                                capacity= 2000)\n",
    "\n",
    "            n_classes = 10\n",
    "            label_batch = tf.one_hot(label_batch, depth = n_classes)\n",
    "\n",
    "            label_batch_list.append(tf.reshape(label_batch, [batch_size, n_classes]))\n",
    "            images_list.append(images)\n",
    "\n",
    "\n",
    "        return images_list, label_batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(layer_name, x, out_channels, kernel_size=[5,5], stride=[1,1,1,1]):\n",
    "    '''Convolution op wrapper, use RELU activation after convolution\n",
    "    Args:\n",
    "        layer_name: e.g. conv1, pool1...\n",
    "        x: input tensor, [batch_size, height, width, channels]\n",
    "        out_channels: number of output channels (or comvolutional kernels)\n",
    "        kernel_size: the size of convolutional kernel, VGG paper used: [3,3]\n",
    "        stride: A list of ints. 1-D of length 4. VGG paper used: [1, 1, 1, 1]\n",
    "        is_pretrain: if load pretrained parameters, freeze all conv layers.\n",
    "        Depending on different situations, you can just set part of conv layers to be freezed.\n",
    "        the parameters of freezed layers will not change when training.\n",
    "    Returns:\n",
    "        4D tensor\n",
    "    '''\n",
    "\n",
    "    in_channels = x.get_shape()[-1]\n",
    "    with tf.variable_scope(layer_name):\n",
    "        w = tf.get_variable(name='weights',\n",
    "                            shape=[kernel_size[0], kernel_size[1], in_channels, out_channels],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer()) # default is uniform distribution initialization\n",
    "        b = tf.get_variable(name='biases',\n",
    "                            shape=[out_channels],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "        x = tf.nn.conv2d(x, w, stride, padding='SAME', name='conv')\n",
    "        x = tf.nn.bias_add(x, b, name='bias_add')\n",
    "        x = tf.nn.relu(x, name='relu')\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pool(layer_name, x, kernel=[1,2,2,1], stride=[1,2,2,1]):\n",
    "    '''Pooling op\n",
    "    Args:\n",
    "        x: input tensor\n",
    "        kernel: pooling kernel, VGG paper used [1,2,2,1], the size of kernel is 2X2\n",
    "        stride: stride size, VGG paper used [1,2,2,1]\n",
    "        padding:'SAME'\n",
    "    '''\n",
    "    \n",
    "    x = tf.nn.max_pool(x, kernel, strides=stride, padding='SAME', name=layer_name)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FC_layer(layer_name, x, out_nodes):\n",
    "    '''Wrapper for fully connected layers with RELU activation as default\n",
    "    Args:\n",
    "        layer_name: e.g. 'FC1', 'FC2'\n",
    "        x: input feature map\n",
    "        out_nodes: number of neurons for current FC layer\n",
    "    '''\n",
    "    shape = x.get_shape()\n",
    "    size = shape[1].value * shape[2].value * shape[3].value\n",
    "\n",
    "    with tf.variable_scope(layer_name):\n",
    "        w = tf.get_variable('weights',\n",
    "                            shape=[size, out_nodes],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable('biases',\n",
    "                            shape=[out_nodes],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "        flat_x = tf.reshape(x, [-1, size]) # flatten into 1D\n",
    "\n",
    "        x = tf.nn.bias_add(tf.matmul(flat_x, w), b)\n",
    "        x = tf.nn.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def final_layer(layer_name, x, out_nodes):\n",
    "    shape = x.get_shape()\n",
    "    size = shape[-1].value\n",
    "    with tf.variable_scope(layer_name):\n",
    "        w = tf.get_variable('weights',\n",
    "                            shape=[size, out_nodes],\n",
    "                            initializer=tf.contrib.layers.xavier_initializer())\n",
    "        b = tf.get_variable('biases',\n",
    "                            shape=[out_nodes],\n",
    "                            initializer=tf.constant_initializer(0.0))\n",
    "        flat_x = tf.reshape(x, [-1, size]) # flatten into 1D\n",
    "        x = tf.nn.bias_add(tf.matmul(flat_x, w), b)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_out(layer_name, x, keep_prob = 0.5):\n",
    "    with tf.variable_scope(layer_name):\n",
    "        x = tf.nn.dropout(x, keep_prob)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lossFn(logits, labels):\n",
    "    with tf.variable_scope('loss') as scope:\n",
    "        labels = tf.cast(labels, tf.int64)\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits\\\n",
    "                        (logits=logits, labels=labels, name='loss')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='loss')\n",
    "        tf.summary.scalar(scope.name+'/loss', loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracyFn(logits, labels):\n",
    "  \"\"\"Evaluate the quality of the logits at predicting the label.\n",
    "  Args:\n",
    "    logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "    labels: Labels tensor,\n",
    "  \"\"\"\n",
    "  with tf.name_scope('accuracy') as scope:\n",
    "      correct = tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1))\n",
    "      correct = tf.cast(correct, tf.float32)\n",
    "      accuracy = tf.reduce_mean(correct)*100.0\n",
    "      tf.summary.scalar(scope+'/accuracy', accuracy)\n",
    "  return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize(loss, learning_rate, global_step):\n",
    "    '''optimization, use Gradient Descent as default\n",
    "    '''\n",
    "    with tf.name_scope('optimizer'):\n",
    "        # optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "        return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SVHN(x, n_classes):\n",
    "    '''\n",
    "    Args:\n",
    "        images: 4D tensor [batch_size, img_width, img_height, img_channel]\n",
    "    Notes:\n",
    "        In each conv layer, the kernel size is:\n",
    "        [kernel_size, kernel_size, number of input channels, number of output channels].\n",
    "        number of input channels are from previuous layer, if previous layer is THE input\n",
    "        layer, number of input channels should be image's channels.\n",
    "\n",
    "\n",
    "    '''\n",
    "    x = conv('conv1', x, 16)\n",
    "    x = pool('pool1', x)\n",
    "\n",
    "    x = conv('conv2', x, 64)\n",
    "    x = pool('pool2', x)\n",
    "\n",
    "    x = conv('conv3', x, 128)\n",
    "    x = pool('pool3', x)\n",
    "\n",
    "    x = FC_layer('fc4', x, out_nodes = 64)\n",
    "    x = drop_out('drop_out', x, keep_prob = 0.5)\n",
    "    x = final_layer('softmax', x, out_nodes=n_classes)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_CLASSES = 10\n",
    "IMG_W =  32 # resize the image, if the input image is too large, training will be very slow.\n",
    "IMG_H = 32\n",
    "RATIO = 0.2 # take 20% of dataset as validation data\n",
    "BATCH_SIZE = 64\n",
    "TEST_BATCH_SIZE = 20\n",
    "CAPACITY = 20\n",
    "MAX_STEP = 10 # with current parameters, it is suggested to use MAX_STEP>10k\n",
    "learning_rate = 0.0001 # with current parameters, it is suggested to use learning rate<0.0001\n",
    "\n",
    "data_dir = './data/SVHN/train_32x32.mat'\n",
    "bin_dir = './data/SVHN/'\n",
    "train_log_dir = './logs/train/'\n",
    "val_log_dir = './logs/val/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "\n",
    "    with tf.name_scope('input'):\n",
    "\n",
    "        image_batch, label_batch  = read_SVHN(data_dir = data_dir,\n",
    "                                                            ratio = 0.1,\n",
    "                                                            batch_size = 64)\n",
    "        tra_image_batch = image_batch[0]\n",
    "        tra_label_batch = label_batch[0]\n",
    "\n",
    "        val_image_batch = image_batch[1]\n",
    "        val_label_batch = label_batch[1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=[BATCH_SIZE, IMG_W, IMG_H, 1])\n",
    "    y_ = tf.placeholder(tf.int16, shape=[BATCH_SIZE,N_CLASSES])\n",
    "\n",
    "    logits = SVHN(x, N_CLASSES)\n",
    "    loss = lossFn(logits, y_)\n",
    "    accuracy = accuracyFn(logits, y_)\n",
    "\n",
    "    my_global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = optimize(loss, learning_rate, my_global_step)\n",
    "\n",
    "\n",
    "#     saver = tf.train.Saver(tf.global_variables())\n",
    "    summary_op = tf.summary.merge_all()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "        tra_summary_writer = tf.summary.FileWriter(train_log_dir, sess.graph)\n",
    "        val_summary_writer = tf.summary.FileWriter(val_log_dir, sess.graph)\n",
    "\n",
    "\n",
    "        try:\n",
    "            for step in np.arange(MAX_STEP):\n",
    "                if coord.should_stop():\n",
    "                    break\n",
    "                tra_images,tra_labels = sess.run([tra_image_batch, tra_label_batch])\n",
    "                _, tra_loss, tra_acc = sess.run([train_op, loss, accuracy], feed_dict={x:tra_images, y_:tra_labels})\n",
    "                if step % 50 == 0 or (step + 1) == MAX_STEP:\n",
    "\n",
    "                    print ('Step: %d, loss: %.4f, accuracy: %.4f%%' % (step, tra_loss, tra_acc))\n",
    "                    _, summary_str = sess.run([train_op, summary_op], feed_dict={x: tra_images, y_: tra_labels})\n",
    "                    tra_summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "                if step % 50 == 0 or (step + 1) == MAX_STEP:\n",
    "                    val_images, val_labels = sess.run([val_image_batch, val_label_batch])\n",
    "                    val_loss, val_acc = sess.run([loss, accuracy], feed_dict={x:val_images,y_:val_labels})\n",
    "\n",
    "                    print('**  Step %d, val loss = %.2f, val accuracy = %.2f%%  **' %(step, val_loss, val_acc))\n",
    "                    _, summary_str = sess.run([train_op, summary_op], feed_dict={x: val_images, y_: val_labels})\n",
    "                    val_summary_writer.add_summary(summary_str, step)\n",
    "\n",
    "#                 if step % 1000 == 0 or (step + 1) == MAX_STEP:\n",
    "#                     checkpoint_path = os.path.join(train_log_dir, 'model.ckpt')\n",
    "#                     saver.save(sess,\n",
    "#                                checkpoint_path,\n",
    "#                                global_step=step,\n",
    "#                                write_meta_graph=False)\n",
    "\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done training -- epoch limit reached')\n",
    "\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------- GCN done -------\n",
      "Step: 0, loss: 2.3689, accuracy: 6.2500%\n",
      "**  Step 0, val loss = 2.33, val accuracy = 7.81%  **\n",
      "Step: 9, loss: 2.3001, accuracy: 15.6250%\n",
      "**  Step 9, val loss = 2.26, val accuracy = 15.62%  **\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv1/weights:0' shape=(5, 5, 1, 16) dtype=float32_ref>,\n",
       " <tf.Variable 'conv1/biases:0' shape=(16,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2/weights:0' shape=(5, 5, 16, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'conv2/biases:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'conv3/weights:0' shape=(5, 5, 64, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'conv3/biases:0' shape=(128,) dtype=float32_ref>,\n",
       " <tf.Variable 'fc4/weights:0' shape=(2048, 64) dtype=float32_ref>,\n",
       " <tf.Variable 'fc4/biases:0' shape=(64,) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax/weights:0' shape=(64, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'softmax/biases:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'conv1/weights:0' shape=(5, 5, 1, 16) dtype=float32_ref>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var = [v for v in tf.trainable_variables() if v.name == \"conv1/weights:0\"][0]\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unpool(value, name='unpool'):\n",
    "    \"\"\"N-dimensional version of the unpooling operation from\n",
    "    https://www.robots.ox.ac.uk/~vgg/rg/papers/Dosovitskiy_Learning_to_Generate_2015_CVPR_paper.pdf\n",
    "    :param value: A Tensor of shape [b, d0, d1, ..., dn, ch]\n",
    "    :return: A Tensor of shape [b, 2*d0, 2*d1, ..., 2*dn, ch]\n",
    "    \"\"\"\n",
    "    with tf.name_scope(name) as scope:\n",
    "        sh = value.get_shape().as_list()\n",
    "        dim = len(sh[1:-1])\n",
    "        out = (tf.reshape(value, [-1] + sh[-dim:]))\n",
    "        for i in range(dim, 0, -1):\n",
    "            out = tf.concat([out, out],i)\n",
    "        out_size = [-1] + [s * 2 for s in sh[1:-1]] + [sh[-1]]\n",
    "        out = tf.reshape(out, out_size, name=scope)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstruction from the layer CNV-1\n",
    "featuresReLu_1 = tf.placeholder(tf.float32,[None, 16, 16, 16])\n",
    "unPool_1 = unpool(featuresReLu_1)\n",
    "unReLu_1 = tf.nn.relu(unPool_1)\n",
    "unBias_1 = unReLu_1\n",
    "#unConv = tf.nn.conv2d_transpose(unBias, W_conv1, output_shape=[1,28,28,1] , strides=[1,1,1,1], padding=\"SAME\")\n",
    "unConv_1 = tf.nn.conv2d_transpose(unBias_1, var, output_shape=[64,32,32,1] , strides=[1,1,1,1], padding=\"SAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the features from the layer CNV-1\n",
    "activations1 = h_pool1.eval(feed_dict={x: mnist.train.next_batch(50)[0], y_: mnist.train.next_batch(50)[1]})\n",
    "print (activations1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'activations1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-3ff4d2a40da7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'CNV1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0misolated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mactivations1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0misolated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0misolated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;31m#These two lines set all other features other than feature i to 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'activations1' is not defined"
     ]
    }
   ],
   "source": [
    "#get every feature (1, 14, 14, 1) from activations1 than reconstruct from \n",
    "plt.figure(num='CNV1',figsize=(10,8))\n",
    "for i in range(32):\n",
    "    isolated = activations1.copy()\n",
    "    isolated[:,:,:,:i] = 0\n",
    "    isolated[:,:,:,i+1:] = 0#These two lines set all other features other than feature i to 0\n",
    "    #print (isolated.shape)\n",
    "    totals = np.sum(isolated,axis=(1,2,3))\n",
    "    best = np.argmax(totals,axis=0)\n",
    "    #These two lines are particular for batch of images.\n",
    "    #That means choose one image from this batch of images which feacture i can judge best.\n",
    "    #print (totals)\n",
    "    pixelactive = unConv_1.eval(feed_dict={featuresReLu_1: isolated})\n",
    "    #print(pixelactive.shape) \n",
    "    #shape of every pixelactive is (50, 28, 28, 1) batchsize=50 picsize=28*28 colors=1\n",
    "    plt.subplot(4,8,i+1)    \n",
    "    plt.imshow(pixelactive[best,:,:,0],cmap=\"gray\")\n",
    "    plt.axis('off') \n",
    "plt.subplots_adjust(top=0.5, bottom=0.08, left=0.10, right=0.95, hspace=0.1,\n",
    "                    wspace=0.35)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
